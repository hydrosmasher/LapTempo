{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5b8c1f44",
   "metadata": {},
   "source": [
    "# Race Pacing Models (AutoML + Deep Learning)\n",
    "Interactive notebook version of `race_pacing_models.py`.\n",
    "\n",
    "**What you can do here:**\n",
    "- Train AutoML + Deep Learning models from your uploaded pacing datasets\n",
    "- Generate pre-race optimal splits (per-50 and per-100)\n",
    "- Run post-race analysis with actionable per-50 suggestions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0918150c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# race_pacing_models.py\n",
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"\n",
    "Race Pacing: AutoML + Deep Learning models for pre-race split prediction and post-race analysis.\n",
    "\n",
    "This module builds two kinds of models per event length (n_splits = distance_m / 50):\n",
    "1) AutoML (best-of-several regressors via cross-validation)\n",
    "2) Deep Learning (Keras, with hyperparameter tuning if available; graceful fallback to sklearn MLP)\n",
    "\n",
    "Targets are predicted as normalized split fractions that sum to 1 across the race.\n",
    "At inference time, fractions are multiplied by the target time to return per-50 splits.\n",
    "\n",
    "Functions you’ll likely call:\n",
    "- train_all_models(data_paths: list[str], out_dir: str = \"models\")\n",
    "- pre_race_optimal_splits(distance_m: int, stroke: str, pool: str, pb50_s: float, target_time_s: float,\n",
    "                          model_kind: str = \"auto\", models_dir: str = \"models\") -> dict\n",
    "- post_race_analysis(distance_m: int, stroke: str, pool: str, splits: list[float], split_interval: int = 50,\n",
    "                     model_kind: str = \"auto\", target_time_s: float | None = None,\n",
    "                     models_dir: str = \"models\") -> dict\n",
    "\n",
    "Data expectations (robust parsing implemented):\n",
    "- A CSV per stroke/category. Useful columns (any subset is fine; best effort parsing):\n",
    "    * 'Event' (e.g., '200 Fly', '400 IM') or 'Distance' (50/100/...)\n",
    "    * 'Stroke' (Free/Back/Breast/Fly/IM) – inferred from file name if missing\n",
    "    * 'Pool' (LCM/SCM/SCY) – optional\n",
    "    * 'Final time in seconds' OR 'FinalTime' OR 'Time' (mm:ss.xx or ss.xx) – optional (will fallback to sum(splits))\n",
    "    * 'Splits' (semicolon-separated string per-50), OR split columns like '50_1', '50_2', ...\n",
    "- Rows whose splits do not match the inferred number of 50s for the event are dropped.\n",
    "\n",
    "Modeling notes:\n",
    "- Features X: [distance_m, one-hot(stroke), one-hot(pool), pb50_estimate, total_time_s]\n",
    "  During training, pb50_estimate = min(per-50 splits) of that row (best effort).\n",
    "- Targets Y: normalized split fractions (length = n_splits); zero-padded to MAX_SPLITS for a *global* model per n_splits.\n",
    "- We train a separate model per n_splits (e.g., 1, 2, 4, 8, 16, 30).\n",
    "\n",
    "Dependencies (used if installed):\n",
    "- numpy, pandas, scikit-learn, joblib\n",
    "- optionally: xgboost, lightgbm, catboost\n",
    "- deep learning: tensorflow/keras (preferred) and keras-tuner (optional); fallback to sklearn.neural_network.MLPRegressor.\n",
    "\n",
    "Author: SwimForge (HydroSmasher)\n",
    "\"\"\"\n",
    "from __future__ import annotations\n",
    "\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import math\n",
    "import warnings\n",
    "from dataclasses import dataclass\n",
    "from typing import List, Dict, Tuple, Optional\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import KFold, cross_val_score, train_test_split\n",
    "from sklearn.preprocessing import OneHotEncoder, StandardScaler\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.ensemble import RandomForestRegressor, ExtraTreesRegressor, GradientBoostingRegressor\n",
    "from sklearn.neural_network import MLPRegressor\n",
    "import joblib\n",
    "\n",
    "# Optional imports\n",
    "try:\n",
    "    import xgboost as xgb\n",
    "    HAS_XGB = True\n",
    "except Exception:\n",
    "    HAS_XGB = False\n",
    "\n",
    "try:\n",
    "    import lightgbm as lgb\n",
    "    HAS_LGBM = True\n",
    "except Exception:\n",
    "    HAS_LGBM = False\n",
    "\n",
    "try:\n",
    "    from catboost import CatBoostRegressor\n",
    "    HAS_CAT = True\n",
    "except Exception:\n",
    "    HAS_CAT = False\n",
    "\n",
    "# Deep learning optional imports\n",
    "try:\n",
    "    import tensorflow as tf\n",
    "    from tensorflow import keras\n",
    "    from tensorflow.keras import layers\n",
    "    HAS_TF = True\n",
    "except Exception:\n",
    "    HAS_TF = False\n",
    "\n",
    "try:\n",
    "    import keras_tuner as kt  # hyperparameter tuning for keras models\n",
    "    HAS_KT = True\n",
    "except Exception:\n",
    "    HAS_KT = False\n",
    "\n",
    "warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "\n",
    "RNG = np.random.default_rng(42)\n",
    "MAX_SPLITS = 30  # supports up to 1500m in 50m increments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "579284e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Utilities: time parsing/formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6d2ac62",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "def time_to_seconds(t: str | float | int) -> float:\n",
    "    \"\"\"Parse time string 'mm:ss.xx' or 'ss.xx' to seconds. Pass through floats/ints.\"\"\"\n",
    "    if isinstance(t, (int, float)):\n",
    "        return float(t)\n",
    "    if not isinstance(t, str):\n",
    "        return np.nan\n",
    "    t = t.strip()\n",
    "    if not t:\n",
    "        return np.nan\n",
    "    # Replace commas with dots\n",
    "    t = t.replace(',', '.')\n",
    "    try:\n",
    "        if ':' in t:\n",
    "            mins, secs = t.split(':', 1)\n",
    "            return float(mins) * 60.0 + float(secs)\n",
    "        return float(t)\n",
    "    except Exception:\n",
    "        return np.nan\n",
    "\n",
    "\n",
    "def seconds_to_time_str(s: float) -> str:\n",
    "    \"\"\"Format seconds into 'M:SS.ss'.\"\"\"\n",
    "    if s is None or np.isnan(s):\n",
    "        return \"\"\n",
    "    m = int(s // 60)\n",
    "    rem = s - 60 * m\n",
    "    return f\"{m}:{rem:05.2f}\"\n",
    "\n",
    "\n",
    "def parse_splits_field(val: str | list | tuple) -> List[float]:\n",
    "    \"\"\"Parse splits either from semicolon-separated string or list-like.\"\"\"\n",
    "    if val is None:\n",
    "        return []\n",
    "    if isinstance(val, (list, tuple, np.ndarray)):\n",
    "        return [float(x) for x in val]\n",
    "    if isinstance(val, str):\n",
    "        raw = [x.strip() for x in val.replace(',', ';').split(';') if x.strip()]\n",
    "        out = []\n",
    "        for x in raw:\n",
    "            out.append(time_to_seconds(x))\n",
    "        return out\n",
    "    return []\n",
    "\n",
    "\n",
    "def aggregate_splits(splits_50: List[float], interval: int = 100) -> List[float]:\n",
    "    \"\"\"Aggregate per-50 splits to per-100 splits if requested.\"\"\"\n",
    "    if interval == 50:\n",
    "        return splits_50\n",
    "    if interval == 100:\n",
    "        out = []\n",
    "        for i in range(0, len(splits_50), 2):\n",
    "            s = splits_50[i:i+2]\n",
    "            out.append(sum(s))\n",
    "        return out\n",
    "    raise ValueError(\"interval must be 50 or 100\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22259a67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Data ingestion & feature building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41521284",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "STROKE_ALIASES = {\n",
    "    'FREE': 'Free',\n",
    "    'FREESTYLE': 'Free',\n",
    "    'BK': 'Back', 'BACK': 'Back', 'BACKSTROKE': 'Back',\n",
    "    'BR': 'Breast', 'BREAST': 'Breast', 'BREASTSTROKE': 'Breast',\n",
    "    'FLY': 'Fly', 'BUTTERFLY': 'Fly',\n",
    "    'IM': 'IM', 'INDIVIDUAL MEDLEY': 'IM'\n",
    "}\n",
    "\n",
    "POOL_ALIASES = {'LCM': 'LCM', 'SCM': 'SCM', 'SCY': 'SCY', 'LC': 'LCM', 'SC': 'SCM'}\n",
    "\n",
    "\n",
    "def infer_stroke_from_filename(path: str) -> Optional[str]:\n",
    "    name = os.path.basename(path).lower()\n",
    "    for key, norm in [('free', 'Free'), ('bk', 'Back'), ('back', 'Back'),\n",
    "                      ('br', 'Breast'), ('breast', 'Breast'),\n",
    "                      ('fly', 'Fly'),\n",
    "                      ('im', 'IM')]:\n",
    "        if key in name:\n",
    "            return norm\n",
    "    return None\n",
    "\n",
    "\n",
    "def clean_stroke(val: str | None, fallback: Optional[str] = None) -> Optional[str]:\n",
    "    if isinstance(val, str):\n",
    "        up = val.strip().upper()\n",
    "        return STROKE_ALIASES.get(up, val.strip().title())\n",
    "    return fallback\n",
    "\n",
    "\n",
    "def clean_pool(val: str | None) -> Optional[str]:\n",
    "    if isinstance(val, str):\n",
    "        up = val.strip().upper()\n",
    "        return POOL_ALIASES.get(up, val.strip().upper())\n",
    "    return None\n",
    "\n",
    "\n",
    "def infer_distance(row: pd.Series) -> Optional[int]:\n",
    "    # Priority: 'Distance' numeric; else extract from 'Event' like '200 Fly'\n",
    "    for col in ['Distance', 'distance', 'DISTANCE']:\n",
    "        if col in row and pd.notnull(row[col]):\n",
    "            try:\n",
    "                return int(float(row[col]))\n",
    "            except Exception:\n",
    "                pass\n",
    "    for col in ['Event', 'event', 'EVENT']:\n",
    "        if col in row and isinstance(row[col], str):\n",
    "            m = re.search(r'(\\d+)', row[col])\n",
    "            if m:\n",
    "                try:\n",
    "                    return int(m.group(1))\n",
    "                except Exception:\n",
    "                    pass\n",
    "    # Fallback: from splits length * 50\n",
    "    if 'splits_50' in row and isinstance(row['splits_50'], list) and len(row['splits_50']) > 0:\n",
    "        return len(row['splits_50']) * 50\n",
    "    return None\n",
    "\n",
    "\n",
    "def extract_splits_from_columns(df: pd.DataFrame) -> List[List[float]]:\n",
    "    \"\"\"Try to extract splits from a variety of column patterns.\"\"\"\n",
    "    potential_split_cols = [c for c in df.columns if re.fullmatch(r'(?:\\D*?)?(\\d{2,3})[_ ]?\\d*', c.strip())]\n",
    "    # Common names\n",
    "    common_names = [c for c in df.columns if c.strip().lower() in ('splits', 'split', 'splits_50', 'per_50_splits')]\n",
    "    splits = []\n",
    "    if common_names:\n",
    "        for _, r in df.iterrows():\n",
    "            first = None\n",
    "            for cn in common_names:\n",
    "                if pd.notnull(r.get(cn)):\n",
    "                    first = r.get(cn)\n",
    "                    break\n",
    "            splits.append(parse_splits_field(first))\n",
    "        return splits\n",
    "\n",
    "    # Try numeric sequence columns (50_1, 50_2, ...)\n",
    "    seq_cols = [c for c in df.columns if re.match(r'^(50|100|split_?\\d+|s\\d+|p\\d+)', c.strip().lower())]\n",
    "    if seq_cols:\n",
    "        # Preserve original order\n",
    "        seq_cols_sorted = sorted(seq_cols, key=lambda x: (len(x), x))\n",
    "        for _, r in df.iterrows():\n",
    "            vals = []\n",
    "            for c in seq_cols_sorted:\n",
    "                v = r.get(c)\n",
    "                if pd.isnull(v):\n",
    "                    continue\n",
    "                vals.append(time_to_seconds(str(v)))\n",
    "            splits.append(vals)\n",
    "        return splits\n",
    "\n",
    "    # Nothing obvious\n",
    "    return [[] for _ in range(len(df))]\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PreparedData:\n",
    "    X: pd.DataFrame\n",
    "    Y: np.ndarray  # shape (n_samples, n_splits)\n",
    "    feature_cols: List[str]\n",
    "    cat_cols: List[str]\n",
    "    n_splits: int\n",
    "\n",
    "\n",
    "def prepare_training_frame(df: pd.DataFrame, default_stroke: Optional[str] = None) -> pd.DataFrame:\n",
    "    cols = {c: c.strip() for c in df.columns}\n",
    "    df = df.rename(columns=cols)\n",
    "    # Standardize key columns if present\n",
    "    if 'Stroke' not in df.columns:\n",
    "        df['Stroke'] = default_stroke\n",
    "    df['Stroke'] = df['Stroke'].apply(lambda s: clean_stroke(s, default_stroke))\n",
    "    if 'Pool' in df.columns:\n",
    "        df['Pool'] = df['Pool'].apply(clean_pool)\n",
    "    else:\n",
    "        df['Pool'] = None\n",
    "\n",
    "    # Time columns\n",
    "    time_col_candidates = [c for c in df.columns if c.lower() in ('final time in seconds', 'finaltime', 'time', 'final_time', 'final_time_s')]\n",
    "    df['splits_50'] = extract_splits_from_columns(df)\n",
    "    # distance\n",
    "    df['Distance_m'] = df.apply(infer_distance, axis=1)\n",
    "\n",
    "    # total time\n",
    "    if time_col_candidates:\n",
    "        # use first available non-null\n",
    "        def _pick_time(r):\n",
    "            for c in time_col_candidates:\n",
    "                v = r.get(c)\n",
    "                if pd.notnull(v):\n",
    "                    return time_to_seconds(v)\n",
    "            return np.nan\n",
    "        df['TotalTime_s'] = df.apply(_pick_time, axis=1).astype(float)\n",
    "    else:\n",
    "        df['TotalTime_s'] = np.nan\n",
    "\n",
    "    # If TotalTime missing, compute from splits\n",
    "    def _fix_total(r):\n",
    "        if pd.notnull(r['TotalTime_s']) and r['TotalTime_s'] > 0:\n",
    "            return r['TotalTime_s']\n",
    "        if isinstance(r['splits_50'], list) and len(r['splits_50']) > 0:\n",
    "            return float(sum(r['splits_50']))\n",
    "        return np.nan\n",
    "    df['TotalTime_s'] = df.apply(_fix_total, axis=1)\n",
    "\n",
    "    # Filter rows\n",
    "    def _valid_row(r):\n",
    "        if pd.isnull(r['Distance_m']) or pd.isnull(r['TotalTime_s']) or r['TotalTime_s'] <= 0:\n",
    "            return False\n",
    "        ns = int(r['Distance_m'] // 50)\n",
    "        return isinstance(r['splits_50'], list) and len(r['splits_50']) == ns\n",
    "    df = df[df.apply(_valid_row, axis=1)].reset_index(drop=True)\n",
    "\n",
    "    # pb50 estimate\n",
    "    df['PB50_est_s'] = df['splits_50'].apply(lambda s: float(np.nanmin(s)) if len(s) > 0 else np.nan)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def make_features_and_targets(df: pd.DataFrame, n_splits: int) -> PreparedData:\n",
    "    work = df[df['Distance_m'] // 50 == n_splits].copy()\n",
    "    if work.empty:\n",
    "        raise ValueError(f\"No rows for n_splits={n_splits}\")\n",
    "    # Targets: normalized split fractions of length n_splits\n",
    "    def _fractions(splits):\n",
    "        arr = np.array(splits, dtype=float)\n",
    "        total = arr.sum() if arr.sum() > 0 else 1.0\n",
    "        return (arr / total).tolist()\n",
    "    work['fractions'] = work['splits_50'].apply(_fractions)\n",
    "    Y = np.stack(work['fractions'].values, axis=0)  # shape (N, n_splits)\n",
    "\n",
    "    # Features\n",
    "    work['Pool'] = work['Pool'].fillna('UNK')\n",
    "    X = work[['Distance_m', 'Stroke', 'Pool', 'PB50_est_s', 'TotalTime_s']].copy()\n",
    "    feature_cols = ['Distance_m', 'PB50_est_s', 'TotalTime_s']\n",
    "    cat_cols = ['Stroke', 'Pool']\n",
    "\n",
    "    return PreparedData(X=X, Y=Y, feature_cols=feature_cols, cat_cols=cat_cols, n_splits=n_splits)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acf2f311",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Model builders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2f07bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "def build_automl_model() -> List[Tuple[str, object]]:\n",
    "    \"\"\"Return a list of (name, estimator) candidates to try; all wrapped later in MultiOutputRegressor.\"\"\"\n",
    "    cands = [\n",
    "        (\"ridge\", Ridge(alpha=1.0)),\n",
    "        (\"rf\", RandomForestRegressor(n_estimators=500, random_state=42)),\n",
    "        (\"etr\", ExtraTreesRegressor(n_estimators=600, random_state=42)),\n",
    "        (\"gbr\", GradientBoostingRegressor(random_state=42))\n",
    "    ]\n",
    "    if HAS_XGB:\n",
    "        cands.append((\"xgb\", xgb.XGBRegressor(\n",
    "            n_estimators=800, max_depth=6, learning_rate=0.05, subsample=0.9, colsample_bytree=0.9,\n",
    "            tree_method=\"hist\", random_state=42, n_jobs=-1)))\n",
    "    if HAS_LGBM:\n",
    "        cands.append((\"lgbm\", lgb.LGBMRegressor(\n",
    "            n_estimators=1000, num_leaves=63, learning_rate=0.05, subsample=0.9, colsample_bytree=0.9,\n",
    "            random_state=42)))\n",
    "    if HAS_CAT:\n",
    "        cands.append((\"cat\", CatBoostRegressor(\n",
    "            iterations=1200, depth=6, learning_rate=0.05, loss_function='MAE', random_seed=42, verbose=False)))\n",
    "    return cands\n",
    "\n",
    "\n",
    "def fit_best_automl(X: pd.DataFrame, Y: np.ndarray, feature_cols: List[str], cat_cols: List[str]) -> Tuple[str, Pipeline]:\n",
    "    \"\"\"Fit several regressors with CV and pick the best MAE. Return (name, pipeline).\"\"\"\n",
    "    pre = ColumnTransformer([\n",
    "        (\"num\", StandardScaler(), feature_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "    ])\n",
    "    best_name, best_score, best_pipe = None, float(\"inf\"), None\n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "\n",
    "    for name, est in build_automl_model():\n",
    "        model = MultiOutputRegressor(est)\n",
    "        pipe = Pipeline([(\"pre\", pre), (\"model\", model)])\n",
    "        # Negative MAE -> higher is better; we store MAE\n",
    "        scores = -cross_val_score(pipe, X, Y, cv=kf, scoring=\"neg_mean_absolute_error\", n_jobs=None)\n",
    "        mae = float(scores.mean())\n",
    "        if mae < best_score:\n",
    "            best_name, best_score, best_pipe = name, mae, pipe\n",
    "\n",
    "    if best_pipe is None:\n",
    "        # Safety fallback\n",
    "        est = MultiOutputRegressor(Ridge())\n",
    "        best_pipe = Pipeline([(\"pre\", pre), (\"model\", est)])\n",
    "        best_name = \"ridge\"\n",
    "        best_pipe.fit(X, Y)\n",
    "        return best_name, best_pipe\n",
    "\n",
    "    best_pipe.fit(X, Y)\n",
    "    return best_name, best_pipe\n",
    "\n",
    "\n",
    "def build_keras_model(input_dim: int, output_dim: int, hp: Optional[\"kt.HyperParameters\"] = None) -> \"keras.Model\":\n",
    "    \"\"\"Create a simple MLP regressor in Keras. If hp provided (keras-tuner), tune width/depth/dropout.\"\"\"\n",
    "    if not HAS_TF:\n",
    "        raise RuntimeError(\"TensorFlow/Keras not available\")\n",
    "\n",
    "    if hp is None:\n",
    "        width = 256\n",
    "        depth = 3\n",
    "        dropout = 0.1\n",
    "        lr = 1e-3\n",
    "    else:\n",
    "        width = hp.Int(\"width\", min_value=128, max_value=512, step=64)\n",
    "        depth = hp.Int(\"depth\", min_value=2, max_value=5, step=1)\n",
    "        dropout = hp.Float(\"dropout\", min_value=0.0, max_value=0.4, step=0.05)\n",
    "        lr = hp.Choice(\"lr\", values=[1e-4, 5e-4, 1e-3])\n",
    "\n",
    "    inputs = keras.Input(shape=(input_dim,), name=\"features\")\n",
    "    x = inputs\n",
    "    for _ in range(depth):\n",
    "        x = layers.Dense(width, activation=\"relu\")(x)\n",
    "        if dropout > 0:\n",
    "            x = layers.Dropout(dropout)(x)\n",
    "    outputs = layers.Dense(output_dim, activation=\"softmax\", name=\"fractions\")(x)\n",
    "    model = keras.Model(inputs=inputs, outputs=outputs)\n",
    "    model.compile(optimizer=keras.optimizers.Adam(learning_rate=lr), loss=\"mae\", metrics=[\"mae\"])\n",
    "    return model\n",
    "\n",
    "\n",
    "def fit_best_deep_model(X: pd.DataFrame, Y: np.ndarray, feature_cols: List[str], cat_cols: List[str]) -> Tuple[str, object, ColumnTransformer]:\n",
    "    \"\"\"\n",
    "    Train a deep model. Preferred: Keras with (optional) keras-tuner.\n",
    "    Fallback: sklearn MLPRegressor with basic hyperparameter search.\n",
    "    Returns (impl_name, trained_model, preprocessor) where preprocessor encodes inputs for the model.\n",
    "    \"\"\"\n",
    "    pre = ColumnTransformer([\n",
    "        (\"num\", StandardScaler(), feature_cols),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown=\"ignore\"), cat_cols)\n",
    "    ])\n",
    "    X_enc = pre.fit_transform(X)\n",
    "    input_dim = X_enc.shape[1]\n",
    "    output_dim = Y.shape[1]\n",
    "\n",
    "    if HAS_TF:\n",
    "        # Keras path\n",
    "        if HAS_KT:\n",
    "            tuner = kt.RandomSearch(\n",
    "                lambda hp: build_keras_model(input_dim, output_dim, hp),\n",
    "                objective=\"val_mae\",\n",
    "                max_trials=12,\n",
    "                executions_per_trial=1,\n",
    "                overwrite=True,\n",
    "                directory=\".kerastuner\",\n",
    "                project_name=f\"pacing_ns{output_dim}\"\n",
    "            )\n",
    "            X_tr, X_va, Y_tr, Y_va = train_test_split(X_enc, Y, test_size=0.2, random_state=42)\n",
    "            stop = keras.callbacks.EarlyStopping(monitor=\"val_mae\", patience=15, restore_best_weights=True)\n",
    "            tuner.search(X_tr, Y_tr, validation_data=(X_va, Y_va), epochs=200, batch_size=64, callbacks=[stop], verbose=0)\n",
    "            model = tuner.get_best_models(num_models=1)[0]\n",
    "            impl = \"keras_tuner\"\n",
    "        else:\n",
    "            model = build_keras_model(input_dim, output_dim, hp=None)\n",
    "            stop = keras.callbacks.EarlyStopping(monitor=\"val_mae\", patience=20, restore_best_weights=True)\n",
    "            model.fit(X_enc, Y, epochs=250, batch_size=64, validation_split=0.2, callbacks=[stop], verbose=0)\n",
    "            impl = \"keras\"\n",
    "        return impl, model, pre\n",
    "\n",
    "    # Fallback: sklearn MLP\n",
    "    param_grid = {\n",
    "        \"hidden_layer_sizes\": [(256, 256), (256, 128), (128, 128, 64)],\n",
    "        \"activation\": [\"relu\"],\n",
    "        \"alpha\": [1e-5, 1e-4, 1e-3],\n",
    "        \"learning_rate_init\": [1e-3, 5e-4],\n",
    "        \"max_iter\": [1000],\n",
    "        \"random_state\": [42]\n",
    "    }\n",
    "    best_model = None\n",
    "    best_mae = float(\"inf\")\n",
    "    for hls in param_grid[\"hidden_layer_sizes\"]:\n",
    "        for alpha in param_grid[\"alpha\"]:\n",
    "            for lr in param_grid[\"learning_rate_init\"]:\n",
    "                mlp = MLPRegressor(hidden_layer_sizes=hls, activation=\"relu\", alpha=alpha,\n",
    "                                   learning_rate_init=lr, max_iter=1000, random_state=42)\n",
    "                model = MultiOutputRegressor(mlp)\n",
    "                kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "                scores = -cross_val_score(Pipeline([(\"model\", model)]), X_enc, Y, cv=kf,\n",
    "                                          scoring=\"neg_mean_absolute_error\")\n",
    "                mae = float(scores.mean())\n",
    "                if mae < best_mae:\n",
    "                    best_mae, best_model = mae, model\n",
    "    # Fit on full data\n",
    "    best_model.fit(X_enc, Y)\n",
    "    return \"sklearn_mlp\", best_model, pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8226c96c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Persistence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebc0b396",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "def _ns_model_paths(models_dir: str, n_splits: int) -> Dict[str, str]:\n",
    "    os.makedirs(models_dir, exist_ok=True)\n",
    "    return {\n",
    "        \"automl\": os.path.join(models_dir, f\"automl_ns{n_splits}.joblib\"),\n",
    "        \"dl_model\": os.path.join(models_dir, f\"dl_ns{n_splits}.keras\" if HAS_TF else f\"dl_ns{n_splits}.joblib\"),\n",
    "        \"dl_prep\": os.path.join(models_dir, f\"dl_prep_ns{n_splits}.joblib\"),\n",
    "        \"meta\": os.path.join(models_dir, f\"meta_ns{n_splits}.json\")\n",
    "    }\n",
    "\n",
    "\n",
    "def save_automl(pipe: Pipeline, path: str):\n",
    "    joblib.dump(pipe, path)\n",
    "\n",
    "\n",
    "def load_automl(path: str) -> Pipeline:\n",
    "    return joblib.load(path)\n",
    "\n",
    "\n",
    "def save_dl(model, preproc: ColumnTransformer, model_path: str, prep_path: str, impl_name: str):\n",
    "    if HAS_TF and isinstance(model, tf.keras.Model):\n",
    "        model.save(model_path, include_optimizer=False)\n",
    "    else:\n",
    "        joblib.dump(model, model_path)\n",
    "    joblib.dump({\"pre\": preproc, \"impl\": impl_name}, prep_path)\n",
    "\n",
    "\n",
    "def load_dl(model_path: str, prep_path: str):\n",
    "    meta = joblib.load(prep_path)\n",
    "    pre = meta[\"pre\"]\n",
    "    impl = meta[\"impl\"]\n",
    "    if HAS_TF and os.path.isdir(model_path):\n",
    "        model = tf.keras.models.load_model(model_path, compile=False)\n",
    "        # compile for inference loss if needed\n",
    "        model.compile(optimizer=\"adam\", loss=\"mae\")\n",
    "    else:\n",
    "        model = joblib.load(model_path)\n",
    "    return impl, model, pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f0dc83d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Training orchestration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51df2f8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "def read_any_csv(path: str) -> pd.DataFrame:\n",
    "    try:\n",
    "        return pd.read_csv(path)\n",
    "    except UnicodeDecodeError:\n",
    "        return pd.read_csv(path, encoding=\"latin1\")\n",
    "    except Exception as e:\n",
    "        raise\n",
    "\n",
    "\n",
    "def concat_datasets(paths: List[str]) -> pd.DataFrame:\n",
    "    frames = []\n",
    "    for p in paths:\n",
    "        if not os.path.exists(p):\n",
    "            print(f\"[WARN] Missing dataset file: {p}\")\n",
    "            continue\n",
    "        df = read_any_csv(p)\n",
    "        stroke_hint = infer_stroke_from_filename(p)\n",
    "        frames.append(prepare_training_frame(df, default_stroke=stroke_hint))\n",
    "    if not frames:\n",
    "        raise RuntimeError(\"No valid datasets found.\")\n",
    "    full = pd.concat(frames, ignore_index=True)\n",
    "    # sanity: only 50m-based distances\n",
    "    full = full[full['Distance_m'] % 50 == 0].reset_index(drop=True)\n",
    "    return full\n",
    "\n",
    "\n",
    "def train_all_models(data_paths: List[str], out_dir: str = \"models\") -> Dict[int, Dict[str, str]]:\n",
    "    \"\"\"\n",
    "    Train AutoML + Deep Learning models per n_splits. Returns dict of n_splits -> saved paths.\n",
    "    \"\"\"\n",
    "    df = concat_datasets(data_paths)\n",
    "    results = {}\n",
    "    for n in sorted(df['Distance_m'].unique() // 50):\n",
    "        try:\n",
    "            prep = make_features_and_targets(df, n_splits=int(n))\n",
    "        except ValueError:\n",
    "            continue\n",
    "\n",
    "        # AutoML\n",
    "        best_name, automl_pipe = fit_best_automl(prep.X, prep.Y, prep.feature_cols, prep.cat_cols)\n",
    "\n",
    "        # Deep\n",
    "        dl_impl, dl_model, dl_pre = fit_best_deep_model(prep.X, prep.Y, prep.feature_cols, prep.cat_cols)\n",
    "\n",
    "        # Save\n",
    "        paths = _ns_model_paths(out_dir, int(n))\n",
    "        save_automl(automl_pipe, paths[\"automl\"])\n",
    "        save_dl(dl_model, dl_pre, paths[\"dl_model\"], paths[\"dl_prep\"], dl_impl)\n",
    "\n",
    "        meta = {\n",
    "            \"n_splits\": int(n),\n",
    "            \"feature_cols\": prep.feature_cols,\n",
    "            \"cat_cols\": prep.cat_cols,\n",
    "            \"automl_best\": best_name,\n",
    "            \"dl_impl\": dl_impl\n",
    "        }\n",
    "        with open(paths[\"meta\"], \"w\") as f:\n",
    "            json.dump(meta, f, indent=2)\n",
    "        results[int(n)] = paths\n",
    "        print(f\"[OK] Trained n_splits={n}: AutoML={best_name}, DL={dl_impl}\")\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1ef9a4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Inference utilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ddb50b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "def _encode_inputs(dfrow: pd.DataFrame, pre: ColumnTransformer) -> np.ndarray:\n",
    "    return pre.transform(dfrow)\n",
    "\n",
    "\n",
    "def _predict_fractions_automl(n_splits: int, xrow: pd.DataFrame, models_dir: str) -> np.ndarray:\n",
    "    paths = _ns_model_paths(models_dir, n_splits)\n",
    "    pipe = load_automl(paths[\"automl\"])\n",
    "    fracs = pipe.predict(xrow)[0]\n",
    "    # normalize just in case\n",
    "    fracs = np.maximum(0, np.array(fracs, dtype=float))\n",
    "    s = fracs.sum()\n",
    "    if s <= 0:\n",
    "        fracs = np.ones(n_splits) / n_splits\n",
    "    else:\n",
    "        fracs = fracs / s\n",
    "    return fracs\n",
    "\n",
    "\n",
    "def _predict_fractions_dl(n_splits: int, xrow: pd.DataFrame, models_dir: str) -> np.ndarray:\n",
    "    paths = _ns_model_paths(models_dir, n_splits)\n",
    "    impl, model, pre = load_dl(paths[\"dl_model\"], paths[\"dl_prep\"])\n",
    "    X_enc = _encode_inputs(xrow, pre)\n",
    "    if HAS_TF and isinstance(model, tf.keras.Model):\n",
    "        fracs = model.predict(X_enc, verbose=0)[0]\n",
    "    else:\n",
    "        fracs = model.predict(X_enc)[0]\n",
    "    fracs = np.maximum(0, np.array(fracs, dtype=float))\n",
    "    s = fracs.sum()\n",
    "    if s <= 0:\n",
    "        fracs = np.ones(n_splits) / n_splits\n",
    "    else:\n",
    "        fracs = fracs / s\n",
    "    return fracs\n",
    "\n",
    "\n",
    "def _build_feature_row(distance_m: int, stroke: str, pool: str, pb50_s: float, total_time_s: float) -> pd.DataFrame:\n",
    "    row = pd.DataFrame([{\n",
    "        \"Distance_m\": int(distance_m),\n",
    "        \"Stroke\": clean_stroke(stroke, stroke),\n",
    "        \"Pool\": clean_pool(pool) or \"UNK\",\n",
    "        \"PB50_est_s\": float(pb50_s),\n",
    "        \"TotalTime_s\": float(total_time_s)\n",
    "    }])\n",
    "    return row\n",
    "\n",
    "\n",
    "def _choose_model_kind(kind: str) -> str:\n",
    "    k = (kind or \"auto\").strip().lower()\n",
    "    if k in (\"auto\", \"automl\", \"ml\"):\n",
    "        return \"automl\"\n",
    "    if k in (\"dl\", \"deep\", \"deep_learning\", \"nn\"):\n",
    "        return \"dl\"\n",
    "    return \"automl\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58cc6fa7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Public API"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfa97958",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "def pre_race_optimal_splits(distance_m: int, stroke: str, pool: str, pb50_s: float, target_time_s: float,\n",
    "                            model_kind: str = \"auto\", models_dir: str = \"models\") -> Dict:\n",
    "    \"\"\"\n",
    "    Predict optimal per-50 splits for a given event and target time.\n",
    "    Returns dict with splits_50 (list), splits_100 (list), and formatted strings.\n",
    "    \"\"\"\n",
    "    assert distance_m % 50 == 0, \"distance_m must be multiple of 50\"\n",
    "    n_splits = int(distance_m // 50)\n",
    "    kind = _choose_model_kind(model_kind)\n",
    "    xrow = _build_feature_row(distance_m, stroke, pool, pb50_s, target_time_s)\n",
    "\n",
    "    if kind == \"automl\":\n",
    "        fracs = _predict_fractions_automl(n_splits, xrow, models_dir)\n",
    "    else:\n",
    "        fracs = _predict_fractions_dl(n_splits, xrow, models_dir)\n",
    "\n",
    "    splits_50 = (fracs * float(target_time_s)).tolist()\n",
    "    splits_100 = aggregate_splits(splits_50, interval=100)\n",
    "    return {\n",
    "        \"event\": {\"distance_m\": distance_m, \"stroke\": clean_stroke(stroke), \"pool\": clean_pool(pool) or \"UNK\"},\n",
    "        \"target_time_s\": float(target_time_s),\n",
    "        \"pb50_s\": float(pb50_s),\n",
    "        \"splits_50_s\": splits_50,\n",
    "        \"splits_50_str\": [seconds_to_time_str(s) for s in splits_50],\n",
    "        \"splits_100_s\": splits_100,\n",
    "        \"splits_100_str\": [seconds_to_time_str(s) for s in splits_100]\n",
    "    }\n",
    "\n",
    "\n",
    "def post_race_analysis(distance_m: int, stroke: str, pool: str, splits: List[float], split_interval: int = 50,\n",
    "                       model_kind: str = \"auto\", target_time_s: Optional[float] = None,\n",
    "                       models_dir: str = \"models\") -> Dict:\n",
    "    \"\"\"\n",
    "    Analyze a finished race. You can pass per-50 or per-100 splits via 'split_interval'.\n",
    "    - Predicts an \"optimal\" split profile for your total time using the trained model.\n",
    "    - Compares actual vs. optimal per-50, and aggregates to per-100 as needed.\n",
    "    Returns: dict with deltas and suggestions.\n",
    "    \"\"\"\n",
    "    assert distance_m % 50 == 0, \"distance_m must be multiple of 50\"\n",
    "    if split_interval not in (50, 100):\n",
    "        raise ValueError(\"split_interval must be 50 or 100\")\n",
    "\n",
    "    # Normalize to per-50 splits\n",
    "    n_splits = int(distance_m // 50)\n",
    "    if split_interval == 100:\n",
    "        # expand per-100 to per-50 (assume even split within each 100m for expansion)\n",
    "        splits50 = []\n",
    "        for s100 in splits:\n",
    "            splits50.extend([float(s100)/2.0, float(s100)/2.0])\n",
    "    else:\n",
    "        splits50 = [float(x) for x in splits]\n",
    "\n",
    "    if len(splits50) != n_splits:\n",
    "        raise ValueError(f\"Expected {n_splits} splits of {50}m; got {len(splits50)} based on split_interval={split_interval}\")\n",
    "\n",
    "    total_time = float(sum(splits50)) if target_time_s is None else float(target_time_s)\n",
    "    pb50_est = float(np.nanmin(splits50)) if splits50 else 0.0\n",
    "\n",
    "    kind = _choose_model_kind(model_kind)\n",
    "    xrow = _build_feature_row(distance_m, stroke, pool, pb50_est, total_time)\n",
    "\n",
    "    if kind == \"automl\":\n",
    "        fracs_opt = _predict_fractions_automl(n_splits, xrow, models_dir)\n",
    "    else:\n",
    "        fracs_opt = _predict_fractions_dl(n_splits, xrow, models_dir)\n",
    "\n",
    "    opt_50 = (fracs_opt * total_time).tolist()\n",
    "    deltas_50 = [a - o for a, o in zip(splits50, opt_50)]  # +ve = slower than optimal; -ve = faster than optimal\n",
    "\n",
    "    # Simple textual suggestions\n",
    "    suggestions = []\n",
    "    for i, d in enumerate(deltas_50, start=1):\n",
    "        if d > 0.20:\n",
    "            suggestions.append(f\"50m #{i}: You were {d:.2f}s slower than model-optimal. Aim to pick up pace here.\")\n",
    "        elif d < -0.20:\n",
    "            suggestions.append(f\"50m #{i}: You were {-d:.2f}s faster than model-optimal. Consider redistributing effort.\")\n",
    "        else:\n",
    "            suggestions.append(f\"50m #{i}: Close to optimal (+/-0.20s).\")\n",
    "\n",
    "    # Aggregate to per-100 if needed\n",
    "    actual_100 = aggregate_splits(splits50, interval=100)\n",
    "    optimal_100 = aggregate_splits(opt_50, interval=100)\n",
    "    deltas_100 = [a - o for a, o in zip(actual_100, optimal_100)]\n",
    "\n",
    "    return {\n",
    "        \"event\": {\"distance_m\": distance_m, \"stroke\": clean_stroke(stroke), \"pool\": clean_pool(pool) or \"UNK\"},\n",
    "        \"actual\": {\n",
    "            \"splits_50_s\": splits50,\n",
    "            \"splits_50_str\": [seconds_to_time_str(s) for s in splits50],\n",
    "            \"splits_100_s\": actual_100,\n",
    "            \"splits_100_str\": [seconds_to_time_str(s) for s in actual_100],\n",
    "            \"total_time_s\": sum(splits50),\n",
    "            \"total_time_str\": seconds_to_time_str(sum(splits50))\n",
    "        },\n",
    "        \"optimal\": {\n",
    "            \"splits_50_s\": opt_50,\n",
    "            \"splits_50_str\": [seconds_to_time_str(s) for s in opt_50],\n",
    "            \"splits_100_s\": optimal_100,\n",
    "            \"splits_100_str\": [seconds_to_time_str(s) for s in optimal_100],\n",
    "        },\n",
    "        \"deltas\": {\n",
    "            \"per_50_s\": deltas_50,\n",
    "            \"per_100_s\": deltas_100\n",
    "        },\n",
    "        \"suggestions\": suggestions\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f05435c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "# Convenience: default dataset list & CLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4323d33b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------------------------------\n",
    "DEFAULT_DATASETS = [\n",
    "    # Update paths as needed; these are the names the user uploaded.\n",
    "    \"/mnt/data/Competitive Pacing (Responses) - All time free fastest times.csv\",\n",
    "    \"/mnt/data/Competitive Pacing (Responses) - All time Bk fastest times .csv\",\n",
    "    \"/mnt/data/Competitive Pacing (Responses) - All time Br fastest times.csv\",\n",
    "    \"/mnt/data/Competitive Pacing (Responses) - All time Fly fastest times .csv\",\n",
    "    \"/mnt/data/Competitive Pacing (Responses) - All time IM fastest times.csv\",\n",
    "]\n",
    "\n",
    "\n",
    "def _maybe_train_default():\n",
    "    \"\"\"\n",
    "    Helper for quick training via:\n",
    "        python race_pacing_models.py --train\n",
    "    \"\"\"\n",
    "    print(\"[INFO] Starting training on default dataset list...\")\n",
    "    results = train_all_models(DEFAULT_DATASETS, out_dir=\"models\")\n",
    "    print(json.dumps(results, indent=2))\n",
    "\n",
    "\n",
    "def _demo_inference():\n",
    "    \"\"\"\n",
    "    Quick demo after training:\n",
    "        python race_pacing_models.py --demo\n",
    "    \"\"\"\n",
    "    # Example: 200 Free, LCM, PB50=24.0s, target=110.0s (1:50.00)\n",
    "    out = pre_race_optimal_splits(distance_m=200, stroke=\"Free\", pool=\"LCM\", pb50_s=24.0, target_time_s=110.0,\n",
    "                                  model_kind=\"auto\", models_dir=\"models\")\n",
    "    print(\"[DEMO] Pre-race optimal splits (200 Free):\")\n",
    "    print(json.dumps(out, indent=2))\n",
    "\n",
    "    # Post-race analysis with hypothetical splits @50m\n",
    "    splits_50 = [26.0, 27.5, 28.0, 28.5]\n",
    "    ana = post_race_analysis(distance_m=200, stroke=\"Free\", pool=\"LCM\", splits=splits_50, split_interval=50,\n",
    "                             model_kind=\"auto\", models_dir=\"models\")\n",
    "    print(\"[DEMO] Post-race analysis (200 Free):\")\n",
    "    print(json.dumps(ana, indent=2))\n",
    "\n",
    "\n",
    "# [Notebook] CLI entry point removed; call functions directly in cells."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc3b58cb",
   "metadata": {},
   "source": [
    "## Usage: Train and Infer\n",
    "Run the following cells to train models on your default datasets and perform inference. Make sure the CSVs are present at the paths listed in `DEFAULT_DATASETS`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8aec251",
   "metadata": {},
   "outputs": [],
   "source": [
    "from race_pacing_models import train_all_models, DEFAULT_DATASETS\n",
    "results = train_all_models(DEFAULT_DATASETS, out_dir='models')\n",
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "405b7f8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from race_pacing_models import pre_race_optimal_splits\n",
    "pre = pre_race_optimal_splits(\n",
    "    distance_m=200, stroke='Free', pool='LCM',\n",
    "    pb50_s=24.0, target_time_s=110.0,  # 1:50.00\n",
    "    model_kind='auto', models_dir='models'\n",
    ")\n",
    "pre"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "001ea5a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from race_pacing_models import post_race_analysis\n",
    "post = post_race_analysis(\n",
    "    distance_m=200, stroke='Free', pool='LCM',\n",
    "    splits=[26.0, 27.5, 28.0, 28.5], split_interval=50,\n",
    "    model_kind='auto', models_dir='models'\n",
    ")\n",
    "post"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
