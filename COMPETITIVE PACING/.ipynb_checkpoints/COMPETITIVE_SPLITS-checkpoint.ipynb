{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10c85e26-86f6-40e9-8af4-ce752938f938",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 1 — Imports & constants\n",
    "import os, json, ast, math\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# Base path = same folder as notebook & CSVs\n",
    "BASE = Path(\".\").resolve()\n",
    "\n",
    "# Tunables for realism constraints\n",
    "MAX_CONSEC_DELTA = 0.20      # ±20% cap between consecutive 50s\n",
    "PB_MIN_MARGIN_S  = 0.20      # every split must be at least PB50 + this many seconds\n",
    "TARGET_LOWER_PAD = 0.02      # target >= PB pace * (1+this)\n",
    "TARGET_UPPER_PAD = 1.50      # target <= PB pace * this\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49e44866-3c6b-4239-8d84-555e9c9ea20a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 2 — Utilities: parsing, prep, estimators\n",
    "\n",
    "def time_to_seconds(t):\n",
    "    if t is None: return np.nan\n",
    "    if isinstance(t, (int, float)): return float(t)\n",
    "    s = str(t).strip().replace(\",\", \".\")\n",
    "    if \":\" in s:\n",
    "        try:\n",
    "            m, sec = s.split(\":\", 1)\n",
    "            return float(m)*60 + float(sec)\n",
    "        except:\n",
    "            return np.nan\n",
    "    try:\n",
    "        return float(s)\n",
    "    except:\n",
    "        return np.nan\n",
    "\n",
    "def parse_splits(raw):\n",
    "    \"\"\"Accepts '33.46;35.67;...' or arrays; returns list[float] seconds.\"\"\"\n",
    "    if raw is None or (isinstance(raw, float) and math.isnan(raw)): return []\n",
    "    if isinstance(raw, (list, tuple, np.ndarray, pd.Series)):\n",
    "        out = []\n",
    "        for x in list(raw):\n",
    "            if x is None or (isinstance(x, float) and math.isnan(x)): continue\n",
    "            sx = str(x).strip()\n",
    "            if not sx: continue\n",
    "            parts = [p.strip() for p in sx.replace(\",\", \";\").split(\";\") if p.strip()]\n",
    "            for p in parts:\n",
    "                sec = time_to_seconds(p)\n",
    "                if not math.isnan(sec): out.append(float(sec))\n",
    "        return out\n",
    "    if isinstance(raw, str):\n",
    "        parts = [p.strip() for p in raw.replace(\",\", \";\").split(\";\") if p.strip()]\n",
    "        out = [time_to_seconds(p) for p in parts]\n",
    "        return [float(v) for v in out if not math.isnan(v)]\n",
    "    sec = time_to_seconds(raw)\n",
    "    return [float(sec)] if not math.isnan(sec) else []\n",
    "\n",
    "def normalize_splits(splits):\n",
    "    arr = np.asarray(splits, dtype=float)\n",
    "    tot = arr.sum()\n",
    "    return (arr/tot).tolist() if tot > 0 else []\n",
    "\n",
    "def find_splits_column(df):\n",
    "    for c in df.columns:\n",
    "        if \"split\" in c.lower():\n",
    "            return c\n",
    "    raise ValueError(f\"No 'split' column found. Headers = {list(df.columns)}\")\n",
    "\n",
    "def prepare_training_frame(csv_path: Path):\n",
    "    \"\"\"\n",
    "    Reads a stroke CSV, auto-detects splits column, builds frac_i targets & features.\n",
    "    Returns prepared DataFrame and list of target columns.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(csv_path)\n",
    "\n",
    "    split_col = find_splits_column(df)\n",
    "    df[\"splits_50\"] = df[split_col].apply(parse_splits)\n",
    "\n",
    "    # Distance (infer if missing)\n",
    "    if \"Distance\" not in df.columns:\n",
    "        df[\"Distance\"] = df[\"splits_50\"].apply(lambda s: len(s)*50 if s else np.nan)\n",
    "\n",
    "    # Total time\n",
    "    time_col = None\n",
    "    for c in df.columns:\n",
    "        lc = c.strip().lower()\n",
    "        if lc in (\"final time in seconds\",\"final_time\",\"time\",\"totaltime_s\"):\n",
    "            time_col = c; break\n",
    "    df[\"TotalTime_s\"] = df[time_col] if time_col else df[\"splits_50\"].apply(lambda s: sum(s) if s else np.nan)\n",
    "\n",
    "    # PB50 estimate if missing\n",
    "    if \"PB50_s\" not in df.columns:\n",
    "        df[\"PB50_s\"] = df[\"splits_50\"].apply(lambda s: min(s) if s else np.nan)\n",
    "\n",
    "    # Build frac targets\n",
    "    n_max = df[\"splits_50\"].apply(lambda s: len(s) if isinstance(s, list) else 0).max()\n",
    "    n_max = int(n_max) if not (pd.isna(n_max) or n_max is None) else 0\n",
    "    targets = []\n",
    "    for i in range(1, n_max+1):\n",
    "        col = f\"frac_{i}\"\n",
    "        df[col] = np.nan\n",
    "        targets.append(col)\n",
    "    for ridx, s in df[\"splits_50\"].items():\n",
    "        if isinstance(s, list) and len(s) > 0:\n",
    "            fr = normalize_splits(s)\n",
    "            for j, f in enumerate(fr, 1):\n",
    "                df.at[ridx, f\"frac_{j}\"] = f\n",
    "\n",
    "    # Clean rows with no targets at all\n",
    "    if targets:\n",
    "        df = df.dropna(subset=[targets[0]], how=\"any\")\n",
    "    return df.reset_index(drop=True), targets\n",
    "\n",
    "def make_preprocessor(feat_cols):\n",
    "    num = [c for c in feat_cols if c in [\"Distance\", \"PB50_s\", \"TotalTime_s\"]]\n",
    "    cat = [c for c in feat_cols if c in [\"Stroke\", \"Pool\"]]\n",
    "    return ColumnTransformer([\n",
    "        (\"num\", StandardScaler(), num),\n",
    "        (\"cat\", OneHotEncoder(handle_unknown='ignore'), cat)\n",
    "    ])\n",
    "\n",
    "# Supported estimators (use one winner per stroke)\n",
    "from sklearn.linear_model import Ridge, Lasso\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "def make_estimator(name: str, params: dict):\n",
    "    if name == \"Ridge\": return Ridge(**params)\n",
    "    if name == \"Lasso\": return Lasso(max_iter=5000, **params)\n",
    "    if name == \"RandomForest\": return RandomForestRegressor(**params)\n",
    "    if name == \"GBR\" or name == \"GradientBoosting\": return GradientBoostingRegressor(**params)\n",
    "    if name == \"SVR\": return SVR(**params)\n",
    "    # Fallback\n",
    "    raise ValueError(f\"Unknown model name: {name}\")\n",
    "\n",
    "def parse_best_params(cell):\n",
    "    if isinstance(cell, dict): \n",
    "        return cell\n",
    "    s = str(cell)\n",
    "    try:\n",
    "        return ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        try:\n",
    "            return json.loads(s.replace(\"'\", '\"'))\n",
    "        except Exception:\n",
    "            return {}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3974840d-803f-4e73-a6c8-24fb13fed695",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 3 — Pick best model per stroke (Option A)\n",
    "def pick_best_model_from_leaderboard(leaderboard_csv: Path):\n",
    "    \"\"\"\n",
    "    Accepts different leaderboard schemas:\n",
    "      - columns may include: ['Model','BestParams','R2']          (our earlier export)\n",
    "      - OR: ['model','params','mean_test_score']                  (alt schema)\n",
    "    Returns (model_name, params_dict).\n",
    "    \"\"\"\n",
    "    lb = pd.read_csv(leaderboard_csv)\n",
    "\n",
    "    # Normalize column names to lowercase for detection\n",
    "    lower_cols = {c.lower(): c for c in lb.columns}\n",
    "\n",
    "    # Case 1: R2\n",
    "    if \"r2\" in lower_cols and \"model\" in lower_cols:\n",
    "        model_col = lower_cols[\"model\"]\n",
    "        r2_col = lower_cols[\"r2\"]\n",
    "        params_col = lower_cols.get(\"bestparams\") or lower_cols.get(\"params\")\n",
    "        lb_sorted = lb.sort_values(r2_col, ascending=False)\n",
    "        row = lb_sorted.iloc[0]\n",
    "        model = str(row[model_col]).strip()\n",
    "        params = parse_best_params(row[params_col]) if params_col else {}\n",
    "        # strip \"model__\" prefixes if any\n",
    "        params = {k.split(\"model__\")[-1]: v for k, v in params.items()}\n",
    "        return model, params\n",
    "\n",
    "    # Case 2: mean_test_score\n",
    "    if \"mean_test_score\" in lower_cols and \"model\" in lower_cols:\n",
    "        model_col = lower_cols[\"model\"]\n",
    "        score_col = lower_cols[\"mean_test_score\"]\n",
    "        params_col = lower_cols.get(\"params\") or lower_cols.get(\"bestparams\")\n",
    "        lb_sorted = lb.sort_values(score_col, ascending=False)\n",
    "        row = lb_sorted.iloc[0]\n",
    "        model = str(row[model_col]).strip()\n",
    "        params = parse_best_params(row[params_col]) if params_col else {}\n",
    "        params = {k.split(\"model__\")[-1]: v for k, v in params.items()}\n",
    "        return model, params\n",
    "\n",
    "    raise ValueError(f\"Leaderboard {leaderboard_csv} has unsupported columns: {list(lb.columns)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07b57884-fe3a-4967-8f43-c945acef9a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 4 — Train & save per-split models using the stroke's single best estimator\n",
    "def train_stroke_models_optionA(stroke_name: str, leaderboard_csv: Path, data_csv: Path, out_dir: Path):\n",
    "    # Pick one winner model for the stroke\n",
    "    model_name, params = pick_best_model_from_leaderboard(leaderboard_csv)\n",
    "    print(f\"[{stroke_name}] Best model from leaderboard: {model_name}  params={params}\")\n",
    "\n",
    "    # Prepare dataset and frac targets\n",
    "    df, targets = prepare_training_frame(data_csv)\n",
    "    if not targets:\n",
    "        print(f\"[{stroke_name}] No frac targets found in {data_csv}.\")\n",
    "        return\n",
    "\n",
    "    feat_pool = [\"Distance\", \"PB50_s\", \"TotalTime_s\", \"Stroke\", \"Pool\"]\n",
    "    present = [c for c in feat_pool if c in df.columns]\n",
    "    pre = make_preprocessor(present)\n",
    "\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "    manifest = {}\n",
    "    trained = 0\n",
    "\n",
    "    for tgt in targets:\n",
    "        sub = df.dropna(subset=[tgt]).copy()\n",
    "        if sub.empty:\n",
    "            print(f\"[{stroke_name}] skip {tgt}: no rows.\")\n",
    "            continue\n",
    "\n",
    "        X = sub[present]\n",
    "        y = sub[tgt].astype(float).values\n",
    "\n",
    "        est = make_estimator(model_name, params)\n",
    "        pipe = Pipeline([(\"pre\", pre), (\"reg\", est)])\n",
    "        pipe.fit(X, y)\n",
    "\n",
    "        r2 = r2_score(y, pipe.predict(X))\n",
    "        print(f\"[{stroke_name}] {tgt} → R2(train)={r2:.4f}\")\n",
    "\n",
    "        path = out_dir / f\"{stroke_name.lower()}_{tgt}.joblib\"\n",
    "        joblib.dump(pipe, path)\n",
    "        manifest[tgt] = {\"model\": model_name, \"params\": params}\n",
    "        trained += 1\n",
    "\n",
    "    with open(out_dir / \"manifest.json\", \"w\") as f:\n",
    "        json.dump(manifest, f, indent=2)\n",
    "\n",
    "    print(f\"✅ [{stroke_name}] Saved {trained} models to {out_dir}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "516095b0-ef83-4a9c-a22e-f13e3598d18f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 5 — Train ALL strokes (run once to export models)\n",
    "\n",
    "CONFIG = [\n",
    "    (\"Freestyle\",\n",
    "     BASE/\"Freestyle_leaderboard.csv\",\n",
    "     BASE/\"Freestyle_dataset.csv\",\n",
    "     BASE/\"models_freestyle\"),\n",
    "\n",
    "    (\"Backstroke\",\n",
    "     BASE/\"Backstroke_leaderboard.csv\",\n",
    "     BASE/\"Backstroke_dataset.csv\",\n",
    "     BASE/\"models_backstroke\"),\n",
    "\n",
    "    (\"Breaststroke\",\n",
    "     BASE/\"Breaststroke_leaderboard.csv\",\n",
    "     BASE/\"Breaststroke_dataset.csv\",\n",
    "     BASE/\"models_breaststroke\"),\n",
    "\n",
    "    (\"Butterfly\",\n",
    "     BASE/\"Butterfly_leaderboard.csv\",\n",
    "     BASE/\"Butterfly_dataset.csv\",\n",
    "     BASE/\"models_butterfly\"),\n",
    "\n",
    "    (\"IM\",\n",
    "     BASE/\"IM_leaderboard.csv\",\n",
    "     BASE/\"IM_dataset.csv\",\n",
    "     BASE/\"models_im\"),\n",
    "]\n",
    "\n",
    "for stroke, lb_csv, data_csv, out_dir in CONFIG:\n",
    "    print(\"\\n\" + \"=\"*90)\n",
    "    print(f\"Training from leaderboard → {stroke}\")\n",
    "    train_stroke_models_optionA(stroke, lb_csv, data_csv, Path(out_dir))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b26643f-6f77-4534-bd5e-666f83441645",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 6 — Load models per stroke + realism constraints\n",
    "\n",
    "def load_models_for_stroke(stroke: str):\n",
    "    folder = BASE / f\"models_{stroke.lower()}\"\n",
    "    manifest_path = folder / \"manifest.json\"\n",
    "    if not manifest_path.exists():\n",
    "        raise FileNotFoundError(f\"No manifest found for stroke '{stroke}' in {folder}\")\n",
    "\n",
    "    with open(manifest_path) as f:\n",
    "        manifest = json.load(f)\n",
    "    # models keyed by 'frac_i'\n",
    "    models = {t: joblib.load(folder / f\"{stroke.lower()}_{t}.joblib\") for t in manifest.keys()}\n",
    "    return models, manifest\n",
    "\n",
    "def enforce_constraints(splits, pb50, total_time,\n",
    "                        max_delta=MAX_CONSEC_DELTA,\n",
    "                        pb_min_margin_s=PB_MIN_MARGIN_S):\n",
    "    \"\"\"\n",
    "    1) If pb50 is provided (non-IM): each 50m >= PB50 + margin\n",
    "       If pb50 is None (IM): skip PB-based floor entirely.\n",
    "    2) Consecutive 50s vary by at most ±max_delta relative to previous\n",
    "    3) Normalize back to total_time\n",
    "    \"\"\"\n",
    "    s = np.array(splits, dtype=float)\n",
    "\n",
    "    # (1) PB floor only when pb50 is known (non-IM)\n",
    "    if pb50 is not None:\n",
    "        min_allowed = pb50 + pb_min_margin_s\n",
    "        s = np.maximum(s, min_allowed)\n",
    "\n",
    "    # (2) Smooth consecutive jumps\n",
    "    for i in range(1, len(s)):\n",
    "        upper = s[i-1] * (1.0 + max_delta)\n",
    "        lower = s[i-1] * (1.0 - max_delta)\n",
    "        s[i] = min(upper, max(lower, s[i]))\n",
    "\n",
    "    # (3) Normalize to exact total\n",
    "    s *= (total_time / s.sum())\n",
    "\n",
    "    # If we applied PB floor, re-apply and renormalize (small tweak)\n",
    "    if pb50 is not None:\n",
    "        s = np.maximum(s, pb50 + pb_min_margin_s)\n",
    "        s *= (total_time / s.sum())\n",
    "\n",
    "    return s\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8afd0b03-7631-458d-b9b2-0dad0c5e5391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 7 — Pre-race and Post-race functions (robust IM handling + auto PB proxy + auto-plot)\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def _is_im(stroke: str) -> bool:\n",
    "    return str(stroke).strip().upper() == \"IM\"\n",
    "\n",
    "def predict_splits_pre_race(models: dict, stroke: str, distance: int, pb50: float or None, target_time: float):\n",
    "    n = distance // 50\n",
    "    is_im = _is_im(stroke)\n",
    "\n",
    "    # If IM, inject a neutral PB50 feature proxy purely for the model's preprocessor\n",
    "    # (this is NOT used for constraints)\n",
    "    pb50_feature = pb50\n",
    "    if is_im:\n",
    "        pb50_feature = max(0.01, target_time / n)  # simple neutral proxy so StandardScaler has a numeric column\n",
    "\n",
    "    X_dict = {\n",
    "        \"Distance\": distance,\n",
    "        \"TotalTime_s\": target_time,\n",
    "        \"Stroke\": \"IM\" if is_im else str(stroke).strip().title(),\n",
    "        \"Pool\": \"LCM\",\n",
    "        \"PB50_s\": pb50_feature,  # always include for the preprocessor\n",
    "    }\n",
    "    X = pd.DataFrame([X_dict])\n",
    "\n",
    "    # Predict fractions with per-split models (fallback equal if missing)\n",
    "    fracs = []\n",
    "    for i in range(1, n+1):\n",
    "        tgt = f\"frac_{i}\"\n",
    "        if tgt not in models:\n",
    "            fracs.append(1.0 / n)\n",
    "        else:\n",
    "            fr = models[tgt].predict(X)[0]\n",
    "            fracs.append(max(0.0, float(fr)))\n",
    "    fracs = np.array(fracs)\n",
    "    fracs = fracs / fracs.sum() if fracs.sum() > 0 else np.ones(n) / n\n",
    "\n",
    "    raw_splits = fracs * target_time\n",
    "    # Constraints: skip PB floor for IM (pb50=None)\n",
    "    return enforce_constraints(raw_splits, pb50=None if is_im else pb50, total_time=target_time)\n",
    "\n",
    "def analyze_post_race(models: dict, stroke: str, distance: int, pb50: float or None, actual_splits_50: np.ndarray):\n",
    "    n = distance // 50\n",
    "    if len(actual_splits_50) != n:\n",
    "        raise ValueError(f\"Expected {n} x 50m splits, got {len(actual_splits_50)}\")\n",
    "\n",
    "    is_im = _is_im(stroke)\n",
    "    total_time = float(np.sum(actual_splits_50))\n",
    "\n",
    "    # For IM, inject PB50 feature proxy for the preprocessor only\n",
    "    pb50_feature = pb50\n",
    "    if is_im:\n",
    "        pb50_feature = max(0.01, total_time / n)\n",
    "\n",
    "    X_dict = {\n",
    "        \"Distance\": distance,\n",
    "        \"TotalTime_s\": total_time,\n",
    "        \"Stroke\": \"IM\" if is_im else str(stroke).strip().title(),\n",
    "        \"Pool\": \"LCM\",\n",
    "        \"PB50_s\": pb50_feature,  # always included for the preprocessor\n",
    "    }\n",
    "    X = pd.DataFrame([X_dict])\n",
    "\n",
    "    # Predict fractional pattern\n",
    "    fracs = []\n",
    "    for i in range(1, n+1):\n",
    "        tgt = f\"frac_{i}\"\n",
    "        if tgt not in models:\n",
    "            fracs.append(1.0 / n)\n",
    "        else:\n",
    "            fr = models[tgt].predict(X)[0]\n",
    "            fracs.append(max(0.0, float(fr)))\n",
    "    fracs = np.array(fracs)\n",
    "    fracs = fracs / fracs.sum() if fracs.sum() > 0 else np.ones(n) / n\n",
    "\n",
    "    ideal_splits = enforce_constraints(\n",
    "        fracs * total_time,\n",
    "        pb50=None if is_im else pb50,   # still skip PB-based floor for IM\n",
    "        total_time=total_time\n",
    "    )\n",
    "\n",
    "    deltas = actual_splits_50 - ideal_splits\n",
    "    suggestions = []\n",
    "    for i, d in enumerate(deltas, 1):\n",
    "        if d > 0.30:\n",
    "            suggestions.append(f\"50m #{i}: +{d:.2f}s vs ideal — speed up slightly here.\")\n",
    "        elif d < -0.30:\n",
    "            suggestions.append(f\"50m #{i}: {d:.2f}s vs ideal — consider easing earlier effort.\")\n",
    "        else:\n",
    "            suggestions.append(f\"50m #{i}: within ±0.30s of ideal — maintain.\")\n",
    "\n",
    "    # ---- Print summary ----\n",
    "    label_stroke = \"IM\" if is_im else str(stroke).strip().title()\n",
    "    print(f\"\\n=== Post-Race Analysis: {label_stroke} {distance}m ===\")\n",
    "    for i, (a, p, d) in enumerate(zip(actual_splits_50, ideal_splits, deltas), 1):\n",
    "        faster_slower = \"slower\" if d > 0 else \"faster\"\n",
    "        print(f\"50m {i}: Actual {a:.2f} | Ideal {p:.2f} | You were {abs(d):.2f}s {faster_slower}\")\n",
    "    print(\"\\nSuggestions:\")\n",
    "    for s in suggestions:\n",
    "        print(\" - \" + s)\n",
    "    print(f\"\\nTotals — Actual: {total_time:.2f}s | Ideal: {float(np.sum(ideal_splits)):.2f}s\")\n",
    "\n",
    "    # ---- Auto-Plot ----\n",
    "    x = np.arange(1, n+1)\n",
    "    plt.figure(figsize=(8,5))\n",
    "    plt.plot(x, actual_splits_50, marker='o', label=\"Actual Splits\")\n",
    "    plt.plot(x, ideal_splits, marker='s', label=\"Ideal Splits\")\n",
    "    plt.title(f\"{label_stroke} {distance}m — Splits (Actual vs Ideal)\")\n",
    "    plt.xlabel(\"Split (per 50m)\")\n",
    "    plt.ylabel(\"Time (s)\")\n",
    "    plt.xticks(x)\n",
    "    plt.grid(True, linestyle=\"--\", alpha=0.5)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    return {\n",
    "        \"ideal_50\": ideal_splits,\n",
    "        \"delta_50\": deltas,\n",
    "        \"suggestions\": suggestions,\n",
    "        \"total_actual\": total_time,\n",
    "        \"total_ideal\": float(np.sum(ideal_splits))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5dff27-6b9a-4da6-a1a3-6e1e9fd42bfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Block 8 — Interactive CLI (never asks PB for IM)\n",
    "mode_raw   = input(\"Select mode (pre/post): \").strip().lower()\n",
    "stroke_raw = input(\"Enter stroke (Freestyle/Backstroke/Breaststroke/Butterfly/IM): \").strip()\n",
    "\n",
    "# Normalize stroke in three forms:\n",
    "stroke_key   = stroke_raw.strip().lower()        # for folders\n",
    "stroke_label = \"IM\" if stroke_key == \"im\" else stroke_raw.strip().title()  # for display/features\n",
    "is_im        = (stroke_key == \"im\")\n",
    "\n",
    "distance = int(input(\"Enter race distance (e.g., 100, 200, 400): \"))\n",
    "\n",
    "# Load models once (using folder key)\n",
    "models, manifest = load_models_for_stroke(stroke_label)\n",
    "\n",
    "if mode_raw == \"pre\":\n",
    "    # PB only for non-IM\n",
    "    pb50 = None\n",
    "    if not is_im:\n",
    "        pb50 = float(input(\"Enter your 50m personal best (in seconds): \"))\n",
    "\n",
    "    target_time = float(input(\"Enter your target total race time (in seconds): \"))\n",
    "\n",
    "    # Validate only for non-IM (PB-based bounds)\n",
    "    if not is_im:\n",
    "        n = distance // 50\n",
    "        min_possible = pb50 * n * (1.0 + TARGET_LOWER_PAD)\n",
    "        max_reasonable = pb50 * n * TARGET_UPPER_PAD\n",
    "        if target_time < min_possible:\n",
    "            print(f\"\\n[Error] Target {target_time:.2f}s too fast for your PB50 ({pb50:.2f}s). \"\n",
    "                  f\"Minimum allowed ≈ {min_possible:.2f}s.\")\n",
    "        elif target_time > max_reasonable:\n",
    "            print(f\"\\n[Error] Target {target_time:.2f}s too slow. \"\n",
    "                  f\"Maximum allowed ≈ {max_reasonable:.2f}s.\")\n",
    "        else:\n",
    "            splits = predict_splits_pre_race(models, stroke=stroke_label, distance=distance, pb50=pb50, target_time=target_time)\n",
    "            print(f\"\\n=== Optimal Predicted Splits for {stroke_label} {distance}m ===\")\n",
    "            for i, s in enumerate(splits, 1):\n",
    "                print(f\"50m {i}: {s:.2f} s\")\n",
    "            print(f\"Target Total: {target_time:.2f} s | Predicted Sum: {np.sum(splits)::.2f} s\")\n",
    "    else:\n",
    "        # IM: no PB prompt or PB-based validation\n",
    "        splits = predict_splits_pre_race(models, stroke=stroke_label, distance=distance, pb50=None, target_time=target_time)\n",
    "        print(f\"\\n=== Optimal Predicted Splits for {stroke_label} {distance}m ===\")\n",
    "        for i, s in enumerate(splits, 1):\n",
    "            print(f\"50m {i}: {s:.2f} s\")\n",
    "        print(f\"Target Total: {target_time:.2f} s | Predicted Sum: {np.sum(splits):.2f} s\")\n",
    "\n",
    "elif mode_raw == \"post\":\n",
    "    step = int(input(\"Enter split interval (50 or 100): \"))\n",
    "    if step not in (50, 100):\n",
    "        raise ValueError(\"Split interval must be 50 or 100.\")\n",
    "    n = distance // step\n",
    "\n",
    "    # PB only for non-IM\n",
    "    pb50 = None\n",
    "    if not is_im:\n",
    "        pb50 = float(input(\"Enter your 50m personal best (in seconds): \"))\n",
    "\n",
    "    # Collect splits\n",
    "    user = []\n",
    "    print(f\"Enter your actual {step}m splits:\")\n",
    "    for i in range(n):\n",
    "        val = float(input(f\"Split {i+1}: \"))\n",
    "        if step == 100:\n",
    "            user.extend([val/2.0, val/2.0])  # expand to 50s\n",
    "        else:\n",
    "            user.append(val)\n",
    "    user = np.array(user, dtype=float)\n",
    "\n",
    "    rep = analyze_post_race(models, stroke=stroke_label, distance=distance, pb50=pb50, actual_splits_50=user)\n",
    "    print(f\"\\n=== Post-Race Analysis: {stroke_label} {distance}m ===\")\n",
    "    for i, (a, p, d) in enumerate(zip(user, rep['ideal_50'], rep['delta_50']), 1):\n",
    "        faster_slower = \"slower\" if d > 0 else \"faster\"\n",
    "        print(f\"50m {i}: Actual {a:.2f} | Ideal {p:.2f} | You were {abs(d):.2f}s {faster_slower}\")\n",
    "    print(\"\\nSuggestions:\")\n",
    "    for s in rep[\"suggestions\"]:\n",
    "        print(\" - \" + s)\n",
    "    print(f\"\\nTotals — Actual: {rep['total_actual']:.2f}s | Ideal: {rep['total_ideal']:.2f}s\")\n",
    "\n",
    "else:\n",
    "    print(\"Invalid mode. Please choose 'pre' or 'post'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfd4b53-fe50-457d-8472-8e429c61e1d3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
