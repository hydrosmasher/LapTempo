{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# Competitive Pacing \u2014 Jupyter Demo\n\nThis notebook sets up the updated module, creates small dummy datasets, trains models, and demonstrates pre-race split generation and post-race analysis with plots."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Write the module file locally so the notebook is self-contained\nfrom pathlib import Path\nmodule_path = Path('competitive_pacing_updated.py')\nmodule_path.write_text('''\nfrom pathlib import Path\nfrom dataclasses import dataclass\nfrom typing import Dict, List, Optional, Tuple, Any\nimport json\nimport re\n\nimport numpy as np\nimport pandas as pd\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\nfrom sklearn.metrics import mean_absolute_error\nfrom joblib import dump, load\n\nimport matplotlib.pyplot as plt\n\n# ------------------------------\n# Constants & Paths\n# ------------------------------\n\nROOT = Path(\".\").resolve()\nDATA_DIR = ROOT / \"data\"\nMODELS_DIR = ROOT / \"models\"\nMODELS_DIR.mkdir(parents=True, exist_ok=True)\n\nVALID_STROKES = [\"Freestyle\", \"Backstroke\", \"Breaststroke\", \"Butterfly\", \"IM\"]\nSTROKE_KEY = {\n    \"freestyle\": \"FREE\",\n    \"free\": \"FREE\",\n    \"fs\": \"FREE\",\n    \"backstroke\": \"BACK\",\n    \"back\": \"BACK\",\n    \"bk\": \"BACK\",\n    \"breaststroke\": \"BRST\",\n    \"breast\": \"BRST\",\n    \"br\": \"BRST\",\n    \"butterfly\": \"FLY\",\n    \"fly\": \"FLY\",\n    \"im\": \"IM\",\n    \"individual medley\": \"IM\",\n}\n\n# Default model registry (can be overridden per stroke)\nDEFAULT_MODEL_CFG = {\n    \"FREE\": {\"estimator\": \"GBR\", \"params\": {\"n_estimators\": 400, \"learning_rate\": 0.05, \"max_depth\": 3, \"random_state\": 42}},\n    \"BACK\": {\"estimator\": \"GBR\", \"params\": {\"n_estimators\": 400, \"learning_rate\": 0.05, \"max_depth\": 3, \"random_state\": 42}},\n    \"FLY\":  {\"estimator\": \"GBR\", \"params\": {\"n_estimators\": 400, \"learning_rate\": 0.05, \"max_depth\": 3, \"random_state\": 42}},\n    \"BRST\": {\"estimator\": \"GBR\", \"params\": {\"n_estimators\": 400, \"learning_rate\": 0.05, \"max_depth\": 3, \"random_state\": 42}},\n    \"IM\":   {\"estimator\": \"GBR\", \"params\": {\"n_estimators\": 500, \"learning_rate\": 0.04, \"max_depth\": 3, \"random_state\": 42}},\n}\n\ndef norm_stroke_label(s: str) -> str:\n    if s is None:\n        raise ValueError(\"Stroke cannot be None\")\n    s2 = s.strip().lower()\n    if s2 not in STROKE_KEY:\n        for v in [\"freestyle\", \"backstroke\", \"breaststroke\", \"butterfly\", \"im\"]:\n            if s2 == v:\n                return STROKE_KEY[v]\n        raise ValueError(f\"Unknown stroke label: {s}\")\n    return STROKE_KEY[s2]\n\ndef time_to_seconds(t: str) -> float:\n    t = str(t).strip()\n    if re.fullmatch(r\"\\d+(\\.\\d+)?\", t):\n        return float(t)\n    parts = t.split(\":\")\n    if len(parts) == 2:\n        m, s = parts\n        return int(m) * 60 + float(s)\n    elif len(parts) == 3:\n        h, m, s = parts\n        return int(h) * 3600 + int(m) * 60 + float(s)\n    else:\n        raise ValueError(f\"Unrecognized time format: {t}\")\n\ndef seconds_to_time(s: float) -> str:\n    s = float(s)\n    if s < 0:\n        s = 0.0\n    m = int(s // 60)\n    rem = s - m * 60\n    return f\"{m}:{rem:05.2f}\"\n\ndef parse_splits_str(s: str) -> List[float]:\n    if s is None:\n        return []\n    parts = [p.strip() for p in re.split(r\"[,\\s]+\", str(s)) if p.strip()]\n    return [time_to_seconds(p) for p in parts]\n\ndef count_split_columns(df: pd.DataFrame) -> Tuple[int, List[str]]:\n    split_cols = [c for c in df.columns if re.fullmatch(r\"(split|lap)_(\\d+)\", c.strip().lower())]\n    def idx(c):\n        m = re.search(r\"(\\d+)$\", c)\n        return int(m.group(1)) if m else 0\n    split_cols_sorted = sorted(split_cols, key=idx)\n    return len(split_cols_sorted), split_cols_sorted\n\ndef fatigue_coeff(stroke_key: str, distance_m: int) -> float:\n    base = 1.10\n    if distance_m == 100:\n        base = 1.045\n    elif distance_m == 200:\n        base = 1.090\n    elif distance_m == 400:\n        base = 1.150\n    elif distance_m in (800, 1500):\n        base = 1.180\n    tweak = {\n        \"FREE\": 0.000,\n        \"BACK\": 0.005,\n        \"FLY\":  0.007,\n        \"BRST\": 0.015,\n        \"IM\":   0.010,\n    }.get(stroke_key, 0.0)\n    return base + tweak\n\ndef target_time_from_pb50(stroke_key: str, distance_m: int, pb50_sec: float) -> float:\n    n50 = distance_m / 50.0\n    raw = n50 * pb50_sec\n    return float(raw * fatigue_coeff(stroke_key, distance_m))\n\ndef prepare_training_long(df: pd.DataFrame, stroke_key: str) -> Tuple[pd.DataFrame, pd.Series]:\n    if \"distance_m\" not in df.columns:\n        raise ValueError(\"Training data must include 'distance_m' per race.\")\n    n, split_cols = count_split_columns(df)\n    if n == 0:\n        raise ValueError(\"No split columns detected. Use 'split_1', 'split_2', ... or 'lap_1', ...\")\n    rows = []\n    targets = []\n    for _, row in df.iterrows():\n        dist = int(row[\"distance_m\"])\n        expected_laps = int(dist // 50)\n        lap_vals = []\n        for c in split_cols:\n            v = row.get(c, np.nan)\n            if pd.isna(v):\n                break\n            lap_vals.append(float(v))\n        if len(lap_vals) != expected_laps:\n            continue\n        splits = np.array(lap_vals, dtype=float)\n        base = splits.mean()\n        residuals = splits - base\n        n_laps = len(splits)\n        for i, r in enumerate(residuals, start=1):\n            rows.append({\n                \"stroke_key\": stroke_key,\n                \"distance_m\": dist,\n                \"n_laps\": n_laps,\n                \"lap_idx\": i,\n                \"lap_idx_norm\": i / n_laps,\n                \"lap_idx_sq\": (i / n_laps) ** 2,\n            })\n            targets.append(r)\n    X = pd.DataFrame(rows)\n    y = pd.Series(targets, name=\"residual\")\n    if len(X) == 0:\n        raise ValueError(f\"No valid rows produced for stroke {stroke_key}. Check your data.\")\n    return X, y\n\ndef build_estimator(estimator: str, params: Dict[str, Any]):\n    if estimator.upper() == \"GBR\":\n        return GradientBoostingRegressor(**params)\n    elif estimator.upper() == \"RF\":\n        return RandomForestRegressor(**params)\n    else:\n        raise ValueError(f\"Unsupported estimator: {estimator}\")\n\ndef train_stroke_model(stroke: str, df: pd.DataFrame, cfg: Optional[Dict[str, Any]] = None) -> Tuple[Pipeline, Dict[str, Any], float]:\n    key = norm_stroke_label(stroke)\n    cfg = cfg or DEFAULT_MODEL_CFG[key]\n    est = build_estimator(cfg[\"estimator\"], cfg[\"params\"])\n    X, y = prepare_training_long(df, key)\n    pipe = Pipeline([\n        (\"scaler\", StandardScaler(with_mean=True, with_std=True)),\n        (\"est\", est)\n    ])\n    split_idx = int(0.8 * len(X))\n    X_tr, X_va = X.iloc[:split_idx], X.iloc[split_idx:]\n    y_tr, y_va = y.iloc[:split_idx], y.iloc[split_idx:]\n    pipe.fit(X_tr, y_tr)\n    yhat_va = pipe.predict(X_va)\n    mae = float(mean_absolute_error(y_va, yhat_va))\n    meta = {\"stroke_key\": key, \"features\": list(X.columns), \"target\": \"residual\", \"mae_valid\": mae}\n    return pipe, meta, mae\n\nfrom joblib import dump, load\ndef save_stroke_model(stroke_key: str, pipe: Pipeline, meta: Dict[str, Any]):\n    p = MODELS_DIR / f\"{stroke_key}_residual_model.joblib\"\n    dump({\"pipe\": pipe, \"meta\": meta}, p)\n\ndef load_stroke_model(stroke_key: str) -> Optional[Dict[str, Any]]:\n    p = MODELS_DIR / f\"{stroke_key}_residual_model.joblib\"\n    if not p.exists():\n        return None\n    return load(p)\n\ndef predict_residuals(stroke_key: str, distance_m: int, n_laps: int) -> np.ndarray:\n    pack = load_stroke_model(stroke_key)\n    X = pd.DataFrame({\n        \"stroke_key\": [stroke_key] * n_laps,\n        \"distance_m\": [distance_m] * n_laps,\n        \"n_laps\": [n_laps] * n_laps,\n        \"lap_idx\": list(range(1, n_laps + 1)),\n    })\n    X[\"lap_idx_norm\"] = X[\"lap_idx\"] / n_laps\n    X[\"lap_idx_sq\"] = X[\"lap_idx_norm\"] ** 2\n    if pack is None:\n        return np.zeros(n_laps, dtype=float)\n    pipe = pack[\"pipe\"]\n    features = pack[\"meta\"][\"features\"]\n    for col in features:\n        if col not in X.columns:\n            X[col] = 0.0\n    X = X[features]\n    return pipe.predict(X).astype(float)\n\ndef shape_splits_to_target(residuals: np.ndarray, target_total: float, clip_sigma: float = 3.0) -> np.ndarray:\n    n = len(residuals)\n    r = residuals.copy()\n    std = r.std() if r.std() > 1e-9 else 1.0\n    r = np.clip(r, -clip_sigma * std, clip_sigma * std)\n    r -= r.mean()\n    base = target_total / n\n    splits = base + r\n    correction = (target_total - splits.sum()) / n\n    splits = splits + correction\n    splits = np.clip(splits, 0.01, None)\n    splits *= target_total / splits.sum()\n    return splits\n\ndef generate_ideal_splits(stroke: str, distance_m: int, pb50_sec: Optional[float] = None,\n                          per_leg_pb50: Optional[Dict[str, float]] = None) -> List[float]:\n    key = norm_stroke_label(stroke)\n    n_laps = int(distance_m // 50)\n\n    if key != \"IM\":\n        if pb50_sec is None:\n            target_total = float(distance_m / 50 * 40.0)\n        else:\n            target_total = target_time_from_pb50(key, distance_m, pb50_sec)\n        residuals = predict_residuals(key, distance_m, n_laps)\n        splits = shape_splits_to_target(residuals, target_total)\n        return splits.tolist()\n\n    if distance_m not in (200, 400):\n        if pb50_sec is None:\n            target_total = float(distance_m / 50 * 45.0)\n        else:\n            target_total = target_time_from_pb50(\"IM\", distance_m, pb50_sec)\n        residuals = predict_residuals(\"IM\", distance_m, n_laps)\n        splits = shape_splits_to_target(residuals, target_total)\n        return splits.tolist()\n\n    leg_size = 1 if distance_m == 200 else 2\n    legs = [\"FLY\", \"BACK\", \"BRST\", \"FREE\"]\n    leg_targets = []\n    for leg in legs:\n        if per_leg_pb50 and leg in per_leg_pb50:\n            leg_target = target_time_from_pb50(leg, leg_size * 50, per_leg_pb50[leg])\n        else:\n            leg_target = float(leg_size * 50 / 50 * 45.0)\n        leg_targets.append(leg_target)\n\n    leg_splits_all = []\n    for leg_key, leg_target in zip(legs, leg_targets):\n        res_leg = predict_residuals(leg_key, leg_size * 50, leg_size)\n        leg_splits = shape_splits_to_target(res_leg, leg_target)\n        leg_splits_all.extend(leg_splits.tolist())\n\n    total = sum(leg_splits_all)\n    desired = sum(leg_targets)\n    scale = desired / total if total > 1e-9 else 1.0\n    leg_splits_all = [s * scale for s in leg_splits_all]\n    return leg_splits_all\n\ndef analyze_post_race(stroke: str, distance_m: int, actual_splits: List[float],\n                      pb50_sec: Optional[float] = None,\n                      per_leg_pb50: Optional[Dict[str, float]] = None,\n                      plot: bool = True) -> Dict[str, Any]:\n    ideal = generate_ideal_splits(stroke, distance_m, pb50_sec, per_leg_pb50)\n    if len(ideal) != len(actual_splits):\n        raise ValueError(f\"Lap count mismatch. Expected {len(ideal)} splits for {distance_m}m, got {len(actual_splits)}.\")\n    ideal_arr = np.array(ideal, dtype=float)\n    actual_arr = np.array(actual_splits, dtype=float)\n    delta = actual_arr - ideal_arr\n\n    out = {\n        \"ideal_splits\": ideal,\n        \"actual_splits\": actual_splits,\n        \"delta_splits\": delta.tolist(),\n        \"ideal_total\": float(ideal_arr.sum()),\n        \"actual_total\": float(actual_arr.sum()),\n        \"delta_total\": float(delta.sum()),\n    }\n\n    if plot:\n        plt.figure(figsize=(9, 4.5))\n        x = np.arange(1, len(ideal) + 1)\n        plt.plot(x, ideal_arr, marker=\"o\", label=\"Ideal (model)\")\n        plt.plot(x, actual_arr, marker=\"s\", label=\"Actual\")\n        plt.title(f\"{stroke} {distance_m}m: Actual vs Ideal per-50 splits\")\n        plt.xlabel(\"Lap (50m)\")\n        plt.ylabel(\"Time (s)\")\n        plt.legend()\n        plt.tight_layout()\n        plt.show()\n\n    return out\n\ndef load_training_csv_for_stroke(stroke: str) -> pd.DataFrame:\n    fname = {\n        \"FREE\": \"Freestyle_dataset.csv\",\n        \"BACK\": \"Backstroke_dataset.csv\",\n        \"BRST\": \"Breaststroke_dataset.csv\",\n        \"FLY\":  \"Butterfly_dataset.csv\",\n        \"IM\":   \"IM_dataset.csv\",\n    }[norm_stroke_label(stroke)]\n    p = (DATA_DIR / fname)\n    if not p.exists():\n        raise FileNotFoundError(f\"Missing training CSV for {stroke}: {p}\")\n    df = pd.read_csv(p)\n    _, split_cols = count_split_columns(df)\n    for c in split_cols:\n        df[c] = df[c].apply(lambda x: time_to_seconds(x) if isinstance(x, str) and \":\" in x else float(x))\n    if \"distance_m\" in df.columns:\n        df[\"distance_m\"] = df[\"distance_m\"].astype(int)\n    return df\n\ndef train_all_strokes(save: bool = True) -> Dict[str, Dict[str, Any]]:\n    results = {}\n    for stroke in VALID_STROKES:\n        key = norm_stroke_label(stroke)\n        try:\n            df = load_training_csv_for_stroke(stroke)\n        except FileNotFoundError:\n            continue\n        pipe, meta, mae = train_stroke_model(stroke, df)\n        results[key] = {\"pipe\": pipe, \"meta\": meta, \"mae_valid\": mae}\n        if save:\n            save_stroke_model(key, pipe, meta)\n    return results\n\ndef _prompt_float(msg: str) -> float:\n    v = input(msg).strip()\n    return time_to_seconds(v) if \":\" in v else float(v)\n\ndef interactive_cli():\n    print(\"== Competitive Pacing ==\")\n    mode = input(\"Choose mode [pre/post]: \").strip().lower()\n    stroke = input(\"Stroke (Freestyle/Backstroke/Breaststroke/Butterfly/IM): \").strip()\n    distance_m = int(input(\"Distance (e.g., 100/200/400): \").strip())\n    if mode == \"pre\":\n        if norm_stroke_label(stroke) == \"IM\":\n            use_legs = input(\"Provide per-leg 50 PBs? [y/n]: \").strip().lower() == \"y\"\n            per_legs = None\n            if use_legs:\n                per_legs = {}\n                for leg in [\"FLY\", \"BACK\", \"BRST\", \"FREE\"]:\n                    per_legs[leg] = _prompt_float(f\"Enter 50 PB for {leg} (sec or mm:ss.xx): \")\n            splits = generate_ideal_splits(stroke, distance_m, None, per_legs)\n        else:\n            have_pb = input(\"Provide 50 PB? [y/n]: \").strip().lower() == \"y\"\n            pb50 = _prompt_float(\"Enter 50 PB (sec or mm:ss.xx): \") if have_pb else None\n            splits = generate_ideal_splits(stroke, distance_m, pb50, None)\n        total = sum(splits)\n        print(f\"\\nPredicted optimal per-50 splits for {stroke} {distance_m}m (Total {seconds_to_time(total)}):\")\n        print(\", \".join(seconds_to_time(s) for s in splits))\n    else:\n        actual = parse_splits_str(input(\"Enter actual per-50 splits (comma/space separated, sec or mm:ss.xx): \"))\n        if norm_stroke_label(stroke) == \"IM\":\n            use_legs = input(\"Provide per-leg 50 PBs? [y/n]: \").strip().lower() == \"y\"\n            per_legs = None\n            if use_legs:\n                per_legs = {}\n                for leg in [\"FLY\", \"BACK\", \"BRST\", \"FREE\"]:\n                    per_legs[leg] = _prompt_float(f\"Enter 50 PB for {leg} (sec or mm:ss.xx): \")\n            out = analyze_post_race(stroke, distance_m, actual, None, per_legs, plot=True)\n        else:\n            have_pb = input(\"Provide 50 PB? [y/n]: \").strip().lower() == \"y\"\n            pb50 = _prompt_float(\"Enter 50 PB (sec or mm:ss.xx): \") if have_pb else None\n            out = analyze_post_race(stroke, distance_m, actual, pb50, None, plot=True)\n        print(\"\\nDiagnostics:\")\n        print(f\"Ideal total:  {seconds_to_time(out['ideal_total'])}\")\n        print(f\"Actual total: {seconds_to_time(out['actual_total'])}\")\n        d = out['delta_total']\n        print(f\"Delta total:  {'+' if d>=0 else ''}{d:.2f} s\")\n\nif __name__ == \"__main__\":\n    print(\"This module provides training, prediction, and analysis utilities for Competitive Pacing.\")\n    print(\"Functions available: train_all_strokes(), generate_ideal_splits(), analyze_post_race(), interactive_cli()\")\n''')\nprint('Wrote', module_path)\n"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "# Imports\nimport pandas as pd\nfrom pathlib import Path\nimport numpy as np\nfrom competitive_pacing_updated import (\n    train_all_strokes, generate_ideal_splits, analyze_post_race, seconds_to_time\n)\n\n# Ensure folders exist\nPath('data').mkdir(exist_ok=True)\nPath('models').mkdir(exist_ok=True)\nprint('Environment ready.')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Create tiny dummy datasets\nWe synthesize a few races per stroke so you can train immediately. Replace these with your real CSVs later."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "import random\ndef make_dataset(stroke_name, distances=[100,200,400], n_races=6, base_50=28.0):\n    rows = []\n    for d in distances:\n        laps = d // 50\n        for _ in range(n_races):\n            # create a gentle shape around base\n            shape = np.linspace(-0.6, 0.6, int(laps)) + np.random.normal(0, 0.15, int(laps))\n            splits = base_50 + shape\n            row = {f'split_{i+1}': float(s) for i, s in enumerate(splits)}\n            row['distance_m'] = int(d)\n            rows.append(row)\n    return pd.DataFrame(rows)\n\nmake_dataset('Freestyle', [100,200,400], 8, base_50=27.5).to_csv('data/Freestyle_dataset.csv', index=False)\nmake_dataset('Backstroke', [100,200], 8, base_50=30.0).to_csv('data/Backstroke_dataset.csv', index=False)\nmake_dataset('Butterfly', [100,200], 8, base_50=29.8).to_csv('data/Butterfly_dataset.csv', index=False)\nmake_dataset('Breaststroke', [100,200], 8, base_50=33.0).to_csv('data/Breaststroke_dataset.csv', index=False)\n# IM: synthesize 200 IM as 4x50\nim_df = make_dataset('IM', [200], 10, base_50=32.0)\nim_df.to_csv('data/IM_dataset.csv', index=False)\nprint('Dummy datasets written to ./data')\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Train models (shape learning)\nThis will save per-stroke residual models into `./models`."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "results = train_all_strokes(save=True)\nresults\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Pre-race: generate ideal splits\nExample: 200m Freestyle with a 50 PB of 26.5s."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "splits_fr_200 = generate_ideal_splits('Freestyle', 200, pb50_sec=26.5)\n[seconds_to_time(s) for s in splits_fr_200], sum(splits_fr_200)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## IM example with per-leg PBs\nWe pass 50 PBs for Fly/Back/Breast/Free to shape each leg for a 200 IM."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "legs = {'FLY': 27.4, 'BACK': 29.2, 'BRST': 32.9, 'FREE': 25.1}\nsplits_im_200 = generate_ideal_splits('IM', 200, per_leg_pb50=legs)\n[seconds_to_time(s) for s in splits_im_200], sum(splits_im_200)\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "## Post-race analysis with plot\nCompare Actual vs Ideal splits for a sample 200 Free."
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": "actual_200fr = [27.8, 28.4, 29.1, 29.5]\ndiag = analyze_post_race('Freestyle', 200, actual_200fr, pb50_sec=26.5, plot=True)\ndiag\n"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "### Notes\n- Replace the dummy CSVs in `data/` with your real race datasets.\n- Re-run the **Train models** cell to fit on your data.\n- All plots use pure Matplotlib (no seaborn) and a single figure per plot, as requested."
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.11.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}