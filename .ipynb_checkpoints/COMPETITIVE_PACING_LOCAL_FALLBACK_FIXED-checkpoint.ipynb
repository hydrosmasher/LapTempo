{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fdfd681a",
   "metadata": {},
   "source": [
    "\n",
    "# COMPETITIVE PACING — Model Training & Inference Notebook\n",
    "\n",
    "This notebook:\n",
    "1. Verifies datasets and the leaderboard of best models.\n",
    "2. (Re)builds `best_models_by_stroke.csv` if it's missing.\n",
    "3. Trains the per-stroke, per-target models using the chosen estimator+params.\n",
    "4. Saves models under `/mnt/data/models` and writes a detailed `build_report.json`.\n",
    "5. Shows how to load and run inference with a trained model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "991fd9f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, ast, re, warnings\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Estimators\n",
    "from sklearn.ensemble import RandomForestRegressor as RandomForest\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBR\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "ROOT = Path(\".\")\n",
    "DATASETS_DIR = ROOT / \"datasets\"\n",
    "MODELS_DIR = ROOT / \"models\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LEADERBOARD_COMBINED = ROOT / \"best_models_by_stroke.csv\"\n",
    "\n",
    "# Individual leaderboards (already uploaded earlier)\n",
    "LB_FILES = {\n",
    "    \"Freestyle\": ROOT / \"Freestyle_leaderboard.csv\",\n",
    "    \"Backstroke\": ROOT / \"Backstroke_leaderboard.csv\",\n",
    "    \"Breaststroke\": ROOT / \"Breaststroke_leaderboard.csv\",\n",
    "    \"Butterfly\": ROOT / \"Butterfly_leaderboard.csv\",\n",
    "    \"IM\": ROOT / \"IM_leaderboard.csv\",\n",
    "}\n",
    "\n",
    "STROKE_TO_FILE = {\n",
    "    \"Freestyle\": \"freestyle_dataset.csv\",\n",
    "    \"Backstroke\": \"backstroke_dataset.csv\",\n",
    "    \"Breaststroke\": \"breaststroke_dataset.csv\",\n",
    "    \"Butterfly\": \"butterfly_dataset.csv\",\n",
    "    \"IM\": \"im_dataset.csv\",\n",
    "}\n",
    "\n",
    "MODEL_MAP = {\n",
    "    \"RandomForest\": RandomForest,\n",
    "    \"GBR\": GBR,\n",
    "    \"Ridge\": Ridge,\n",
    "    \"Lasso\": Lasso,\n",
    "    \"ElasticNet\": ElasticNet,\n",
    "    \"LinearRegression\": LinearRegression,\n",
    "    \"SVR\": SVR,\n",
    "}\n",
    "\n",
    "SCALER_MODELS = {\"Ridge\",\"Lasso\",\"ElasticNet\",\"LinearRegression\",\"SVR\"}\n",
    "TARGET_PATTERN = re.compile(r\"^frac_\\d+$\", flags=re.IGNORECASE)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c717908e",
   "metadata": {},
   "source": [
    "## 1) Verify datasets are present"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39c0e636",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "available = []\n",
    "if DATASETS_DIR.exists():\n",
    "    for f in DATASETS_DIR.glob(\"*.csv\"):\n",
    "        available.append(f.name)\n",
    "\n",
    "print(\"Found dataset files:\", available)\n",
    "missing = [v for v in STROKE_TO_FILE.values() if v not in available]\n",
    "if missing:\n",
    "    print(\"[WARN] Missing expected dataset files:\", missing)\n",
    "else:\n",
    "    print(\"[OK] All expected datasets present.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adf936c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Diagnostics: show columns for each dataset (head only)\n",
    "for stroke, fname in STROKE_TO_FILE.items():\n",
    "    p = DATASETS_DIR / fname\n",
    "    if p.exists():\n",
    "        dfh = pd.read_csv(p, nrows=3)\n",
    "        print(f\"\\n[{stroke}] {fname} → columns: {list(dfh.columns)}\")\n",
    "        display(dfh.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04c00be9",
   "metadata": {},
   "source": [
    "## 2) Build/Load combined leaderboard of best models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77a5f69a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pick_best(df):\n",
    "    # Expect columns: target, model, bestparams, r2, mse (case-insensitive handled below)\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    # Sort by r2 desc, mse asc\n",
    "    df_sorted = df.sort_values(by=[\"r2\", \"mse\"], ascending=[False, True])\n",
    "    best = df_sorted.groupby(\"target\", as_index=False).first()\n",
    "    return best\n",
    "\n",
    "if not LEADERBOARD_COMBINED.exists():\n",
    "    print(\"[INFO] Combined leaderboard not found. Recomputing from individual files...\")\n",
    "    frames = []\n",
    "    for stroke, path in LB_FILES.items():\n",
    "        if path.exists():\n",
    "            df = pd.read_csv(path)\n",
    "            best = pick_best(df)\n",
    "            best.insert(0, \"stroke\", stroke)\n",
    "            frames.append(best)\n",
    "        else:\n",
    "            print(f\"[WARN] Missing leaderboard for {stroke}: {path}\")\n",
    "    if not frames:\n",
    "        raise FileNotFoundError(\"No leaderboards found to build combined leaderboard.\")\n",
    "    combined = pd.concat(frames, ignore_index=True)\n",
    "    combined.rename(columns={\"bestparams\":\"bestparams\",\"model\":\"model\",\"target\":\"target\",\"r2\":\"r2\",\"mse\":\"mse\"}, inplace=True)\n",
    "    combined.to_csv(LEADERBOARD_COMBINED, index=False)\n",
    "    print(\"[OK] Wrote combined leaderboard to\", LEADERBOARD_COMBINED)\n",
    "else:\n",
    "    combined = pd.read_csv(LEADERBOARD_COMBINED)\n",
    "    print(\"[OK] Loaded existing combined leaderboard from\", LEADERBOARD_COMBINED)\n",
    "\n",
    "display(combined.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a45fbd0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Fallback: if combined leaderboard not present and individual leaderboards missing,\n",
    "# write an embedded CSV payload into ./best_models_by_stroke.csv and proceed.\n",
    "_fallback_csv_payload = r\"\"\"stroke,target,model,bestparams,r2,mse\n",
    "Freestyle,frac_1,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.999870243378946,3.446300985404293e-06\n",
    "Freestyle,frac_10,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9998943409633224,2.269611027931617e-08\n",
    "Freestyle,frac_11,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9998884575181034,2.403124123045788e-08\n",
    "Freestyle,frac_12,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9998824672075748,2.5413726915240372e-08\n",
    "Freestyle,frac_13,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.999880564246292,2.5981500273515697e-08\n",
    "Freestyle,frac_14,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9998710225436998,2.8192505027017152e-08\n",
    "Freestyle,frac_15,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9998765681982152,2.7097569500937284e-08\n",
    "Freestyle,frac_16,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9964365193390609,7.678266339955384e-07\n",
    "Freestyle,frac_17,Ridge,{'model__alpha': 0.1},0.0,2.988990637699851e-08\n",
    "Freestyle,frac_18,Ridge,{'model__alpha': 0.1},0.0,2.5349672450572248e-08\n",
    "Freestyle,frac_19,Ridge,{'model__alpha': 0.1},0.0,2.2408701089148216e-08\n",
    "Freestyle,frac_2,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9998867955175011,3.57427629763549e-06\n",
    "Freestyle,frac_20,Ridge,{'model__alpha': 0.1},0.0,1.5852440136225758e-08\n",
    "Freestyle,frac_21,Ridge,{'model__alpha': 0.1},0.0,1.2104395266390137e-08\n",
    "Freestyle,frac_22,Ridge,{'model__alpha': 0.1},0.0,1.202243271136594e-08\n",
    "Freestyle,frac_23,Ridge,{'model__alpha': 0.1},0.0,1.1919455483054742e-08\n",
    "Freestyle,frac_24,Ridge,{'model__alpha': 0.1},0.0,1.7149887415871123e-08\n",
    "Freestyle,frac_25,Ridge,{'model__alpha': 0.1},0.0,3.0218453715648795e-08\n",
    "Freestyle,frac_26,Ridge,{'model__alpha': 0.1},0.0,1.4024195867334521e-07\n",
    "Freestyle,frac_27,Ridge,{'model__alpha': 0.1},0.0,6.314455592110409e-07\n",
    "Freestyle,frac_28,Ridge,{'model__alpha': 0.1},0.0,8.880265920459282e-07\n",
    "Freestyle,frac_29,Ridge,{'model__alpha': 0.1},0.0,8.688721906725003e-07\n",
    "Freestyle,frac_3,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9998709600381422,9.556862590077076e-07\n",
    "Freestyle,frac_30,Ridge,{'model__alpha': 0.1},0.0,8.309361340034765e-07\n",
    "Freestyle,frac_4,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9999064518848968,6.696272014015025e-07\n",
    "Freestyle,frac_5,RandomForest,\"{'model__max_depth': 3, 'model__n_estimators': 100}\",0.9999709211876188,4.2780859101891336e-08\n",
    "Freestyle,frac_6,RandomForest,\"{'model__max_depth': 3, 'model__n_estimators': 100}\",0.9999532784999926,6.87564897870968e-08\n",
    "Freestyle,frac_7,RandomForest,\"{'model__max_depth': 3, 'model__n_estimators': 100}\",0.9999334164741176,9.807550310729845e-08\n",
    "Freestyle,frac_8,RandomForest,\"{'model__max_depth': 3, 'model__n_estimators': 50}\",0.9992697709056444,1.1442479334816152e-06\n",
    "Freestyle,frac_9,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9998878635463874,2.3983271145407182e-08\n",
    "Backstroke,frac_1,RandomForest,\"{'model__max_depth': 3, 'model__n_estimators': 50}\",0.9994268075799432,8.867917637670797e-06\n",
    "Backstroke,frac_2,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9995320930324364,7.929250320901907e-06\n",
    "Backstroke,frac_3,Ridge,{'model__alpha': 0.1},0.0,8.36422372652802e-06\n",
    "Backstroke,frac_4,Ridge,{'model__alpha': 0.1},0.0,1.756630052177816e-05\n",
    "Breaststroke,frac_1,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9988548437938534,1.6389929209266215e-05\n",
    "Breaststroke,frac_2,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9990713463228648,1.8044181130454614e-05\n",
    "Breaststroke,frac_3,Ridge,{'model__alpha': 0.1},0.0,3.1925173522318018e-06\n",
    "Breaststroke,frac_4,Ridge,{'model__alpha': 0.1},0.0,1.0488003264994874e-05\n",
    "Butterfly,frac_1,RandomForest,\"{'model__max_depth': 3, 'model__n_estimators': 50}\",0.9987736496840494,1.8391454265397885e-05\n",
    "Butterfly,frac_2,RandomForest,\"{'model__max_depth': 3, 'model__n_estimators': 100}\",0.9991241698770432,1.6971678700986156e-05\n",
    "Butterfly,frac_3,Ridge,{'model__alpha': 0.1},0.0,6.019565155644881e-06\n",
    "Butterfly,frac_4,Ridge,{'model__alpha': 0.1},0.0,2.3645284938661704e-05\n",
    "IM,frac_1,RandomForest,\"{'model__max_depth': 3, 'model__n_estimators': 50}\",0.9977444968117418,6.883277693457198e-06\n",
    "IM,frac_2,RandomForest,\"{'model__max_depth': 3, 'model__n_estimators': 100}\",0.9974358107136986,1.1070725017522065e-05\n",
    "IM,frac_3,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9965213315108744,2.2576616555748427e-05\n",
    "IM,frac_4,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9977884821132098,7.467268845815134e-06\n",
    "IM,frac_5,Ridge,{'model__alpha': 0.1},0.0,7.963029513171503e-06\n",
    "IM,frac_6,Ridge,{'model__alpha': 0.1},0.0,1.4448839049173334e-05\n",
    "IM,frac_7,Ridge,{'model__alpha': 0.1},0.0,9.62360429367415e-06\n",
    "IM,frac_8,Ridge,{'model__alpha': 0.1},0.0,9.857673151697714e-06\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "\n",
    "if not LEADERBOARD_COMBINED.exists():\n",
    "    candidate = Path(\"best_models_by_stroke.csv\")\n",
    "    if candidate.exists():\n",
    "        combined = pd.read_csv(candidate)\n",
    "        combined.to_csv(LEADERBOARD_COMBINED, index=False)\n",
    "        print(\"[OK] Loaded combined leaderboard from local file and saved to\", LEADERBOARD_COMBINED)\n",
    "    else:\n",
    "        # Try to rebuild from individual files (already attempted above). If 'combined' not defined, embed fallback.\n",
    "        if 'combined' not in globals():\n",
    "            if _fallback_csv_payload.strip():\n",
    "                print(\"[INFO] Using embedded fallback leaderboard → writing best_models_by_stroke.csv\")\n",
    "                Path(\"best_models_by_stroke.csv\").write_text(_fallback_csv_payload)\n",
    "                combined = pd.read_csv(\"best_models_by_stroke.csv\")\n",
    "                combined.to_csv(LEADERBOARD_COMBINED, index=False)\n",
    "                print(\"[OK] Wrote combined leaderboard to\", LEADERBOARD_COMBINED)\n",
    "            else:\n",
    "                raise FileNotFoundError(\"No leaderboards found and no embedded fallback available. \"\n",
    "                                        \"Place 'best_models_by_stroke.csv' next to this notebook or copy the 5 per-stroke leaderboard CSVs.\")\n",
    "display(combined.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f377b7eb",
   "metadata": {},
   "source": [
    "## 3) Helpers for training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96495104",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def parse_bestparams(s: str) -> dict:\n",
    "    if pd.isna(s) or not str(s).strip():\n",
    "        return {}\n",
    "    try:\n",
    "        d = ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        # naive fallback\n",
    "        d = {}\n",
    "        s2 = str(s).strip().strip(\"{}\")\n",
    "        for part in s2.split(\",\"):\n",
    "            if not part.strip(): continue\n",
    "            if \":\" in part:\n",
    "                k, v = part.split(\":\", 1)\n",
    "                d[k.strip().strip(\"'\").strip('\"')] = ast.literal_eval(v.strip())\n",
    "    return d\n",
    "\n",
    "def make_pipeline(model_name: str, params: dict, feature_cols):\n",
    "    model_cls = MODEL_MAP[model_name]\n",
    "    model = model_cls()\n",
    "\n",
    "    # Normalize params to 'model__' namespaced keys\n",
    "    p2 = {}\n",
    "    for k, v in params.items():\n",
    "        if k.startswith(\"model__\"):\n",
    "            p2[k] = v\n",
    "        else:\n",
    "            p2[f\"model__{k}\"] = v\n",
    "\n",
    "    pre = ColumnTransformer(\n",
    "        [(\"num\", StandardScaler() if model_name in SCALER_MODELS else \"passthrough\", list(feature_cols))],\n",
    "        remainder=\"drop\"\n",
    "    )\n",
    "\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"model\", model)])\n",
    "    if p2:\n",
    "        pipe.set_params(**p2)\n",
    "    return pipe\n",
    "\n",
    "def kfold_metrics(model, X, y, n_splits=5):\n",
    "    cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        r2 = cross_val_score(model, X, y, cv=cv, scoring=\"r2\")\n",
    "        neg_mse = cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "    return {\n",
    "        \"r2_mean\": float(np.mean(r2)), \"r2_std\": float(np.std(r2)),\n",
    "        \"mse_mean\": float(np.mean(-neg_mse)), \"mse_std\": float(np.std(-neg_mse))\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "985457fc",
   "metadata": {},
   "source": [
    "## 4) Train and save models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51fcd9a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import joblib\n",
    "\n",
    "build_report = []\n",
    "\n",
    "for stroke, sub in combined.groupby(\"stroke\"):\n",
    "    ds_name = STROKE_TO_FILE.get(stroke)\n",
    "    ds_path = DATASETS_DIR / ds_name if ds_name else None\n",
    "\n",
    "    if not ds_path or not ds_path.exists():\n",
    "        print(f\"[WARN] Dataset for {stroke} not found at {ds_path}. Skipping this stroke.\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(ds_path)\n",
    "\n",
    "    expected_targets = list(sub[\"target\"].astype(str).unique())\n",
    "    target_cols = detect_target_cols(df.columns, expected_targets=expected_targets)\n",
    "\n",
    "    # Try a second chance by normalizing names (e.g., frac1 -> frac_1)\n",
    "    if not target_cols:\n",
    "        norm_map = {c: normalize_target_name(c) for c in df.columns}\n",
    "        # If normalized name exists as a column, create a copy\n",
    "        renamed = {}\n",
    "        for c, n in norm_map.items():\n",
    "            if c != n and n in expected_targets:\n",
    "                df[n] = df[c]\n",
    "                renamed[c] = n\n",
    "        if renamed:\n",
    "            target_cols = detect_target_cols(df.columns, expected_targets=expected_targets)\n",
    "\n",
    "    # features: numeric that are not targets\n",
    "    feature_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c not in target_cols]\n",
    "    if not feature_cols:\n",
    "        feature_cols = [c for c in df.columns if c not in target_cols]\n",
    "\n",
    "    print(f\"\\n[INFO] {stroke}: Using {len(feature_cols)} features and {len(target_cols)} targets: {target_cols}\")\n",
    "    if not target_cols:\n",
    "        print(f\"[ERROR] No target columns found for {stroke}.\\n  Expected targets from leaderboard: {expected_targets}\\n  Dataset columns: {list(df.columns)}\\n  Hint: Rename your target columns to match (e.g., 'frac1' -> 'frac_1'). Skipping {stroke}.\")\n",
    "        continue\n",
    "\n",
    "    stroke_dir = MODELS_DIR / stroke\n",
    "    stroke_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for _, row in sub.iterrows():\n",
    "        target = row[\"target\"]\n",
    "        model_name = row[\"model\"]\n",
    "        params = parse_bestparams(row[\"bestparams\"])\n",
    "\n",
    "        if target not in df.columns:\n",
    "            print(f\"[WARN] Target '{target}' missing in dataset for {stroke}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        X = df[feature_cols].copy()\n",
    "        y = df[target].values\n",
    "\n",
    "        pipe = make_pipeline(model_name, params, feature_cols)\n",
    "\n",
    "        # CV metrics on training data (for record)\n",
    "        try:\n",
    "            cvm = kfold_metrics(pipe, X, y, n_splits=5)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] CV failed for {stroke}/{target} with {model_name}: {e}\")\n",
    "            cvm = None\n",
    "\n",
    "        pipe.fit(X, y)\n",
    "\n",
    "        model_path = stroke_dir / f\"{target}_{model_name}.joblib\"\n",
    "        joblib.dump(pipe, model_path)\n",
    "\n",
    "        meta = {\n",
    "            \"stroke\": stroke,\n",
    "            \"target\": target,\n",
    "            \"model\": model_name,\n",
    "            \"bestparams\": params,\n",
    "            \"features\": feature_cols,\n",
    "            \"dataset\": str(ds_path),\n",
    "            \"model_path\": str(model_path),\n",
    "            \"cv_metrics\": cvm,\n",
    "        }\n",
    "        build_report.append(meta)\n",
    "        print(f\"[OK] Saved {stroke}/{target} -> {model_path.name}\")\n",
    "\n",
    "    with open(stroke_dir / \"index.json\", \"w\") as f:\n",
    "        json.dump([m for m in build_report if m[\"stroke\"] == stroke], f, indent=2)\n",
    "\n",
    "# global report\n",
    "with open(MODELS_DIR / \"build_report.json\", \"w\") as f:\n",
    "    json.dump(build_report, f, indent=2)\n",
    "\n",
    "print(\"\\n[DONE] Training complete. Summary:\")\n",
    "pd.DataFrame(build_report)[[\"stroke\",\"target\",\"model\",\"model_path\"]].head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bdd21b2e",
   "metadata": {},
   "source": [
    "## 5) Build report summary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "824e9857",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "report_df = pd.DataFrame(build_report)\n",
    "if report_df.empty:\n",
    "    print(\"[INFO] No models were trained. See warnings above about missing targets.\")\n",
    "else:\n",
    "    display(report_df.head(20))\n",
    "    report_csv = MODELS_DIR / \"build_report.csv\"\n",
    "    report_df.to_csv(report_csv, index=False)\n",
    "    print(\"Saved:\", report_csv)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ef44c5c",
   "metadata": {},
   "source": [
    "## 6) Load a trained model and predict (registry-like helpers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "926210cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import json, joblib\n",
    "\n",
    "def list_trained(stroke: str):\n",
    "    idx_path = MODELS_DIR / stroke / \"index.json\"\n",
    "    if not idx_path.exists():\n",
    "        return []\n",
    "    with open(idx_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "    return meta\n",
    "\n",
    "def load_model(stroke: str, target: str):\n",
    "    metas = list_trained(stroke)\n",
    "    candidates = [m for m in metas if m[\"target\"] == target]\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"No trained model for {stroke}/{target}\")\n",
    "    m = candidates[0]\n",
    "    path = Path(m[\"model_path\"])\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Model file missing at {path}\")\n",
    "    return joblib.load(path), m\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25237c24",
   "metadata": {},
   "source": [
    "### Inference demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca77f5e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Pick a stroke & target that were trained (adjust as needed)\n",
    "example_stroke = \"Freestyle\"\n",
    "example_target = \"frac_1\"\n",
    "\n",
    "# Load dataset to get feature columns\n",
    "ds_path = DATASETS_DIR / STROKE_TO_FILE[example_stroke]\n",
    "df = pd.read_csv(ds_path)\n",
    "\n",
    "target_cols = [c for c in df.columns if TARGET_PATTERN.match(str(c))]\n",
    "feature_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c not in target_cols]\n",
    "if not feature_cols:\n",
    "    feature_cols = [c for c in df.columns if c not in target_cols]\n",
    "\n",
    "pipe, meta = load_model(example_stroke, example_target)\n",
    "\n",
    "# Use a few samples from the dataset for a quick check\n",
    "X_sample = df[feature_cols].head(5)\n",
    "y_pred = pipe.predict(X_sample)\n",
    "\n",
    "print(\"Loaded model:\", meta[\"model\"], \"for\", example_stroke, example_target)\n",
    "print(\"Predictions:\", y_pred)\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
