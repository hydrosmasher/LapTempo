{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76dd891e-91c7-471c-be2c-903666d5ac5b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Competitive Pacing (updated) ===\n",
      "Loaded rows: 0\n",
      "Models trained for events: []\n",
      "No leaderboard ratios found; using trained models or heuristics.\n",
      "\n",
      "Tip: call list_available_events() to see which (stroke, distance) are model-ready.\n",
      "Run cli_loop() to start the interactive menu.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Competitive Pacing — updated single-cell notebook\n",
    "# -------------------------------------------------\n",
    "# What changed vs last version?\n",
    "# • Reads your uploaded files by default:\n",
    "#     /mnt/data/freestyle_dataset.csv\n",
    "#     /mnt/data/backstroke_dataset.csv\n",
    "#     /mnt/data/breaststroke_dataset.csv\n",
    "#     /mnt/data/butterfly_dataset.csv\n",
    "#     /mnt/data/im_dataset.csv\n",
    "#     /mnt/data/best_models_by_stroke.csv   (leaderboard/best-model hints)\n",
    "#   ...and still falls back to ./datasets/*.csv if you move them there.\n",
    "# • Uses \"best model by stroke\" (if provided) to choose a regressor for each stroke.\n",
    "#   Supported strings (case-insensitive): randomforest, gradientboosting, ridge, lasso,\n",
    "#   elasticnet, svr, knn. Unknown → defaults to RandomForest.\n",
    "# • Leaderboard file ALSO doubles as pacing baseline if it contains ratio_* or split_* columns\n",
    "#   per (stroke, distance), or a \"splits\" semicolon string. Otherwise it’s treated only as model hints.\n",
    "#\n",
    "# Usage:\n",
    "#   1) Put CSVs in /mnt/data or ./datasets.\n",
    "#   2) Run this cell.\n",
    "#   3) list_available_events()\n",
    "#   4) cli_loop()   # interactive Pre-race / Post-race CLI\n",
    "#\n",
    "# Requirements:\n",
    "#   pip install pandas numpy scikit-learn matplotlib\n",
    "\n",
    "import os, re, sys, math, textwrap, warnings\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "from collections import defaultdict\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# -------------------------------\n",
    "# Utilities: parsing + formatting\n",
    "# -------------------------------\n",
    "\n",
    "STROKE_ALIASES = {\n",
    "    \"free\": \"freestyle\", \"freestyle\": \"freestyle\", \"fr\": \"freestyle\", \"fs\": \"freestyle\",\n",
    "    \"back\": \"backstroke\", \"backstroke\": \"backstroke\", \"bk\": \"backstroke\",\n",
    "    \"breast\": \"breaststroke\", \"breaststroke\": \"breaststroke\", \"br\": \"breaststroke\",\n",
    "    \"fly\": \"butterfly\", \"butterfly\": \"butterfly\", \"bf\": \"butterfly\", \"fl\": \"butterfly\",\n",
    "    \"im\": \"im\", \"medley\": \"im\", \"individual medley\": \"im\", \"individualmedley\": \"im\"\n",
    "}\n",
    "\n",
    "TOTAL_TIME_CANDIDATES = [\"time_total\",\"total_time\",\"final_time\",\"result_time\",\"time\",\"seed_time\",\"pb_time\",\"official_time\"]\n",
    "DISTANCE_CANDIDATES   = [\"distance\",\"event_distance\",\"dist\",\"meters\",\"metres\"]\n",
    "STROKE_CANDIDATES     = [\"stroke\",\"event_stroke\",\"style\",\"st\"]\n",
    "SPLITS_BUNDLE         = [\"splits\",\"split_string\",\"lap_splits\"]\n",
    "\n",
    "def normalize_stroke(x: str) -> Optional[str]:\n",
    "    if x is None or (isinstance(x, float) and math.isnan(x)): return None\n",
    "    s = str(x).strip().lower().replace(\"-\", \" \")\n",
    "    s = re.sub(r'[^a-z ]+', '', s)\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return STROKE_ALIASES.get(s, s if s in STROKE_ALIASES.values() else None)\n",
    "\n",
    "def parse_time_to_seconds(t) -> Optional[float]:\n",
    "    if t is None or (isinstance(t, float) and math.isnan(t)): return None\n",
    "    s = str(t).strip()\n",
    "    if not s: return None\n",
    "    # direct float\n",
    "    try:\n",
    "        return float(s)\n",
    "    except: pass\n",
    "    # M:SS(.xx)\n",
    "    if \":\" in s:\n",
    "        parts = s.split(\":\")\n",
    "        if len(parts) == 2:\n",
    "            try:\n",
    "                m = float(parts[0]); sec = float(parts[1])\n",
    "                return m*60.0 + sec\n",
    "            except: return None\n",
    "    s2 = re.sub(r'[^0-9\\.]', '', s)\n",
    "    try:\n",
    "        return float(s2)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def seconds_to_time_str(sec: float) -> str:\n",
    "    if sec is None or not np.isfinite(sec): return \"NA\"\n",
    "    sec = float(sec)\n",
    "    if sec < 60:\n",
    "        return f\"{sec:0.2f}\"\n",
    "    m = int(sec // 60)\n",
    "    s = sec - 60*m\n",
    "    return f\"{m}:{s:05.2f}\"\n",
    "\n",
    "def parse_splits_from_string(s: str) -> List[float]:\n",
    "    if s is None or (isinstance(s, float) and math.isnan(s)): return []\n",
    "    parts = [p.strip() for p in str(s).replace(\",\", \";\").split(\";\") if p.strip()]\n",
    "    vals = [parse_time_to_seconds(p) for p in parts]\n",
    "    return [v for v in vals if v is not None and v > 0]\n",
    "\n",
    "def infer_split_columns(df: pd.DataFrame) -> List[str]:\n",
    "    cols = list(df.columns)\n",
    "    split_cols = []\n",
    "    # bundled?\n",
    "    for c in SPLITS_BUNDLE:\n",
    "        if c in df.columns:\n",
    "            return [c]\n",
    "    # split-like columns\n",
    "    pattern = re.compile(r'^(split|lap|l50|s50|fifty)(_)?(\\d+)?', re.IGNORECASE)\n",
    "    for c in cols:\n",
    "        if pattern.match(str(c)): split_cols.append(c)\n",
    "    return split_cols\n",
    "\n",
    "def extract_total_time_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    for c in TOTAL_TIME_CANDIDATES:\n",
    "        if c in df.columns: return c\n",
    "    for c in df.columns:\n",
    "        if str(c).lower() == \"result\": return c\n",
    "    return None\n",
    "\n",
    "def extract_distance_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    for c in DISTANCE_CANDIDATES:\n",
    "        if c in df.columns: return c\n",
    "    return None\n",
    "\n",
    "def extract_stroke_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    for c in STROKE_CANDIDATES:\n",
    "        if c in df.columns: return c\n",
    "    return None\n",
    "\n",
    "def n50s_for_distance(distance: int) -> int:\n",
    "    return max(0, int(distance // 50))\n",
    "\n",
    "def softmax_like(x: np.ndarray, eps: float = 1e-9) -> np.ndarray:\n",
    "    x = np.array(x, dtype=float)\n",
    "    x = np.maximum(x, eps)\n",
    "    total = np.sum(x)\n",
    "    if total <= eps: return np.ones_like(x)/len(x)\n",
    "    return x / total\n",
    "\n",
    "# ---------------------------------------\n",
    "# Data loading, cleaning, and harmonizing\n",
    "# ---------------------------------------\n",
    "\n",
    "def load_csv_if_exists(path) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            df = pd.read_csv(path)\n",
    "            df.columns = [re.sub(r'\\s+', '_', str(c).strip().lower()) for c in df.columns]\n",
    "            return df\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] Could not read {path}: {e}\")\n",
    "    return None\n",
    "\n",
    "def resample_splits(splits: List[float], n_target: int) -> List[float]:\n",
    "    total = sum(splits) if splits else 0.0\n",
    "    if n_target <= 0 or total <= 0: return []\n",
    "    # robust equal redistribution; avoids distortion when input counts differ\n",
    "    return [total / n_target] * n_target\n",
    "\n",
    "def extract_splits_from_row(df: pd.DataFrame, row: pd.Series) -> List[float]:\n",
    "    split_cols = infer_split_columns(df)\n",
    "    if not split_cols: return []\n",
    "    if len(split_cols) == 1 and split_cols[0] in SPLITS_BUNDLE and split_cols[0] in df.columns:\n",
    "        return parse_splits_from_string(row[split_cols[0]])\n",
    "    # sort by trailing number if present\n",
    "    def split_key(c):\n",
    "        m = re.search(r'(\\d+)$', str(c))\n",
    "        return int(m.group(1)) if m else 9999\n",
    "    cols = sorted(split_cols, key=split_key)\n",
    "    vals = []\n",
    "    for c in cols:\n",
    "        v = row.get(c, None)\n",
    "        if v is None or (isinstance(v, float) and math.isnan(v)): continue\n",
    "        v = parse_time_to_seconds(v)\n",
    "        if v is not None and v > 0: vals.append(v)\n",
    "    return vals\n",
    "\n",
    "def standardize_event_rows(df: pd.DataFrame, default_stroke: Optional[str]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # stroke\n",
    "    sc = extract_stroke_column(df)\n",
    "    if sc is None:\n",
    "        df[\"stroke\"] = default_stroke\n",
    "    else:\n",
    "        df[\"stroke\"] = df[sc].apply(normalize_stroke)\n",
    "        if default_stroke and df[\"stroke\"].isna().all():\n",
    "            df[\"stroke\"] = default_stroke\n",
    "\n",
    "    # distance\n",
    "    dc = extract_distance_column(df)\n",
    "    if dc is None:\n",
    "        split_cols = infer_split_columns(df)\n",
    "        if split_cols:\n",
    "            dists = []\n",
    "            for _, r in df.iterrows():\n",
    "                n = len(extract_splits_from_row(df, r))\n",
    "                dists.append(n*50 if n>0 else np.nan)\n",
    "            df[\"distance\"] = dists\n",
    "        else:\n",
    "            df[\"distance\"] = np.nan\n",
    "    else:\n",
    "        df[\"distance\"] = pd.to_numeric(df[dc], errors=\"coerce\")\n",
    "\n",
    "    # total time\n",
    "    tc = extract_total_time_column(df)\n",
    "    if tc is None:\n",
    "        totals = []\n",
    "        for _, r in df.iterrows():\n",
    "            splits = extract_splits_from_row(df, r)\n",
    "            totals.append(sum(splits) if splits else np.nan)\n",
    "        df[\"time_total_sec\"] = totals\n",
    "    else:\n",
    "        df[\"time_total_sec\"] = df[tc].apply(parse_time_to_seconds)\n",
    "\n",
    "    # validity\n",
    "    df = df[~df[\"stroke\"].isna()]\n",
    "    df = df[~df[\"distance\"].isna()]\n",
    "    df = df[~df[\"time_total_sec\"].isna()]\n",
    "    df = df[df[\"distance\"] % 50 == 0]\n",
    "    df = df[df[\"time_total_sec\"] > 0]\n",
    "\n",
    "    # build split & ratio columns\n",
    "    split_list, ratio_list, max_s = [], [], 0\n",
    "    for _, row in df.iterrows():\n",
    "        splits = extract_splits_from_row(df, row)\n",
    "        total = row[\"time_total_sec\"]\n",
    "        if not splits:\n",
    "            n = n50s_for_distance(int(row[\"distance\"]))\n",
    "            splits = [total / n]*n if n>0 else []\n",
    "        else:\n",
    "            n_expected = n50s_for_distance(int(row[\"distance\"]))\n",
    "            if n_expected>0 and len(splits)!=n_expected:\n",
    "                splits = resample_splits(splits, n_expected)\n",
    "        ratios = [s/total for s in splits] if total>0 else []\n",
    "        split_list.append(splits)\n",
    "        ratio_list.append(ratios)\n",
    "        max_s = max(max_s, len(splits))\n",
    "\n",
    "    for i in range(max_s):\n",
    "        df[f\"split_{i+1}_sec\"] = [(v[i] if i<len(v) else np.nan) for v in split_list]\n",
    "        df[f\"ratio_{i+1}\"]     = [(v[i] if i<len(v) else np.nan) for v in ratio_list]\n",
    "\n",
    "    # feature columns (numeric, excluding targets)\n",
    "    exclude_prefixes = (\"split_\", \"ratio_\")\n",
    "    numeric_cols = []\n",
    "    for c in df.columns:\n",
    "        if c in [\"stroke\",\"distance\",\"time_total_sec\"]: continue\n",
    "        if c.startswith(exclude_prefixes): continue\n",
    "        if pd.api.types.is_numeric_dtype(df[c]): numeric_cols.append(c)\n",
    "    feature_cols = [\"time_total_sec\"] + numeric_cols\n",
    "    df[\"_feature_cols\"] = [feature_cols]*len(df)\n",
    "    return df\n",
    "\n",
    "# -----------------------------------\n",
    "# Leaderboards / Best-models parsing\n",
    "# -----------------------------------\n",
    "\n",
    "def load_leaderboard_and_models(path: str):\n",
    "    \"\"\"\n",
    "    Returns:\n",
    "      leaderboard: Dict[(stroke, distance) -> ratio list]  # if present\n",
    "      model_hints: Dict[stroke -> model_name]              # if present\n",
    "    Accepts flexible schema:\n",
    "      • stroke, distance, ratio_1..ratio_k\n",
    "      • stroke, distance, split_1..split_k  (converted to ratios)\n",
    "      • stroke, distance, splits (semicolon string)\n",
    "      • stroke, model\n",
    "    \"\"\"\n",
    "    leaderboard = {}\n",
    "    model_hints = {}\n",
    "    df = load_csv_if_exists(path)\n",
    "    if df is None or df.empty: return leaderboard, model_hints\n",
    "\n",
    "    # normalize stroke\n",
    "    if \"stroke\" in df.columns:\n",
    "        df[\"stroke\"] = df[\"stroke\"].apply(normalize_stroke)\n",
    "\n",
    "    # model hints\n",
    "    if \"model\" in df.columns and \"stroke\" in df.columns:\n",
    "        for _, r in df.dropna(subset=[\"stroke\",\"model\"]).iterrows():\n",
    "            model_hints[r[\"stroke\"]] = str(r[\"model\"]).strip().lower()\n",
    "\n",
    "    # ratios / splits per (stroke, distance)\n",
    "    has_distance = \"distance\" in df.columns\n",
    "    if \"stroke\" in df.columns and has_distance:\n",
    "        tmp = df.dropna(subset=[\"stroke\",\"distance\"]).copy()\n",
    "        tmp[\"distance\"] = pd.to_numeric(tmp[\"distance\"], errors=\"coerce\")\n",
    "        tmp = tmp.dropna(subset=[\"distance\"])\n",
    "        # ratio columns?\n",
    "        ratio_cols = [c for c in tmp.columns if re.match(r'^(avg_)?ratio_\\d+$', str(c))]\n",
    "        if ratio_cols:\n",
    "            for _, r in tmp.iterrows():\n",
    "                ratios = [r[c] for c in ratio_cols]\n",
    "                ratios = [float(x) for x in ratios if x is not None and np.isfinite(x)]\n",
    "                if ratios:\n",
    "                    leaderboard[(r[\"stroke\"], int(r[\"distance\"]))] = softmax_like(np.array(ratios)).tolist()\n",
    "        else:\n",
    "            # split columns?\n",
    "            split_cols = [c for c in tmp.columns if re.match(r'^(avg_)?split_\\d+(_sec)?$', str(c))]\n",
    "            if split_cols:\n",
    "                for _, r in tmp.iterrows():\n",
    "                    splits = [parse_time_to_seconds(r[c]) for c in split_cols]\n",
    "                    splits = [x for x in splits if x is not None and x > 0]\n",
    "                    if splits:\n",
    "                        leaderboard[(r[\"stroke\"], int(r[\"distance\"]))] = softmax_like(np.array(splits)).tolist()\n",
    "            # bundled splits?\n",
    "            if \"splits\" in tmp.columns:\n",
    "                for _, r in tmp.iterrows():\n",
    "                    s = parse_splits_from_string(r[\"splits\"])\n",
    "                    if s:\n",
    "                        leaderboard[(r[\"stroke\"], int(r[\"distance\"]))] = softmax_like(np.array(s)).tolist()\n",
    "\n",
    "    return leaderboard, model_hints\n",
    "\n",
    "# -------------------------\n",
    "# Model store + training\n",
    "# -------------------------\n",
    "\n",
    "def make_base_estimator(name: str):\n",
    "    n = (name or \"\").strip().lower()\n",
    "    if n in (\"randomforest\",\"rf\",\"random_forest\"):\n",
    "        return RandomForestRegressor(n_estimators=250, random_state=42, min_samples_leaf=3, n_jobs=-1)\n",
    "    if n in (\"gradientboosting\",\"gb\",\"gbr\",\"gradient_boosting\"):\n",
    "        return GradientBoostingRegressor(random_state=42)\n",
    "    if n in (\"ridge\",):\n",
    "        return Ridge(alpha=1.0, random_state=42)\n",
    "    if n in (\"lasso\",):\n",
    "        return Lasso(alpha=0.0005, random_state=42, max_iter=20000)\n",
    "    if n in (\"elasticnet\",\"enet\"):\n",
    "        return ElasticNet(alpha=0.0005, l1_ratio=0.3, random_state=42, max_iter=20000)\n",
    "    if n in (\"svr\",\"svm\"):\n",
    "        return SVR(kernel=\"rbf\", C=10.0, gamma=\"scale\")\n",
    "    if n in (\"knn\",\"kneighbors\",\"k-neighbors\"):\n",
    "        return KNeighborsRegressor(n_neighbors=7, weights=\"distance\")\n",
    "    # default\n",
    "    return RandomForestRegressor(n_estimators=250, random_state=42, min_samples_leaf=3, n_jobs=-1)\n",
    "\n",
    "class SplitModelStore:\n",
    "    \"\"\"\n",
    "    Stores per-(stroke, distance) multi-output models that predict split ratios.\n",
    "    Chooses regressor per stroke using model_hints if available.\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        self.models: Dict[Tuple[str,int], MultiOutputRegressor] = {}\n",
    "        self.feature_cols: Dict[Tuple[str,int], List[str]] = {}\n",
    "        self.output_dims: Dict[Tuple[str,int], int] = {}\n",
    "        self.leaderboard: Dict[Tuple[str,int], List[float]] = {}\n",
    "        self.model_hints: Dict[str, str] = {}\n",
    "\n",
    "    def set_leaderboard(self, lb: Dict[Tuple[str,int], List[float]]):\n",
    "        self.leaderboard = lb or {}\n",
    "\n",
    "    def set_model_hints(self, hints: Dict[str, str]):\n",
    "        self.model_hints = {k: (v or \"\").strip().lower() for k,v in (hints or {}).items()}\n",
    "\n",
    "    def _estimator_for_stroke(self, stroke: str):\n",
    "        hint = self.model_hints.get(stroke, \"\")\n",
    "        base = make_base_estimator(hint)\n",
    "        return base\n",
    "\n",
    "    def train_from_dataframe(self, df: pd.DataFrame):\n",
    "        if df is None or df.empty: return\n",
    "        groups = df.groupby([\"stroke\",\"distance\"])\n",
    "        for (stroke, dist), g in groups:\n",
    "            stroke = normalize_stroke(stroke)\n",
    "            if not stroke: continue\n",
    "            dist = int(dist) if pd.notna(dist) else None\n",
    "            if not dist or dist % 50 != 0: continue\n",
    "            n_out = n50s_for_distance(dist)\n",
    "            feat_cols = g[\"_feature_cols\"].iloc[0] if \"_feature_cols\" in g.columns else [\"time_total_sec\"]\n",
    "            feat_cols = [c for c in feat_cols if c in g.columns]\n",
    "            X = g[feat_cols].apply(pd.to_numeric, errors=\"coerce\").replace([np.inf,-np.inf], np.nan)\n",
    "            target_cols = [f\"ratio_{i+1}\" for i in range(n_out)]\n",
    "            Y = g[target_cols].apply(pd.to_numeric, errors=\"coerce\").replace([np.inf,-np.inf], np.nan)\n",
    "\n",
    "            XY = X.join(Y).dropna()\n",
    "            if XY.empty: continue\n",
    "            Xc = XY[feat_cols].values\n",
    "            Yc = XY[target_cols].values\n",
    "            row_sums = Yc.sum(axis=1, keepdims=True)\n",
    "            row_sums[row_sums==0] = 1.0\n",
    "            Yc = Yc / row_sums\n",
    "\n",
    "            base = self._estimator_for_stroke(stroke)\n",
    "            model = MultiOutputRegressor(base)\n",
    "            model.fit(Xc, Yc)\n",
    "            self.models[(stroke, dist)] = model\n",
    "            self.feature_cols[(stroke, dist)] = feat_cols\n",
    "            self.output_dims[(stroke, dist)] = n_out\n",
    "\n",
    "    def predict_ratios(self, stroke: str, distance: int, total_time_sec: float, extra_features: Optional[Dict[str,float]] = None) -> List[float]:\n",
    "        key = (normalize_stroke(stroke), int(distance))\n",
    "        n_out = n50s_for_distance(distance)\n",
    "        feat_cols = self.feature_cols.get(key, [\"time_total_sec\"])\n",
    "        feats = {c: np.nan for c in feat_cols}\n",
    "        feats[\"time_total_sec\"] = float(total_time_sec)\n",
    "        if extra_features:\n",
    "            for k,v in extra_features.items():\n",
    "                if k in feats and v is not None and np.isfinite(v):\n",
    "                    feats[k] = float(v)\n",
    "        Xrow = np.array([[feats[c] if np.isfinite(feats[c]) else 0.0 for c in feat_cols]])\n",
    "\n",
    "        if key in self.models:\n",
    "            pred = self.models[key].predict(Xrow)[0]\n",
    "            ratios = softmax_like(pred)\n",
    "            if len(ratios) != n_out:\n",
    "                ratios = _resize_ratios(ratios, n_out)\n",
    "            return ratios\n",
    "\n",
    "        if key in self.leaderboard:\n",
    "            ratios = np.array(self.leaderboard[key], dtype=float)\n",
    "            return softmax_like(_resize_ratios(ratios, n_out))\n",
    "\n",
    "        # Heuristic fallback\n",
    "        if distance <= 200:\n",
    "            base = np.linspace(1.05, 0.95, n_out)\n",
    "        else:\n",
    "            base = np.linspace(0.98, 1.02, n_out)\n",
    "        return softmax_like(base)\n",
    "\n",
    "def _resize_ratios(r: np.ndarray, n_out: int) -> np.ndarray:\n",
    "    r = np.array(r, dtype=float)\n",
    "    if n_out <= 0: return np.array([])\n",
    "    if len(r) == n_out: return r\n",
    "    if len(r) <= 0: return np.ones(n_out)/n_out\n",
    "    # simple even redistribution\n",
    "    return np.ones(n_out)/n_out\n",
    "\n",
    "# ----------------------------\n",
    "# Data ingestion from datasets\n",
    "# ----------------------------\n",
    "\n",
    "def first_existing(*paths) -> Optional[str]:\n",
    "    for p in paths:\n",
    "        if p and os.path.exists(p): return p\n",
    "    return None\n",
    "\n",
    "def load_all_datasets() -> pd.DataFrame:\n",
    "    # Prefer /mnt/data (your uploads), fallback to ./datasets\n",
    "    files = {\n",
    "        \"freestyle\":  first_existing(\"/mnt/data/freestyle_dataset.csv\",  \"./datasets/freestyle.csv\",  \"./datasets/freestyle_dataset.csv\"),\n",
    "        \"backstroke\": first_existing(\"/mnt/data/backstroke_dataset.csv\", \"./datasets/backstroke.csv\", \"./datasets/backstroke_dataset.csv\"),\n",
    "        \"breaststroke\": first_existing(\"/mnt/data/breaststroke_dataset.csv\",\"./datasets/breaststroke.csv\",\"./datasets/breaststroke_dataset.csv\"),\n",
    "        \"butterfly\":  first_existing(\"/mnt/data/butterfly_dataset.csv\", \"./datasets/butterfly.csv\",  \"./datasets/butterfly_dataset.csv\"),\n",
    "        \"im\":         first_existing(\"/mnt/data/im_dataset.csv\",         \"./datasets/im.csv\",         \"./datasets/im_dataset.csv\"),\n",
    "    }\n",
    "    frames = []\n",
    "    for stroke_name, path in files.items():\n",
    "        if not path: \n",
    "            continue\n",
    "        df = load_csv_if_exists(path)\n",
    "        if df is None or df.empty:\n",
    "            continue\n",
    "        df = standardize_event_rows(df, default_stroke=stroke_name)\n",
    "        frames.append(df)\n",
    "    return pd.concat(frames, ignore_index=True) if frames else pd.DataFrame()\n",
    "\n",
    "def load_leaderboard_and_hints():\n",
    "    path = first_existing(\"/mnt/data/best_models_by_stroke.csv\", \"./datasets/best_models_by_stroke.csv\", \"./datasets/leaderboards.csv\")\n",
    "    return load_leaderboard_and_models(path) if path else ({}, {})\n",
    "\n",
    "# ----------------------------\n",
    "# CLI Helpers (Notebook-safe)\n",
    "# ----------------------------\n",
    "\n",
    "def ask(prompt: str, cast=str, allow_blank=False):\n",
    "    while True:\n",
    "        val = input(prompt).strip()\n",
    "        if not val and allow_blank:\n",
    "            return None\n",
    "        try:\n",
    "            return cast(val)\n",
    "        except Exception:\n",
    "            print(\"  Invalid input, try again.\")\n",
    "\n",
    "def parse_stroke_input(s: str) -> str:\n",
    "    st = normalize_stroke(s)\n",
    "    if not st:\n",
    "        raise ValueError(\"Unknown stroke. Use Freestyle, Backstroke, Breaststroke, Butterfly, or IM.\")\n",
    "    return st\n",
    "\n",
    "def parse_distance_input(s: str) -> int:\n",
    "    d = int(float(s))\n",
    "    if d % 50 != 0 or d <= 0:\n",
    "        raise ValueError(\"Distance must be a positive multiple of 50 (e.g., 50, 100, 200, 400).\")\n",
    "    return d\n",
    "\n",
    "def pretty_print_splits(splits_sec: List[float]) -> str:\n",
    "    return \";\".join(seconds_to_time_str(x) for x in splits_sec)\n",
    "\n",
    "def figure_compare_splits(given: List[float], ideal: List[float], title: str = \"Splits Comparison\"):\n",
    "    n = max(len(given), len(ideal))\n",
    "    if n == 0:\n",
    "        print(\"[info] Nothing to plot.\")\n",
    "        return\n",
    "    if len(given) != n:\n",
    "        given = _resize_to_len(given, n)\n",
    "    if len(ideal) != n:\n",
    "        ideal = _resize_to_len(ideal, n)\n",
    "    xs = np.arange(1, n+1)\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    plt.plot(xs, given, marker='o', label=\"Given splits (sec)\")\n",
    "    plt.plot(xs, ideal, marker='o', label=\"Ideal splits (sec)\")\n",
    "    plt.xlabel(\"50m split #\")\n",
    "    plt.ylabel(\"Time (s)\")\n",
    "    plt.title(title)\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "def _resize_to_len(v: List[float], n: int) -> List[float]:\n",
    "    if len(v) == n: return v\n",
    "    if len(v) == 0: return [0.0]*n\n",
    "    total = sum(v)\n",
    "    if total <= 0: return [0.0]*n\n",
    "    return (np.ones(n) * (total/n)).tolist()\n",
    "\n",
    "# -----------------------------------\n",
    "# Core functions (Pre-race/Post-race)\n",
    "# -----------------------------------\n",
    "\n",
    "def ideal_splits_from_target(stroke: str, distance: int, target_time_str: str, personal_best_str: Optional[str] = None) -> List[float]:\n",
    "    target_sec = parse_time_to_seconds(target_time_str)\n",
    "    if target_sec is None or target_sec <= 0:\n",
    "        raise ValueError(\"Invalid target time.\")\n",
    "    extra = {}\n",
    "    if personal_best_str:\n",
    "        pb_sec = parse_time_to_seconds(personal_best_str)\n",
    "        if pb_sec and np.isfinite(pb_sec) and pb_sec > 0:\n",
    "            extra[\"pb_sec\"] = pb_sec\n",
    "    ratios = MODEL_STORE.predict_ratios(stroke, distance, total_time_sec=target_sec, extra_features=extra)\n",
    "    splits = (np.array(ratios) * target_sec).tolist()\n",
    "    return splits\n",
    "\n",
    "def analyze_post_race(stroke: str, distance: int, given_splits_str: str, personal_best_str: Optional[str] = None):\n",
    "    given = parse_splits_from_string(given_splits_str)\n",
    "    if not given:\n",
    "        raise ValueError(\"Could not parse given splits. Use format like '32.33;33.11;...'.\")\n",
    "    n_expected = n50s_for_distance(distance)\n",
    "    if n_expected <= 0: raise ValueError(\"Distance must be a positive multiple of 50.\")\n",
    "    if len(given) != n_expected:\n",
    "        given = resample_splits(given, n_expected)\n",
    "    total_given = sum(given)\n",
    "\n",
    "    # Ideal total: prefer PB if provided; else match given total\n",
    "    target_sec = None\n",
    "    if personal_best_str:\n",
    "        pb = parse_time_to_seconds(personal_best_str)\n",
    "        if pb and pb > 0:\n",
    "            target_sec = pb\n",
    "    if not target_sec:\n",
    "        target_sec = total_given\n",
    "\n",
    "    ratios = MODEL_STORE.predict_ratios(stroke, distance, total_time_sec=target_sec, extra_features={\"pb_sec\": target_sec})\n",
    "    ideal = (np.array(ratios) * target_sec).tolist()\n",
    "\n",
    "    diff_per_split = (np.array(given) - np.array(ideal)).tolist()\n",
    "    cumulative_given = np.cumsum(given)\n",
    "    cumulative_ideal = np.cumsum(ideal)\n",
    "    total_delta = total_given - sum(ideal)\n",
    "\n",
    "    report = {\n",
    "        \"stroke\": stroke,\n",
    "        \"distance\": distance,\n",
    "        \"total_given_sec\": total_given,\n",
    "        \"target_sec_used\": target_sec,\n",
    "        \"total_ideal_sec\": float(sum(ideal)),\n",
    "        \"total_delta_sec\": float(total_delta),\n",
    "        \"given_splits_sec\": [float(x) for x in given],\n",
    "        \"ideal_splits_sec\": [float(x) for x in ideal],\n",
    "        \"split_delta_sec\": [float(x) for x in diff_per_split],\n",
    "        \"cumulative_given_sec\": [float(x) for x in cumulative_given],\n",
    "        \"cumulative_ideal_sec\": [float(x) for x in cumulative_ideal],\n",
    "    }\n",
    "    return report\n",
    "\n",
    "# --------------------------\n",
    "# Interactive CLI (Notebook)\n",
    "# --------------------------\n",
    "\n",
    "MENU = textwrap.dedent(\"\"\"\n",
    "    -------------------------\n",
    "    Competitive Pacing - CLI\n",
    "    -------------------------\n",
    "    Choose mode:\n",
    "      1) Pre-race (PB + Target -> ideal 50s splits)\n",
    "      2) Post-race (Given splits + PB -> analysis & chart)\n",
    "      3) Exit\n",
    "\"\"\")\n",
    "\n",
    "def cli_loop():\n",
    "    while True:\n",
    "        print(MENU)\n",
    "        choice = ask(\"Enter choice (1/2/3): \", cast=str)\n",
    "        if choice == \"3\":\n",
    "            print(\"Bye!\")\n",
    "            break\n",
    "        elif choice == \"1\":\n",
    "            try:\n",
    "                stroke = parse_stroke_input(ask(\"Stroke (Freestyle/Backstroke/Breaststroke/Butterfly/IM): \", cast=str))\n",
    "                distance = parse_distance_input(ask(\"Distance (e.g., 50/100/200/400): \", cast=str))\n",
    "                pb = ask(\"Personal Best time (e.g., 1:45.23 or 65.23) [optional]: \", cast=str, allow_blank=True)\n",
    "                target = ask(\"Target time (e.g., 1:40.00 or 100.0): \", cast=str)\n",
    "                splits = ideal_splits_from_target(stroke, distance, target_time_str=target, personal_best_str=pb)\n",
    "                print(\"\\nIdeal 50m splits:\")\n",
    "                print(pretty_print_splits(splits))\n",
    "                print(\"(semicolon-separated; times shown as M:SS.xx or SS.xx)\")\n",
    "                figure_compare_splits([], splits, title=f\"Ideal Splits — {stroke.title()} {distance}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[error] {e}\")\n",
    "        elif choice == \"2\":\n",
    "            try:\n",
    "                stroke = parse_stroke_input(ask(\"Stroke (Freestyle/Backstroke/Breaststroke/Butterfly/IM): \", cast=str))\n",
    "                distance = parse_distance_input(ask(\"Distance (e.g., 50/100/200/400): \", cast=str))\n",
    "                given = ask(\"Given splits (semicolon-separated, e.g., 32.33;33.11;...): \", cast=str)\n",
    "                pb = ask(\"Personal Best time (e.g., 1:45.23 or 65.23) [optional]: \", cast=str, allow_blank=True)\n",
    "                report = analyze_post_race(stroke, distance, given_splits_str=given, personal_best_str=pb)\n",
    "                print(\"\\n--- Post-race Analysis ---\")\n",
    "                print(f\"Event: {stroke.title()} {distance}m\")\n",
    "                print(f\"Given total: {seconds_to_time_str(report['total_given_sec'])}\")\n",
    "                print(f\"Ideal total (from model): {seconds_to_time_str(report['total_ideal_sec'])}\")\n",
    "                print(f\"Delta (Given - Ideal): {report['total_delta_sec']:+.2f} s\")\n",
    "                print(\"\\nSplit-by-split (Given vs Ideal | Δ):\")\n",
    "                for i, (g, idl, dlt) in enumerate(zip(report[\"given_splits_sec\"], report[\"ideal_splits_sec\"], report[\"split_delta_sec\"]), start=1):\n",
    "                    print(f\"  50#{i:>2}: {g:6.2f}  |  {idl:6.2f}  |  {dlt:+6.2f}\")\n",
    "                figure_compare_splits(report[\"given_splits_sec\"], report[\"ideal_splits_sec\"], \n",
    "                                      title=f\"Given vs Ideal — {stroke.title()} {distance}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[error] {e}\")\n",
    "        else:\n",
    "            print(\"Invalid choice.\")\n",
    "\n",
    "# --------------------------\n",
    "# Train from your datasets\n",
    "# --------------------------\n",
    "\n",
    "ALL_DATA = load_all_datasets()\n",
    "LEADERBOARD, MODEL_HINTS = load_leaderboard_and_hints()\n",
    "\n",
    "MODEL_STORE = SplitModelStore()\n",
    "MODEL_STORE.set_leaderboard(LEADERBOARD)\n",
    "MODEL_STORE.set_model_hints(MODEL_HINTS)\n",
    "MODEL_STORE.train_from_dataframe(ALL_DATA)\n",
    "\n",
    "print(\"=== Competitive Pacing (updated) ===\")\n",
    "print(f\"Loaded rows: {len(ALL_DATA)}\")\n",
    "if MODEL_HINTS:\n",
    "    print(\"Model hints by stroke:\", MODEL_HINTS)\n",
    "print(f\"Models trained for events: {sorted(list(MODEL_STORE.models.keys()))}\")\n",
    "if LEADERBOARD:\n",
    "    print(f\"Leaderboard baselines available for: {sorted(list(LEADERBOARD.keys()))}\")\n",
    "else:\n",
    "    print(\"No leaderboard ratios found; using trained models or heuristics.\")\n",
    "\n",
    "def list_available_events():\n",
    "    keys = set(MODEL_STORE.models.keys()) | set(MODEL_STORE.leaderboard.keys())\n",
    "    if not keys:\n",
    "        print(\"No events available yet. Add CSVs and re-run this cell.\")\n",
    "        return\n",
    "    print(\"Available (stroke, distance) events:\")\n",
    "    for k in sorted(keys):\n",
    "        print(\"  \", k)\n",
    "\n",
    "print(\"\\nTip: call list_available_events() to see which (stroke, distance) are model-ready.\")\n",
    "print(\"Run cli_loop() to start the interactive menu.\\n\")\n",
    "\n",
    "# Example:\n",
    "# list_available_events()\n",
    "# cli_loop()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
