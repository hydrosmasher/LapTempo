{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56e1e3e-bb97-41b7-ac5b-c644f299426b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import os, json, ast, re, warnings, math, sys, subprocess\n",
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.impute import SimpleImputer\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.model_selection import KFold, cross_val_score\n",
    "\n",
    "# Estimators\n",
    "from sklearn.ensemble import RandomForestRegressor as RandomForest\n",
    "from sklearn.ensemble import GradientBoostingRegressor as GBR\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet, LinearRegression\n",
    "from sklearn.svm import SVR\n",
    "\n",
    "ROOT = Path(\".\").resolve()\n",
    "DATASETS_DIR = ROOT / \"datasets\"\n",
    "MODELS_DIR = ROOT / \"models\"\n",
    "MODELS_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "LEADERBOARD_COMBINED = ROOT / \"best_models_by_stroke.csv\"\n",
    "\n",
    "# Individual leaderboards (optional; used to rebuild combined if present)\n",
    "LB_FILES = {\n",
    "    \"Freestyle\": ROOT / \"Freestyle_leaderboard.csv\",\n",
    "    \"Backstroke\": ROOT / \"Backstroke_leaderboard.csv\",\n",
    "    \"Breaststroke\": ROOT / \"Breaststroke_leaderboard.csv\",\n",
    "    \"Butterfly\": ROOT / \"Butterfly_leaderboard.csv\",\n",
    "    \"IM\": ROOT / \"IM_leaderboard.csv\",\n",
    "}\n",
    "\n",
    "STROKE_TO_FILE = {\n",
    "    \"Freestyle\": \"freestyle_dataset.csv\",\n",
    "    \"Backstroke\": \"backstroke_dataset.csv\",\n",
    "    \"Breaststroke\": \"breaststroke_dataset.csv\",\n",
    "    \"Butterfly\": \"butterfly_dataset.csv\",\n",
    "    \"IM\": \"im_dataset.csv\",\n",
    "}\n",
    "\n",
    "MODEL_MAP = {\n",
    "    \"RandomForest\": RandomForest,\n",
    "    \"GBR\": GBR,\n",
    "    \"Ridge\": Ridge,\n",
    "    \"Lasso\": Lasso,\n",
    "    \"ElasticNet\": ElasticNet,\n",
    "    \"LinearRegression\": LinearRegression,\n",
    "    \"SVR\": SVR,\n",
    "}\n",
    "\n",
    "SCALER_MODELS = {\"Ridge\",\"Lasso\",\"ElasticNet\",\"LinearRegression\",\"SVR\"}\n",
    "\n",
    "# Flexible target detection\n",
    "TARGET_PATTERNS = [\n",
    "    re.compile(r\"^frac_\\d+$\", flags=re.IGNORECASE),   # frac_1\n",
    "    re.compile(r\"^frac\\d+$\", flags=re.IGNORECASE),    # frac1\n",
    "    re.compile(r\"^FRAC_\\d+$\"),\n",
    "]\n",
    "\n",
    "def detect_target_cols(columns, expected_targets=None):\n",
    "    cols = []\n",
    "    for c in columns:\n",
    "        s = str(c)\n",
    "        if any(p.match(s) for p in TARGET_PATTERNS):\n",
    "            cols.append(c)\n",
    "    if expected_targets:\n",
    "        lower_expected = {t.lower() for t in expected_targets}\n",
    "        cols = [c for c in cols if str(c).lower() in lower_expected or str(c).lower().replace(\"_\",\"\") in lower_expected]\n",
    "    return cols\n",
    "\n",
    "def normalize_target_name(name: str):\n",
    "    s = str(name)\n",
    "    if re.match(r\"^frac\\d+$\", s, flags=re.IGNORECASE):\n",
    "        return re.sub(r\"(?i)^frac(\\d+)$\", r\"frac_\\1\", s.lower())\n",
    "    return s\n",
    "\n",
    "\n",
    "# Silence sklearn SimpleImputer all-NaN warnings (we already handle features safely)\n",
    "import warnings as _warn\n",
    "_warn.filterwarnings('ignore', message='Skipping features without any observed values', category=UserWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12a9280d-be59-4aed-8a90-58b183a73a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "available = []\n",
    "if DATASETS_DIR.exists():\n",
    "    for f in DATASETS_DIR.glob(\"*.csv\"):\n",
    "        available.append(f.name)\n",
    "\n",
    "print(\"Found dataset files:\", available)\n",
    "missing = [v for v in STROKE_TO_FILE.values() if v not in available]\n",
    "if missing:\n",
    "    print(\"[WARN] Missing expected dataset files:\", missing)\n",
    "else:\n",
    "    print(\"[OK] All expected datasets present.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59a0f21-6760-48ae-8536-7473022736e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Diagnostics: show columns for each dataset (head only)\n",
    "for stroke, fname in STROKE_TO_FILE.items():\n",
    "    p = DATASETS_DIR / fname\n",
    "    if p.exists():\n",
    "        dfh = pd.read_csv(p, nrows=3)\n",
    "        print(f\"\\n[{stroke}] {fname} → columns: {list(dfh.columns)}\")\n",
    "        display(dfh.head(3))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a95a01b7-f1e6-47c9-b1cb-df54fb05a4c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def pick_best(df):\n",
    "    df = df.copy()\n",
    "    df.columns = [c.strip().lower() for c in df.columns]\n",
    "    df_sorted = df.sort_values(by=[\"r2\",\"mse\"], ascending=[False, True])\n",
    "    best = df_sorted.groupby(\"target\", as_index=False).first()\n",
    "    return best\n",
    "\n",
    "frames = []\n",
    "if not LEADERBOARD_COMBINED.exists():\n",
    "    print(\"[INFO] Combined leaderboard not found. Recomputing from individual files if available...\")\n",
    "    for stroke, path in LB_FILES.items():\n",
    "        if path.exists():\n",
    "            try:\n",
    "                df = pd.read_csv(path)\n",
    "                best = pick_best(df)\n",
    "                best.insert(0, \"stroke\", stroke)\n",
    "                frames.append(best)\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN] Failed to read {path}: {e}\")\n",
    "    if frames:\n",
    "        combined = pd.concat(frames, ignore_index=True)\n",
    "        combined.to_csv(LEADERBOARD_COMBINED, index=False)\n",
    "        print(\"[OK] Wrote combined leaderboard to\", LEADERBOARD_COMBINED)\n",
    "    else:\n",
    "        print(\"[WARN] Per-stroke leaderboards not found. Will try local 'best_models_by_stroke.csv' or fallback next.\")\n",
    "else:\n",
    "    print(\"[OK] Found combined leaderboard at\", LEADERBOARD_COMBINED)\n",
    "\n",
    "# If still missing, try local sibling or embedded fallback\n",
    "if not LEADERBOARD_COMBINED.exists():\n",
    "    candidate = Path(\"best_models_by_stroke.csv\")\n",
    "    if candidate.exists():\n",
    "        combined = pd.read_csv(candidate)\n",
    "        combined.to_csv(LEADERBOARD_COMBINED, index=False)\n",
    "        print(\"[OK] Loaded local best_models_by_stroke.csv into\", LEADERBOARD_COMBINED)\n",
    "    else:\n",
    "        # Will be handled by next cell that may embed fallback if provided.\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d0ad7ba-7ab4-45f0-8a54-b0d39a978868",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Embedded fallback (auto-written only if nothing else was found)\n",
    "_fallback_csv_payload = r\"\"\"stroke,target,model,bestparams,r2,mse\n",
    "Freestyle,frac_1,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.999870243378946,3.446300985404293e-06\n",
    "Freestyle,frac_10,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9998943409633224,2.269611027931617e-08\n",
    "Freestyle,frac_11,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9998884575181034,2.403124123045788e-08\n",
    "Freestyle,frac_12,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9998824672075748,2.5413726915240372e-08\n",
    "Freestyle,frac_13,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.999880564246292,2.5981500273515697e-08\n",
    "Freestyle,frac_14,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9998710225436998,2.8192505027017152e-08\n",
    "Freestyle,frac_15,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9998765681982152,2.7097569500937284e-08\n",
    "Freestyle,frac_16,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9964365193390609,7.678266339955384e-07\n",
    "Freestyle,frac_17,Ridge,{'model__alpha': 0.1},0.0,2.988990637699851e-08\n",
    "Freestyle,frac_18,Ridge,{'model__alpha': 0.1},0.0,2.5349672450572248e-08\n",
    "Freestyle,frac_19,Ridge,{'model__alpha': 0.1},0.0,2.2408701089148216e-08\n",
    "Freestyle,frac_2,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9998867955175011,3.57427629763549e-06\n",
    "Freestyle,frac_20,Ridge,{'model__alpha': 0.1},0.0,1.5852440136225758e-08\n",
    "Freestyle,frac_21,Ridge,{'model__alpha': 0.1},0.0,1.2104395266390137e-08\n",
    "Freestyle,frac_22,Ridge,{'model__alpha': 0.1},0.0,1.202243271136594e-08\n",
    "Freestyle,frac_23,Ridge,{'model__alpha': 0.1},0.0,1.1919455483054742e-08\n",
    "Freestyle,frac_24,Ridge,{'model__alpha': 0.1},0.0,1.7149887415871123e-08\n",
    "Freestyle,frac_25,Ridge,{'model__alpha': 0.1},0.0,3.0218453715648795e-08\n",
    "Freestyle,frac_26,Ridge,{'model__alpha': 0.1},0.0,1.4024195867334521e-07\n",
    "Freestyle,frac_27,Ridge,{'model__alpha': 0.1},0.0,6.314455592110409e-07\n",
    "Freestyle,frac_28,Ridge,{'model__alpha': 0.1},0.0,8.880265920459282e-07\n",
    "Freestyle,frac_29,Ridge,{'model__alpha': 0.1},0.0,8.688721906725003e-07\n",
    "Freestyle,frac_3,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9998709600381422,9.556862590077076e-07\n",
    "Freestyle,frac_30,Ridge,{'model__alpha': 0.1},0.0,8.309361340034765e-07\n",
    "Freestyle,frac_4,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9999064518848968,6.696272014015025e-07\n",
    "Freestyle,frac_5,RandomForest,\"{'model__max_depth': 3, 'model__n_estimators': 100}\",0.9999709211876188,4.2780859101891336e-08\n",
    "Freestyle,frac_6,RandomForest,\"{'model__max_depth': 3, 'model__n_estimators': 100}\",0.9999532784999926,6.87564897870968e-08\n",
    "Freestyle,frac_7,RandomForest,\"{'model__max_depth': 3, 'model__n_estimators': 100}\",0.9999334164741176,9.807550310729845e-08\n",
    "Freestyle,frac_8,RandomForest,\"{'model__max_depth': 3, 'model__n_estimators': 50}\",0.9992697709056444,1.1442479334816152e-06\n",
    "Freestyle,frac_9,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9998878635463874,2.3983271145407182e-08\n",
    "Backstroke,frac_1,RandomForest,\"{'model__max_depth': 3, 'model__n_estimators': 50}\",0.9994268075799432,8.867917637670797e-06\n",
    "Backstroke,frac_2,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9995320930324364,7.929250320901907e-06\n",
    "Backstroke,frac_3,Ridge,{'model__alpha': 0.1},0.0,8.36422372652802e-06\n",
    "Backstroke,frac_4,Ridge,{'model__alpha': 0.1},0.0,1.756630052177816e-05\n",
    "Breaststroke,frac_1,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9988548437938534,1.6389929209266215e-05\n",
    "Breaststroke,frac_2,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9990713463228648,1.8044181130454614e-05\n",
    "Breaststroke,frac_3,Ridge,{'model__alpha': 0.1},0.0,3.1925173522318018e-06\n",
    "Breaststroke,frac_4,Ridge,{'model__alpha': 0.1},0.0,1.0488003264994874e-05\n",
    "Butterfly,frac_1,RandomForest,\"{'model__max_depth': 3, 'model__n_estimators': 50}\",0.9987736496840494,1.8391454265397885e-05\n",
    "Butterfly,frac_2,RandomForest,\"{'model__max_depth': 3, 'model__n_estimators': 100}\",0.9991241698770432,1.6971678700986156e-05\n",
    "Butterfly,frac_3,Ridge,{'model__alpha': 0.1},0.0,6.019565155644881e-06\n",
    "Butterfly,frac_4,Ridge,{'model__alpha': 0.1},0.0,2.3645284938661704e-05\n",
    "IM,frac_1,RandomForest,\"{'model__max_depth': 3, 'model__n_estimators': 50}\",0.9977444968117418,6.883277693457198e-06\n",
    "IM,frac_2,RandomForest,\"{'model__max_depth': 3, 'model__n_estimators': 100}\",0.9974358107136986,1.1070725017522065e-05\n",
    "IM,frac_3,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9965213315108744,2.2576616555748427e-05\n",
    "IM,frac_4,GBR,\"{'model__learning_rate': 0.1, 'model__n_estimators': 100}\",0.9977884821132098,7.467268845815134e-06\n",
    "IM,frac_5,Ridge,{'model__alpha': 0.1},0.0,7.963029513171503e-06\n",
    "IM,frac_6,Ridge,{'model__alpha': 0.1},0.0,1.4448839049173334e-05\n",
    "IM,frac_7,Ridge,{'model__alpha': 0.1},0.0,9.62360429367415e-06\n",
    "IM,frac_8,Ridge,{'model__alpha': 0.1},0.0,9.857673151697714e-06\n",
    "\"\"\"\n",
    "\n",
    "if not LEADERBOARD_COMBINED.exists():\n",
    "    if _fallback_csv_payload.strip():\n",
    "        Path(\"best_models_by_stroke.csv\").write_text(_fallback_csv_payload)\n",
    "        combined = pd.read_csv(\"best_models_by_stroke.csv\")\n",
    "        combined.to_csv(LEADERBOARD_COMBINED, index=False)\n",
    "        print(\"[OK] Wrote embedded fallback to\", LEADERBOARD_COMBINED)\n",
    "    else:\n",
    "        raise FileNotFoundError(\"No leaderboards available. Place 'best_models_by_stroke.csv' next to this notebook.\")\n",
    "else:\n",
    "    combined = pd.read_csv(LEADERBOARD_COMBINED)\n",
    "\n",
    "display(combined.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb57e033-636b-41b2-ac09-b3c0cbd33a5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_bestparams(s: str) -> dict:\n",
    "    if pd.isna(s) or not str(s).strip():\n",
    "        return {}\n",
    "    try:\n",
    "        d = ast.literal_eval(s)\n",
    "    except Exception:\n",
    "        d = {}\n",
    "        s2 = str(s).strip().strip(\"{}\")\n",
    "        for part in s2.split(\",\"):\n",
    "            if not part.strip(): continue\n",
    "            if \":\" in part:\n",
    "                k, v = part.split(\":\", 1)\n",
    "                try:\n",
    "                    d[k.strip().strip(\"'\").strip('\"')] = ast.literal_eval(v.strip())\n",
    "                except Exception:\n",
    "                    d[k.strip().strip(\"'\").strip('\"')] = v.strip()\n",
    "    return d\n",
    "\n",
    "def make_pipeline(model_name: str, params: dict, feature_cols):\n",
    "    model_cls = MODEL_MAP[model_name]\n",
    "    model = model_cls()\n",
    "\n",
    "    p2 = {}\n",
    "    for k, v in params.items():\n",
    "        if k.startswith(\"model__\"):\n",
    "            p2[k] = v\n",
    "        else:\n",
    "            p2[f\"model__{k}\"] = v\n",
    "\n",
    "    num_pipe = Pipeline([(\"imputer\", SimpleImputer(strategy=\"median\")), (\"scaler\", StandardScaler() if model_name in SCALER_MODELS else \"passthrough\")])\n",
    "    pre = ColumnTransformer([(\"num\", num_pipe, list(feature_cols))], remainder=\"drop\")\n",
    "    pipe = Pipeline([(\"pre\", pre), (\"model\", model)])\n",
    "    if p2:\n",
    "        pipe.set_params(**p2)\n",
    "    return pipe\n",
    "\n",
    "def kfold_metrics(model, X, y, n_splits=5):\n",
    "    n_splits = min(n_splits, max(2, len(y)//2))  # shrink folds if dataset is small\n",
    "    cv = KFold(n_splits=n_splits, shuffle=True, random_state=42)\n",
    "    with warnings.catch_warnings():\n",
    "        warnings.simplefilter(\"ignore\")\n",
    "        r2 = cross_val_score(model, X, y, cv=cv, scoring=\"r2\")\n",
    "        neg_mse = cross_val_score(model, X, y, cv=cv, scoring=\"neg_mean_squared_error\")\n",
    "    return {\n",
    "        \"r2_mean\": float(np.mean(r2)), \"r2_std\": float(np.std(r2)),\n",
    "        \"mse_mean\": float(np.mean(-neg_mse)), \"mse_std\": float(np.std(neg_mse)),\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "144a9f06-c11a-42c7-9bdc-cdfcdfeb88a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_splits_to_list(s):\n",
    "    if pd.isna(s):\n",
    "        return None\n",
    "    nums = re.findall(r\"[-+]?\\d*\\.?\\d+\", str(s))\n",
    "    if not nums:\n",
    "        return None\n",
    "    return [float(x) for x in nums]\n",
    "\n",
    "def make_frac_targets_from_splits(df_in: pd.DataFrame, expected_targets: list):\n",
    "    df = df_in.copy()\n",
    "    split_col = None\n",
    "    for c in [\n",
    "        'add splits per 50 with (;) between them\\n (eg. \"33.46; 35.67; 36.88; 33.84\")\\n do not put words in this question',\n",
    "        'splits', 'Splits'\n",
    "    ]:\n",
    "        if c in df.columns:\n",
    "            split_col = c\n",
    "            break\n",
    "\n",
    "    if split_col is None:\n",
    "        return df\n",
    "\n",
    "    splits = df[split_col].apply(parse_splits_to_list)\n",
    "    total = splits.apply(lambda lst: np.sum(lst) if isinstance(lst, list) and len(lst)>0 else np.nan)\n",
    "\n",
    "    max_frac = 0\n",
    "    for t in expected_targets or []:\n",
    "        m = re.match(r\"(?i)^frac[_]?(\\d+)$\", str(t))\n",
    "        if m:\n",
    "            max_frac = max(max_frac, int(m.group(1)))\n",
    "\n",
    "    for i in range(1, max(1, max_frac)+1):\n",
    "        df[f\"frac_{i}\"] = np.nan\n",
    "\n",
    "    for idx, lst in splits.items():\n",
    "        if isinstance(lst, list) and len(lst) > 0 and not pd.isna(total.loc[idx]):\n",
    "            denom = total.loc[idx]\n",
    "            if denom:\n",
    "                for i in range(1, len(lst)+1):\n",
    "                    col = f\"frac_{i}\"\n",
    "                    if col in df.columns:\n",
    "                        df.at[idx, col] = lst[i-1] / denom\n",
    "    return df\n",
    "\n",
    "def engineer_numeric_features(df_in: pd.DataFrame):\n",
    "    df = df_in.copy()\n",
    "    df[\"Distance\"] = pd.to_numeric(df.get(\"Distance\"), errors=\"coerce\")\n",
    "\n",
    "    pool_map = {\"LCM 50\": 50, \"SCM 25\": 25, \"LCM50\": 50, \"SCM25\": 25}\n",
    "    if \"pool\" in df.columns:\n",
    "        df[\"pool_len\"] = df[\"pool\"].map(pool_map)\n",
    "\n",
    "    df[\"athlete age\"] = pd.to_numeric(df.get(\"athlete age\"), errors=\"coerce\")\n",
    "\n",
    "    strat_map = {\"positive\": -1, \"even\": 0, \"negative\": 1, \"all-out sprint\": 2}\n",
    "    if \"Strategy\" in df.columns:\n",
    "        df[\"strategy_code\"] = df[\"Strategy\"].map(strat_map).fillna(0)\n",
    "    else:\n",
    "        df[\"strategy_code\"] = 0\n",
    "\n",
    "    df[\"final_time_sec\"] = pd.to_numeric(df.get(\"Final time in seconds\"), errors=\"coerce\")\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0d2ade0-4bc3-4f92-b38c-59fe659a2954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import joblib\n",
    "\n",
    "build_report = []\n",
    "\n",
    "for stroke, sub in combined.groupby(\"stroke\"):\n",
    "    ds_name = STROKE_TO_FILE.get(stroke)\n",
    "    ds_path = DATASETS_DIR / ds_name if ds_name else None\n",
    "\n",
    "    if not ds_path or not ds_path.exists():\n",
    "        print(f\"[WARN] Dataset for {stroke} not found at {ds_path}. Skipping this stroke.\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(ds_path)\n",
    "\n",
    "    expected_targets = list(sub[\"target\"].astype(str).unique())\n",
    "    df = make_frac_targets_from_splits(df, expected_targets=expected_targets)\n",
    "    df = engineer_numeric_features(df)\n",
    "    # Drop columns that are entirely NaN (e.g., IM stroke labels)\n",
    "    df = df.dropna(axis=1, how='all')\n",
    "\n",
    "    # Detect targets now present\n",
    "    target_cols = detect_target_cols(df.columns, expected_targets=expected_targets)\n",
    "\n",
    "    # features: numeric not in targets\n",
    "    feature_cols = [c for c in df.select_dtypes(include=[np.number]).columns if c not in target_cols]\n",
    "    if not feature_cols:\n",
    "        print(f\"[ERROR] No numeric features found for {stroke}. Check dataset columns.\")\n",
    "        continue\n",
    "\n",
    "    print(f\"\\n[INFO] {stroke}: Using {len(feature_cols)} features and {len(target_cols)} targets: {target_cols}\")\n",
    "\n",
    "    stroke_dir = MODELS_DIR / stroke\n",
    "    stroke_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    for _, row in sub.iterrows():\n",
    "        target = row[\"target\"]\n",
    "        model_name = row[\"model\"]\n",
    "        params = parse_bestparams(row[\"bestparams\"])\n",
    "\n",
    "        # Normalize target name to 'frac_i'\n",
    "        t_norm = normalize_target_name(target)\n",
    "\n",
    "        if t_norm not in df.columns:\n",
    "            print(f\"[WARN] Target '{t_norm}' missing after split parsing for {stroke}; skipping.\")\n",
    "            continue\n",
    "\n",
    "        X = df[feature_cols].copy()\n",
    "        y = df[t_norm].astype(float).values\n",
    "\n",
    "        # Drop rows where target is NaN or not finite\n",
    "        import numpy as _np\n",
    "        mask = _np.isfinite(y)\n",
    "        if mask.sum() < len(y):\n",
    "            X = X.loc[mask]\n",
    "            y = y[mask]\n",
    "        # If still too few samples or no variation, skip\n",
    "        if len(y) < 5 or (len(_np.unique(y)) <= 1):\n",
    "            print(f\"[WARN] Not enough valid samples for {stroke}/{t_norm} (n={len(y)}). Skipping.\")\n",
    "            continue\n",
    "\n",
    "        pipe = make_pipeline(model_name, params, feature_cols)\n",
    "\n",
    "        try:\n",
    "            cvm = kfold_metrics(pipe, X, y, n_splits=5)\n",
    "        except Exception as e:\n",
    "            print(f\"[WARN] CV failed for {stroke}/{t_norm} with {model_name}: {e}\")\n",
    "            cvm = None\n",
    "\n",
    "        pipe.fit(X, y)\n",
    "\n",
    "        model_path = stroke_dir / f\"{t_norm}_{model_name}.joblib\"\n",
    "        joblib.dump(pipe, model_path)\n",
    "\n",
    "        meta = {\n",
    "            \"stroke\": stroke,\n",
    "            \"target\": t_norm,\n",
    "            \"model\": model_name,\n",
    "            \"bestparams\": params,\n",
    "            \"features\": feature_cols,\n",
    "            \"dataset\": str(ds_path),\n",
    "            \"model_path\": str(model_path),\n",
    "            \"cv_metrics\": cvm,\n",
    "        }\n",
    "        build_report.append(meta)\n",
    "        print(f\"[OK] Saved {stroke}/{t_norm} -> {model_path.name}\")\n",
    "\n",
    "    with open(stroke_dir / \"index.json\", \"w\") as f:\n",
    "        json.dump([m for m in build_report if m[\"stroke\"] == stroke], f, indent=2)\n",
    "\n",
    "# global report\n",
    "with open(MODELS_DIR / \"build_report.json\", \"w\") as f:\n",
    "    json.dump(build_report, f, indent=2)\n",
    "\n",
    "print(\"\\n[DONE] Training complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afb7bbec-0d16-4ee2-8152-fcb7360ef3fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_df = pd.DataFrame(build_report)\n",
    "if report_df.empty:\n",
    "    print(\"[INFO] No models were trained. See warnings above about missing or unparsable targets.\")\n",
    "else:\n",
    "    display(report_df.head(30))\n",
    "    report_csv = MODELS_DIR / \"build_report.csv\"\n",
    "    report_df.to_csv(report_csv, index=False)\n",
    "    print(\"Saved:\", report_csv)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b702e8f1-1e50-4120-870a-866816a10710",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, joblib\n",
    "\n",
    "def list_trained(stroke: str):\n",
    "    idx_path = MODELS_DIR / stroke / \"index.json\"\n",
    "    if not idx_path.exists():\n",
    "        return []\n",
    "    with open(idx_path, \"r\") as f:\n",
    "        meta = json.load(f)\n",
    "    return meta\n",
    "\n",
    "def load_model(stroke: str, target: str):\n",
    "    metas = list_trained(stroke)\n",
    "    target_norm = normalize_target_name(target)\n",
    "    candidates = [m for m in metas if normalize_target_name(m[\"target\"]) == target_norm]\n",
    "    if not candidates:\n",
    "        raise FileNotFoundError(f\"No trained model for {stroke}/{target_norm}\")\n",
    "    m = candidates[0]\n",
    "    path = Path(m[\"model_path\"])\n",
    "    if not path.exists():\n",
    "        raise FileNotFoundError(f\"Model file missing at {path}\")\n",
    "    return joblib.load(path), m\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e6c15ab-166c-4211-80cb-cffcd831544e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example: Freestyle frac_1 (adjust as needed)\n",
    "example_stroke = \"Freestyle\"\n",
    "example_target = \"frac_1\"\n",
    "\n",
    "# Load dataset to get feature columns\n",
    "ds_path = DATASETS_DIR / STROKE_TO_FILE[example_stroke]\n",
    "if ds_path.exists():\n",
    "    df_demo = pd.read_csv(ds_path)\n",
    "    df_demo = make_frac_targets_from_splits(df_demo, expected_targets=[example_target])\n",
    "    df_demo = engineer_numeric_features(df_demo)\n",
    "\n",
    "    targets_demo = detect_target_cols(df_demo.columns, expected_targets=[example_target])\n",
    "    feats_demo = [c for c in df_demo.select_dtypes(include=[np.number]).columns if c not in targets_demo]\n",
    "\n",
    "    if feats_demo:\n",
    "        try:\n",
    "            pipe, meta = load_model(example_stroke, example_target)\n",
    "            X_sample = df_demo[feats_demo].head(5)\n",
    "            y_pred = pipe.predict(X_sample)\n",
    "            print(\"Loaded model:\", meta[\"model\"], \"for\", example_stroke, example_target)\n",
    "            print(\"Predictions:\", y_pred)\n",
    "        except Exception as e:\n",
    "            print(\"[INFO] Demo couldn't run:\", e)\n",
    "    else:\n",
    "        print(\"[INFO] No numeric features to demo.\")\n",
    "else:\n",
    "    print(\"[INFO] Demo dataset not found:\", ds_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fae685e9-9814-4f1a-beb9-40dc7e3d9e47",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Competitive Pacing: Simple CLI (Pre-race & Post-race) ---\n",
    "# - Uses your existing per-stroke frac_k models from models/<Stroke>/index.json\n",
    "# - No changes to your training code are needed\n",
    "# - Adds an option to save the comparison/ideal chart as a JPEG\n",
    "\n",
    "import re, json\n",
    "from pathlib import Path\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import joblib\n",
    "import matplotlib.pyplot as plt\n",
    "from IPython.display import display\n",
    "\n",
    "# ----------------------------\n",
    "# Basic config / constants\n",
    "# ----------------------------\n",
    "ROOT = Path(\".\").resolve()\n",
    "MODELS_DIR = ROOT / \"models\"\n",
    "STROKES = [\"Freestyle\", \"Backstroke\", \"Breaststroke\", \"Butterfly\", \"IM\"]\n",
    "\n",
    "# ----------------------------\n",
    "# Small helpers\n",
    "# ----------------------------\n",
    "def parse_time_to_seconds(s: str) -> float:\n",
    "    s = str(s).strip()\n",
    "    if not s:\n",
    "        return np.nan\n",
    "    if \":\" not in s:\n",
    "        try:\n",
    "            return float(s)\n",
    "        except:\n",
    "            return np.nan\n",
    "    parts = s.split(\":\")\n",
    "    try:\n",
    "        parts = [float(p) for p in parts]\n",
    "    except:\n",
    "        return np.nan\n",
    "    secs = 0.0\n",
    "    for p in parts:\n",
    "        secs = secs * 60.0 + p\n",
    "    return secs\n",
    "\n",
    "def seconds_to_time_str(x: float) -> str:\n",
    "    if x is None or np.isnan(x):\n",
    "        return \"NA\"\n",
    "    x = float(x)\n",
    "    m = int(x // 60)\n",
    "    s = x - 60*m\n",
    "    return f\"{m}:{s:05.2f}\"\n",
    "\n",
    "def parse_splits_string(s: str) -> List[float]:\n",
    "    nums = re.findall(r\"[-+]?\\d*\\.?\\d+\", str(s))\n",
    "    return [float(x) for x in nums] if nums else []\n",
    "\n",
    "def expected_num_splits(distance: int) -> int:\n",
    "    return distance // 50 if distance and distance > 0 else 0\n",
    "\n",
    "def frac_name(i: int) -> str:\n",
    "    return f\"frac_{i}\"\n",
    "\n",
    "def print_splits(splits: List[float]) -> str:\n",
    "    return \"; \".join(f\"{v:.2f}\" for v in splits)\n",
    "\n",
    "# ----------------------------\n",
    "# Model loading / inference\n",
    "# ----------------------------\n",
    "def load_index(stroke: str) -> List[Dict]:\n",
    "    idx = MODELS_DIR / stroke / \"index.json\"\n",
    "    if not idx.exists():\n",
    "        raise FileNotFoundError(f\"Missing index for {stroke}: {idx}\")\n",
    "    meta = json.loads(idx.read_text())\n",
    "    meta = [m for m in meta if re.match(r\"(?i)^frac(_)?\\d+$\", str(m.get(\"target\",\"\")))]\n",
    "    for m in meta:\n",
    "        t = str(m[\"target\"]).lower()\n",
    "        if re.match(r\"^frac\\d+$\", t):\n",
    "            t = re.sub(r\"^frac(\\d+)$\", r\"frac_\\1\", t)\n",
    "        m[\"target\"] = t\n",
    "    meta.sort(key=lambda d: int(re.findall(r\"\\d+\", d[\"target\"])[0]))\n",
    "    return meta\n",
    "\n",
    "def load_frac_models(stroke: str) -> Tuple[Dict[str, object], Dict[str, Dict]]:\n",
    "    metas = load_index(stroke)\n",
    "    models, meta_map = {}, {}\n",
    "    for m in metas:\n",
    "        p = Path(m[\"model_path\"])\n",
    "        if not p.exists():\n",
    "            alt = MODELS_DIR / stroke / p.name\n",
    "            if alt.exists(): p = alt\n",
    "        if not p.exists():\n",
    "            raise FileNotFoundError(f\"Model file missing: {p}\")\n",
    "        models[m[\"target\"]] = joblib.load(p)\n",
    "        meta_map[m[\"target\"]] = m\n",
    "    return models, meta_map\n",
    "\n",
    "def build_feature_row(required_feats: List[str], distance_m: int, total_time_s: float) -> pd.DataFrame:\n",
    "    row = {f: np.nan for f in required_feats}\n",
    "    if \"Distance\" in row: row[\"Distance\"] = float(distance_m)\n",
    "    if \"final_time_sec\" in row: row[\"final_time_sec\"] = float(total_time_s)\n",
    "    if \"Final time in seconds\" in row: row[\"Final time in seconds\"] = float(total_time_s)\n",
    "    return pd.DataFrame([row])\n",
    "\n",
    "def predict_fractions(stroke: str, distance_m: int, total_time_s: float) -> pd.Series:\n",
    "    k = expected_num_splits(distance_m)\n",
    "    if k <= 0: raise ValueError(\"Distance must be a positive multiple of 50.\")\n",
    "    models, metas = load_frac_models(stroke)\n",
    "\n",
    "    preds = []\n",
    "    for i in range(1, k+1):\n",
    "        t = frac_name(i)\n",
    "        if t not in models: break\n",
    "        pipe = models[t]\n",
    "        feats = metas[t].get(\"features\", [])\n",
    "        X = build_feature_row(feats, distance_m, total_time_s)\n",
    "        y = float(pipe.predict(X)[0])\n",
    "        preds.append((t, max(0.0, y)))\n",
    "    if not preds:\n",
    "        raise RuntimeError(f\"No usable frac models for {stroke} at {distance_m}m.\")\n",
    "    ser = pd.Series(dict(preds))\n",
    "    total = ser.sum()\n",
    "    ser = ser / total if total > 0 else pd.Series(np.full(len(ser), 1/len(ser)), index=ser.index)\n",
    "    return ser\n",
    "\n",
    "def ideal_splits_seconds(stroke: str, distance_m: int, total_time_s: float) -> List[float]:\n",
    "    fracs = predict_fractions(stroke, distance_m, total_time_s)\n",
    "    return [float(f * total_time_s) for f in fracs.values]\n",
    "\n",
    "# ----------------------------\n",
    "# Analysis / plotting\n",
    "# ----------------------------\n",
    "def compare_df(given: List[float], ideal: List[float]) -> pd.DataFrame:\n",
    "    n = min(len(given), len(ideal))\n",
    "    df = pd.DataFrame({\n",
    "        \"Distance (m)\": [i*50 for i in range(1, n+1)],\n",
    "        \"Given (s)\": [float(x) for x in given[:n]],\n",
    "        \"Ideal (s)\": [float(x) for x in ideal[:n]],\n",
    "    })\n",
    "    df[\"Delta (Given - Ideal)\"] = df[\"Given (s)\"] - df[\"Ideal (s)\"]\n",
    "    return df\n",
    "\n",
    "def plot_lines(x, y_list, labels, title, xlabel, ylabel, save_jpeg_path: str = None):\n",
    "    plt.figure(figsize=(8,4))\n",
    "    for y, lab in zip(y_list, labels):\n",
    "        plt.plot(x, y, marker=\"o\", label=lab)\n",
    "    plt.title(title)\n",
    "    plt.xlabel(xlabel)\n",
    "    plt.ylabel(ylabel)\n",
    "    plt.grid(True, alpha=0.3)\n",
    "    plt.legend()\n",
    "    if save_jpeg_path:\n",
    "        plt.tight_layout()\n",
    "        plt.savefig(save_jpeg_path, dpi=200, format=\"jpeg\")\n",
    "        print(f\"Saved chart: {save_jpeg_path}\")\n",
    "    plt.show()\n",
    "\n",
    "# ----------------------------\n",
    "# Tiny prompt helpers\n",
    "# ----------------------------\n",
    "def pick_from_list(prompt: str, options: List[str]) -> str:\n",
    "    print(prompt)\n",
    "    for i, opt in enumerate(options, 1):\n",
    "        print(f\"  {i}. {opt}\")\n",
    "    while True:\n",
    "        s = input(\"> \").strip()\n",
    "        if s.isdigit() and 1 <= int(s) <= len(options):\n",
    "            return options[int(s)-1]\n",
    "        for opt in options:\n",
    "            if s.lower() == opt.lower():\n",
    "                return opt\n",
    "        print(\"Choose a valid option.\")\n",
    "\n",
    "def ask_int(prompt: str) -> int:\n",
    "    while True:\n",
    "        s = input(prompt).strip()\n",
    "        try:\n",
    "            v = int(s)\n",
    "            return v\n",
    "        except:\n",
    "            print(\"Please enter an integer.\")\n",
    "\n",
    "def ask_time_seconds(prompt: str) -> float:\n",
    "    while True:\n",
    "        s = input(prompt).strip()\n",
    "        v = parse_time_to_seconds(s)\n",
    "        if np.isfinite(v): return v\n",
    "        print(\"Please enter a valid time (e.g., 59.80 or 1:05.23).\")\n",
    "\n",
    "def ask_yes_no(prompt: str) -> bool:\n",
    "    s = input(prompt + \" [y/n]: \").strip().lower()\n",
    "    return s in (\"y\",\"yes\")\n",
    "\n",
    "def ask_save_path(default_name: str) -> str:\n",
    "    s = input(f\"Enter JPEG filename (or press Enter for '{default_name}'): \").strip()\n",
    "    if not s:\n",
    "        s = default_name\n",
    "    if not s.lower().endswith(\".jpg\") and not s.lower().endswith(\".jpeg\"):\n",
    "        s += \".jpg\"\n",
    "    return s\n",
    "\n",
    "# ----------------------------\n",
    "# CLI modes\n",
    "# ----------------------------\n",
    "def cli_pre_race():\n",
    "    print(\"\\n=== PRE-RACE: Ideal Splits ===\")\n",
    "    stroke = pick_from_list(\"Select stroke:\", STROKES)\n",
    "    distance = ask_int(\"Enter distance (e.g., 100, 200, 400): \")\n",
    "    target_sec = ask_time_seconds(\"Enter TARGET time(in seconds): \")\n",
    "\n",
    "    try:\n",
    "        ideal = ideal_splits_seconds(stroke, distance, target_sec)\n",
    "    except Exception as e:\n",
    "        print(f\"Prediction failed: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nIdeal splits (s per 50m):\")\n",
    "    print(print_splits(ideal))\n",
    "    print(f\"Target total: {seconds_to_time_str(target_sec)}\")\n",
    "\n",
    "    x = [i*50 for i in range(1, len(ideal)+1)]\n",
    "    title = f\"Ideal Splits — {stroke} {distance}m (target {seconds_to_time_str(target_sec)})\"\n",
    "    save_path = None\n",
    "    if ask_yes_no(\"Save chart as JPEG?\"):\n",
    "        save_path = ask_save_path(f\"ideal_{stroke}_{distance}m.jpg\")\n",
    "    plot_lines(x, [ideal], [\"Ideal\"], title, \"Distance (m)\", \"Split (s)\", save_path)\n",
    "\n",
    "def cli_post_race():\n",
    "    print(\"\\n=== POST-RACE: Given vs Ideal ===\")\n",
    "    stroke = pick_from_list(\"Select stroke:\", STROKES)\n",
    "    distance = ask_int(\"Enter distance (e.g., 100, 200, 400): \")\n",
    "    pb = ask_time_seconds(\"Enter PERSONAL BEST: \")\n",
    "    splits_str = input('Paste race splits per 50 separated by \";\" (e.g., 32.33; 33.22; 34.10):\\n> ')\n",
    "    given = parse_splits_string(splits_str)\n",
    "    if not given:\n",
    "        print(\"No numeric splits detected.\")\n",
    "        return\n",
    "\n",
    "    total_actual = float(np.sum(given))\n",
    "    try:\n",
    "        ideal = ideal_splits_seconds(stroke, distance, total_actual)\n",
    "    except Exception as e:\n",
    "        print(f\"Prediction failed: {e}\")\n",
    "        return\n",
    "\n",
    "    print(\"\\nGiven splits (s):\")\n",
    "    print(print_splits(given))\n",
    "    print(\"\\nIdeal splits (s):\")\n",
    "    print(print_splits(ideal))\n",
    "    print(f\"\\nActual total: {seconds_to_time_str(total_actual)} | PB: {seconds_to_time_str(pb)}\")\n",
    "\n",
    "    df = compare_df(given, ideal)\n",
    "    display(df)\n",
    "\n",
    "    x = df[\"Distance (m)\"].tolist()\n",
    "    title = f\"{stroke} {distance}m — Given vs Ideal (total {seconds_to_time_str(total_actual)})\"\n",
    "    save_path = None\n",
    "    if ask_yes_no(\"Save comparison chart as JPEG?\"):\n",
    "        save_path = ask_save_path(f\"compare_{stroke}_{distance}m.jpg\")\n",
    "    plot_lines(x, [df[\"Given (s)\"].tolist(), df[\"Ideal (s)\"].tolist()],\n",
    "               [\"Given\", \"Ideal\"], title, \"Distance (m)\", \"Split (s)\", save_path)\n",
    "\n",
    "# ----------------------------\n",
    "# Main loop\n",
    "# ----------------------------\n",
    "def main():\n",
    "    print(\"Competitive Pacing — Simple CLI\")\n",
    "    print(\"1) Pre-race (ideal splits)\")\n",
    "    print(\"2) Post-race (compare given vs ideal)\")\n",
    "    print(\"q) Quit\")\n",
    "    while True:\n",
    "        choice = input(\"\\nChoose 1 / 2 / q: \").strip().lower()\n",
    "        if choice == \"1\": cli_pre_race()\n",
    "        elif choice == \"2\": cli_post_race()\n",
    "        elif choice in {\"q\",\"quit\",\"exit\"}:\n",
    "            print(\"Bye.\")\n",
    "            break\n",
    "        else:\n",
    "            print(\"Please choose 1, 2, or q.\")\n",
    "\n",
    "# Run\n",
    "main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0962b0b-6da2-44ca-8f7b-abe39769010e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
