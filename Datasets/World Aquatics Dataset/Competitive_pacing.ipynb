{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80fe7f06-e15c-4d5f-bed7-eae54f1b4d01",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Competitive Pacing — resilient single-cell\n",
    "# ------------------------------------------\n",
    "# Improvements:\n",
    "# • Robust CSV loader: tries multiple encodings (utf-8, latin-1) and delimiter sniffing (sep=None), falls back to ',', ';', '\\t'.\n",
    "# • Reads your uploaded files from /mnt/data by default, with ./datasets fallback.\n",
    "# • Parses event strings like \"200 Free\" / \"100 Butterfly\" into distance+stroke if those columns are missing.\n",
    "# • Recognizes many split column styles: split_1, split50_1, 50m_1, lap_1, s50_1, etc., or a single \"splits\" string \"32.3;33.1;...\".\n",
    "# • Accepts a \"best_models_by_stroke.csv\" file for both (a) per-(stroke,distance) baseline pacing ratios and (b) model hints per stroke.\n",
    "\n",
    "import os, re, sys, math, textwrap, warnings, io\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from typing import Dict, Tuple, List, Optional\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor\n",
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.multioutput import MultiOutputRegressor\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# ------------------ helpers: stroke/time parsing ------------------\n",
    "\n",
    "STROKE_ALIASES = {\n",
    "    \"free\": \"freestyle\", \"freestyle\": \"freestyle\", \"fr\": \"freestyle\", \"fs\": \"freestyle\",\n",
    "    \"back\": \"backstroke\", \"backstroke\": \"backstroke\", \"bk\": \"backstroke\",\n",
    "    \"breast\": \"breaststroke\", \"breaststroke\": \"breaststroke\", \"br\": \"breaststroke\",\n",
    "    \"fly\": \"butterfly\", \"butterfly\": \"butterfly\", \"bf\": \"butterfly\",\n",
    "    \"im\": \"im\", \"medley\": \"im\", \"individual medley\": \"im\", \"individualmedley\": \"im\"\n",
    "}\n",
    "STROKE_WORDS = [\"freestyle\",\"free\",\"backstroke\",\"back\",\"breaststroke\",\"breast\",\"butterfly\",\"fly\",\"im\",\"medley\"]\n",
    "\n",
    "TOTAL_TIME_CANDIDATES = [\"time_total\",\"total_time\",\"final_time\",\"result_time\",\"time\",\"seed_time\",\"pb_time\",\"official_time\"]\n",
    "DISTANCE_CANDIDATES   = [\"distance\",\"event_distance\",\"dist\",\"meters\",\"metres\",\"metres(m)\"]\n",
    "STROKE_CANDIDATES     = [\"stroke\",\"event_stroke\",\"style\",\"st\"]\n",
    "EVENT_CANDIDATES      = [\"event\",\"event_name\",\"race\",\"race_name\"]\n",
    "SPLITS_BUNDLE         = [\"splits\",\"split_string\",\"lap_splits\"]\n",
    "\n",
    "def normalize_stroke(x: str) -> Optional[str]:\n",
    "    if x is None or (isinstance(x, float) and math.isnan(x)): return None\n",
    "    s = str(x).strip().lower().replace(\"-\", \" \")\n",
    "    s = re.sub(r'[^a-z ]+', ' ', s)\n",
    "    s = re.sub(r'\\s+', ' ', s)\n",
    "    return STROKE_ALIASES.get(s, s if s in STROKE_ALIASES.values() else None)\n",
    "\n",
    "def parse_event_to_distance_stroke(ev: str) -> Tuple[Optional[int], Optional[str]]:\n",
    "    if ev is None: return (None, None)\n",
    "    s = str(ev).strip().lower().replace(\"-\", \" \")\n",
    "    # e.g., \"200 free\", \"100m butterfly\", \"4x100 medley\" (we'll ignore relays)\n",
    "    # prefer the first number as distance (ignore \"4x100\" relays)\n",
    "    # Remove 'm' suffix after number (100m)\n",
    "    m = re.search(r'(\\d+)\\s*m?\\b', s)\n",
    "    distance = int(m.group(1)) if m else None\n",
    "    stroke = None\n",
    "    for w in STROKE_WORDS:\n",
    "        if re.search(r'\\b'+re.escape(w)+r'\\b', s):\n",
    "            stroke = normalize_stroke(w)\n",
    "            break\n",
    "    return (distance, stroke)\n",
    "\n",
    "def parse_time_to_seconds(t) -> Optional[float]:\n",
    "    if t is None or (isinstance(t, float) and math.isnan(t)): return None\n",
    "    s = str(t).strip()\n",
    "    if not s: return None\n",
    "    # numeric?\n",
    "    try:\n",
    "        return float(s)\n",
    "    except: pass\n",
    "    # M:SS(.xx)\n",
    "    if \":\" in s:\n",
    "        parts = s.split(\":\")\n",
    "        if len(parts) == 2:\n",
    "            try:\n",
    "                m = float(parts[0]); sec = float(parts[1])\n",
    "                return m*60.0 + sec\n",
    "            except: return None\n",
    "    # Strip stray chars\n",
    "    s2 = re.sub(r'[^0-9\\.]', '', s)\n",
    "    try:\n",
    "        return float(s2)\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "def seconds_to_time_str(sec: float) -> str:\n",
    "    if sec is None or not np.isfinite(sec): return \"NA\"\n",
    "    sec = float(sec)\n",
    "    if sec < 60:\n",
    "        return f\"{sec:0.2f}\"\n",
    "    m = int(sec // 60)\n",
    "    s = sec - 60*m\n",
    "    return f\"{m}:{s:05.2f}\"\n",
    "\n",
    "def parse_splits_from_string(s: str) -> List[float]:\n",
    "    if s is None or (isinstance(s, float) and math.isnan(s)): return []\n",
    "    parts = [p.strip() for p in str(s).replace(\",\", \";\").split(\";\") if p.strip()]\n",
    "    vals = [parse_time_to_seconds(p) for p in parts]\n",
    "    return [v for v in vals if v is not None and v > 0]\n",
    "\n",
    "def n50s_for_distance(distance: int) -> int:\n",
    "    return max(0, int(distance // 50))\n",
    "\n",
    "# ------------------ robust CSV loader ------------------\n",
    "\n",
    "def try_read_csv(path: str) -> Optional[pd.DataFrame]:\n",
    "    if not os.path.exists(path): return None\n",
    "    # try sep sniffing + utf-8, fallback to others\n",
    "    attempts = [\n",
    "        dict(sep=None, engine=\"python\", encoding=\"utf-8\"),\n",
    "        dict(sep=None, engine=\"python\", encoding=\"latin-1\"),\n",
    "        dict(sep=\",\", engine=\"python\", encoding=\"utf-8\"),\n",
    "        dict(sep=\";\", engine=\"python\", encoding=\"utf-8\"),\n",
    "        dict(sep=\"\\t\", engine=\"python\", encoding=\"utf-8\"),\n",
    "    ]\n",
    "    for kw in attempts:\n",
    "        try:\n",
    "            df = pd.read_csv(path, **kw)\n",
    "            # normalize headers\n",
    "            df.columns = [re.sub(r'\\s+', '_', str(c).strip().lower()) for c in df.columns]\n",
    "            if len(df) > 0:\n",
    "                return df\n",
    "        except Exception:\n",
    "            continue\n",
    "    # last resort: read raw and try manual split\n",
    "    try:\n",
    "        with open(path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
    "            head = f.read(1024)\n",
    "        delim = \";\" if head.count(\";\") > head.count(\",\") else \",\"\n",
    "        df = pd.read_csv(path, sep=delim, engine=\"python\", encoding=\"utf-8\", on_bad_lines=\"skip\")\n",
    "        df.columns = [re.sub(r'\\s+', '_', str(c).strip().lower()) for c in df.columns]\n",
    "        return df if len(df)>0 else None\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def load_csv_if_exists(path) -> Optional[pd.DataFrame]:\n",
    "    try:\n",
    "        if os.path.exists(path):\n",
    "            return try_read_csv(path)\n",
    "    except Exception as e:\n",
    "        print(f\"[warn] Could not read {path}: {e}\")\n",
    "    return None\n",
    "\n",
    "def first_existing(*paths) -> Optional[str]:\n",
    "    for p in paths:\n",
    "        if p and os.path.exists(p): return p\n",
    "    return None\n",
    "\n",
    "# ------------------ dataset harmonization ------------------\n",
    "\n",
    "SPLIT_COL_REGEXES = [\n",
    "    r'^(split|lap|l50|s50|fifty)(_)?(\\d+)$',   # split_1, lap_2, l50_3, s50_1, fifty_4\n",
    "    r'^50m[_-]?(\\d+)$',                       # 50m_1\n",
    "    r'^m50[_-]?(\\d+)$',                       # m50_1\n",
    "    r'^split50[_-]?(\\d+)$',                   # split50_1\n",
    "]\n",
    "\n",
    "def infer_split_columns(df: pd.DataFrame) -> List[str]:\n",
    "    cols = list(df.columns)\n",
    "    # bundled?\n",
    "    for c in SPLITS_BUNDLE:\n",
    "        if c in df.columns:\n",
    "            return [c]\n",
    "    # numbered patterns\n",
    "    out = []\n",
    "    for c in cols:\n",
    "        for pat in SPLIT_COL_REGEXES:\n",
    "            if re.match(pat, str(c), re.IGNORECASE):\n",
    "                out.append(c)\n",
    "                break\n",
    "    return out\n",
    "\n",
    "def extract_total_time_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    for c in TOTAL_TIME_CANDIDATES:\n",
    "        if c in df.columns: return c\n",
    "    for c in df.columns:\n",
    "        if str(c).lower() == \"result\": return c\n",
    "    return None\n",
    "\n",
    "def extract_distance_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    for c in DISTANCE_CANDIDATES:\n",
    "        if c in df.columns: return c\n",
    "    return None\n",
    "\n",
    "def extract_stroke_column(df: pd.DataFrame) -> Optional[str]:\n",
    "    for c in STROKE_CANDIDATES:\n",
    "        if c in df.columns: return c\n",
    "    return None\n",
    "\n",
    "def resample_splits(splits: List[float], n_target: int) -> List[float]:\n",
    "    total = sum(splits) if splits else 0.0\n",
    "    if n_target <= 0 or total <= 0: return []\n",
    "    # equal redistribution (robust)\n",
    "    return [total / n_target] * n_target\n",
    "\n",
    "def extract_splits_from_row(df: pd.DataFrame, row: pd.Series) -> List[float]:\n",
    "    split_cols = infer_split_columns(df)\n",
    "    if not split_cols: return []\n",
    "    if len(split_cols) == 1 and split_cols[0] in SPLITS_BUNDLE and split_cols[0] in df.columns:\n",
    "        return parse_splits_from_string(row[split_cols[0]])\n",
    "    # order by trailing number if present\n",
    "    def split_key(c):\n",
    "        m = re.search(r'(\\d+)$', str(c))\n",
    "        return int(m.group(1)) if m else 9999\n",
    "    cols = sorted(split_cols, key=split_key)\n",
    "    vals = []\n",
    "    for c in cols:\n",
    "        v = row.get(c, None)\n",
    "        if v is None or (isinstance(v, float) and math.isnan(v)): continue\n",
    "        v = parse_time_to_seconds(v)\n",
    "        if v is not None and v > 0: vals.append(v)\n",
    "    return vals\n",
    "\n",
    "def standardize_event_rows(df: pd.DataFrame, default_stroke: Optional[str]) -> pd.DataFrame:\n",
    "    df = df.copy()\n",
    "\n",
    "    # stroke\n",
    "    sc = extract_stroke_column(df)\n",
    "    if sc is None:\n",
    "        # attempt via event text\n",
    "        ev_col = None\n",
    "        for c in EVENT_CANDIDATES:\n",
    "            if c in df.columns: ev_col = c; break\n",
    "        if ev_col is not None:\n",
    "            df[\"stroke\"] = df[ev_col].apply(lambda s: parse_event_to_distance_stroke(s)[1] or default_stroke)\n",
    "        else:\n",
    "            df[\"stroke\"] = default_stroke\n",
    "    else:\n",
    "        df[\"stroke\"] = df[sc].apply(normalize_stroke)\n",
    "        if default_stroke and df[\"stroke\"].isna().all():\n",
    "            df[\"stroke\"] = default_stroke\n",
    "\n",
    "    # distance\n",
    "    dc = extract_distance_column(df)\n",
    "    if dc is None:\n",
    "        ev_col = None\n",
    "        for c in EVENT_CANDIDATES:\n",
    "            if c in df.columns: ev_col = c; break\n",
    "        if ev_col is not None:\n",
    "            df[\"distance\"] = df[ev_col].apply(lambda s: parse_event_to_distance_stroke(s)[0])\n",
    "        else:\n",
    "            split_cols = infer_split_columns(df)\n",
    "            if split_cols:\n",
    "                df[\"distance\"] = [len(extract_splits_from_row(df, r))*50 for _,r in df.iterrows()]\n",
    "            else:\n",
    "                df[\"distance\"] = np.nan\n",
    "    else:\n",
    "        df[\"distance\"] = pd.to_numeric(df[dc], errors=\"coerce\")\n",
    "\n",
    "    # total time\n",
    "    tc = extract_total_time_column(df)\n",
    "    if tc is None:\n",
    "        totals = []\n",
    "        for _, r in df.iterrows():\n",
    "            splits = extract_splits_from_row(df, r)\n",
    "            totals.append(sum(splits) if splits else np.nan)\n",
    "        df[\"time_total_sec\"] = totals\n",
    "    else:\n",
    "        df[\"time_total_sec\"] = df[tc].apply(parse_time_to_seconds)\n",
    "\n",
    "    # validity + cleanups\n",
    "    df[\"stroke\"] = df[\"stroke\"].apply(normalize_stroke)\n",
    "    df = df[~df[\"stroke\"].isna()]\n",
    "    df = df[~df[\"distance\"].isna()]\n",
    "    df = df[~df[\"time_total_sec\"].isna()]\n",
    "    df[\"distance\"] = df[\"distance\"].astype(float)\n",
    "    df = df[df[\"distance\"] > 0]\n",
    "    df = df[df[\"distance\"] % 50 == 0]\n",
    "    df = df[df[\"time_total_sec\"] > 0]\n",
    "\n",
    "    # build split & ratio columns\n",
    "    split_list, ratio_list, max_s = [], [], 0\n",
    "    for _, row in df.iterrows():\n",
    "        splits = extract_splits_from_row(df, row)\n",
    "        total = row[\"time_total_sec\"]\n",
    "        if not splits:\n",
    "            n = n50s_for_distance(int(row[\"distance\"]))\n",
    "            splits = [total / n]*n if n>0 else []\n",
    "        else:\n",
    "            n_expected = n50s_for_distance(int(row[\"distance\"]))\n",
    "            if n_expected>0 and len(splits)!=n_expected:\n",
    "                splits = resample_splits(splits, n_expected)\n",
    "        ratios = [s/total for s in splits] if total>0 else []\n",
    "        split_list.append(splits)\n",
    "        ratio_list.append(ratios)\n",
    "        max_s = max(max_s, len(splits))\n",
    "\n",
    "    for i in range(max_s):\n",
    "        df[f\"split_{i+1}_sec\"] = [(v[i] if i<len(v) else np.nan) for v in split_list]\n",
    "        df[f\"ratio_{i+1}\"]     = [(v[i] if i<len(v) else np.nan) for v in ratio_list]\n",
    "\n",
    "    # features (numeric, excluding targets)\n",
    "    exclude_prefixes = (\"split_\", \"ratio_\")\n",
    "    numeric_cols = []\n",
    "    for c in df.columns:\n",
    "        if c in [\"stroke\",\"distance\",\"time_total_sec\"]: continue\n",
    "        if c.startswith(exclude_prefixes): continue\n",
    "        if pd.api.types.is_numeric_dtype(df[c]): numeric_cols.append(c)\n",
    "    feature_cols = [\"time_total_sec\"] + numeric_cols\n",
    "    df[\"_feature_cols\"] = [feature_cols]*len(df)\n",
    "    return df\n",
    "\n",
    "# ------------------ leaderboard / model hints ------------------\n",
    "\n",
    "def load_leaderboard_and_models(path: str):\n",
    "    leaderboard = {}\n",
    "    model_hints = {}\n",
    "    df = load_csv_if_exists(path)\n",
    "    if df is None or df.empty: return leaderboard, model_hints\n",
    "\n",
    "    if \"stroke\" in df.columns:\n",
    "        df[\"stroke\"] = df[\"stroke\"].apply(normalize_stroke)\n",
    "\n",
    "    # model hints: columns stroke, model\n",
    "    if \"model\" in df.columns and \"stroke\" in df.columns:\n",
    "        for _, r in df.dropna(subset=[\"stroke\",\"model\"]).iterrows():\n",
    "            model_hints[r[\"stroke\"]] = str(r[\"model\"]).strip().lower()\n",
    "\n",
    "    # ratios/splits per (stroke, distance)\n",
    "    if \"stroke\" in df.columns and \"distance\" in df.columns:\n",
    "        tmp = df.dropna(subset=[\"stroke\",\"distance\"]).copy()\n",
    "        tmp[\"distance\"] = pd.to_numeric(tmp[\"distance\"], errors=\"coerce\")\n",
    "        tmp = tmp.dropna(subset=[\"distance\"])\n",
    "        ratio_cols = [c for c in tmp.columns if re.match(r'^(avg_)?ratio_\\d+$', str(c))]\n",
    "        if ratio_cols:\n",
    "            for _, r in tmp.iterrows():\n",
    "                ratios = [r[c] for c in ratio_cols]\n",
    "                ratios = [float(x) for x in ratios if x is not None and np.isfinite(x)]\n",
    "                if ratios:\n",
    "                    leaderboard[(r[\"stroke\"], int(r[\"distance\"]))] = (np.array(ratios)/np.sum(ratios)).tolist()\n",
    "        else:\n",
    "            split_cols = [c for c in tmp.columns if re.match(r'^(avg_)?split_\\d+(_sec)?$', str(c))]\n",
    "            if split_cols:\n",
    "                for _, r in tmp.iterrows():\n",
    "                    splits = [parse_time_to_seconds(r[c]) for c in split_cols]\n",
    "                    splits = [x for x in splits if x is not None and x > 0]\n",
    "                    if splits:\n",
    "                        ratios = np.array(splits)/np.sum(splits)\n",
    "                        leaderboard[(r[\"stroke\"], int(r[\"distance\"]))] = ratios.tolist()\n",
    "            if \"splits\" in tmp.columns:\n",
    "                for _, r in tmp.iterrows():\n",
    "                    s = parse_splits_from_string(r[\"splits\"])\n",
    "                    if s:\n",
    "                        ratios = np.array(s)/np.sum(s)\n",
    "                        leaderboard[(r[\"stroke\"], int(r[\"distance\"]))] = ratios.tolist()\n",
    "\n",
    "    return leaderboard, model_hints\n",
    "\n",
    "# ------------------ model store ------------------\n",
    "\n",
    "def make_base_estimator(name: str):\n",
    "    n = (name or \"\").strip().lower()\n",
    "    if n in (\"randomforest\",\"rf\",\"random_forest\"):\n",
    "        return RandomForestRegressor(n_estimators=300, random_state=42, min_samples_leaf=3, n_jobs=-1)\n",
    "    if n in (\"gradientboosting\",\"gb\",\"gbr\",\"gradient_boosting\"):\n",
    "        return GradientBoostingRegressor(random_state=42)\n",
    "    if n in (\"ridge\",):\n",
    "        return Ridge(alpha=1.0, random_state=42)\n",
    "    if n in (\"lasso\",):\n",
    "        return Lasso(alpha=0.0005, random_state=42, max_iter=20000)\n",
    "    if n in (\"elasticnet\",\"enet\"):\n",
    "        return ElasticNet(alpha=0.0005, l1_ratio=0.3, random_state=42, max_iter=20000)\n",
    "    if n in (\"svr\",\"svm\"):\n",
    "        return SVR(kernel=\"rbf\", C=10.0, gamma=\"scale\")\n",
    "    if n in (\"knn\",\"kneighbors\",\"k-neighbors\"):\n",
    "        return KNeighborsRegressor(n_neighbors=7, weights=\"distance\")\n",
    "    return RandomForestRegressor(n_estimators=300, random_state=42, min_samples_leaf=3, n_jobs=-1)\n",
    "\n",
    "class SplitModelStore:\n",
    "    def __init__(self):\n",
    "        self.models = {}                      # (stroke, dist) -> MultiOutputRegressor\n",
    "        self.feature_cols = {}                # (stroke, dist) -> [features]\n",
    "        self.output_dims = {}                 # (stroke, dist) -> n50s\n",
    "        self.leaderboard = {}                 # (stroke, dist) -> ratio list\n",
    "        self.model_hints = {}                 # stroke -> model name\n",
    "\n",
    "    def set_leaderboard(self, lb): self.leaderboard = lb or {}\n",
    "    def set_model_hints(self, hints): self.model_hints = {k:(v or \"\").lower() for k,v in (hints or {}).items()}\n",
    "\n",
    "    def _estimator_for_stroke(self, stroke: str):\n",
    "        return make_base_estimator(self.model_hints.get(stroke, \"\"))\n",
    "\n",
    "    def train_from_dataframe(self, df: pd.DataFrame):\n",
    "        if df is None or df.empty: return\n",
    "        for (stroke, dist), g in df.groupby([\"stroke\",\"distance\"]):\n",
    "            stroke = normalize_stroke(stroke)\n",
    "            if not stroke: continue\n",
    "            dist = int(dist) if pd.notna(dist) else None\n",
    "            if not dist or dist % 50 != 0: continue\n",
    "            n_out = n50s_for_distance(dist)\n",
    "\n",
    "            feat_cols = g[\"_feature_cols\"].iloc[0] if \"_feature_cols\" in g.columns else [\"time_total_sec\"]\n",
    "            feat_cols = [c for c in feat_cols if c in g.columns]\n",
    "            X = g[feat_cols].apply(pd.to_numeric, errors=\"coerce\").replace([np.inf,-np.inf], np.nan)\n",
    "            target_cols = [f\"ratio_{i+1}\" for i in range(n_out) if f\"ratio_{i+1}\" in g.columns]\n",
    "            if len(target_cols) != n_out: \n",
    "                # cannot train without complete targets\n",
    "                continue\n",
    "            Y = g[target_cols].apply(pd.to_numeric, errors=\"coerce\").replace([np.inf,-np.inf], np.nan)\n",
    "            XY = X.join(Y).dropna()\n",
    "            if XY.empty: continue\n",
    "\n",
    "            Xc = XY[feat_cols].values\n",
    "            Yc = XY[target_cols].values\n",
    "            row_sums = Yc.sum(axis=1, keepdims=True)\n",
    "            row_sums[row_sums==0] = 1.0\n",
    "            Yc = Yc / row_sums\n",
    "\n",
    "            base = self._estimator_for_stroke(stroke)\n",
    "            model = MultiOutputRegressor(base)\n",
    "            model.fit(Xc, Yc)\n",
    "            key = (stroke, dist)\n",
    "            self.models[key] = model\n",
    "            self.feature_cols[key] = feat_cols\n",
    "            self.output_dims[key] = n_out\n",
    "\n",
    "    def predict_ratios(self, stroke: str, distance: int, total_time_sec: float, extra_features: Optional[dict]=None):\n",
    "        key = (normalize_stroke(stroke), int(distance))\n",
    "        n_out = n50s_for_distance(distance)\n",
    "        # Build feature vector\n",
    "        feat_cols = self.feature_cols.get(key, [\"time_total_sec\"])\n",
    "        feats = {c: 0.0 for c in feat_cols}\n",
    "        feats[\"time_total_sec\"] = float(total_time_sec)\n",
    "        if extra_features:\n",
    "            for k,v in extra_features.items():\n",
    "                if k in feats and v is not None and np.isfinite(v):\n",
    "                    feats[k] = float(v)\n",
    "        Xrow = np.array([[feats[c] for c in feat_cols]])\n",
    "\n",
    "        if key in self.models:\n",
    "            pred = self.models[key].predict(Xrow)[0]\n",
    "            pred = np.maximum(pred, 1e-9)\n",
    "            ratios = pred / np.sum(pred)\n",
    "            if len(ratios) != n_out:\n",
    "                ratios = np.ones(n_out)/n_out\n",
    "            return ratios.tolist()\n",
    "\n",
    "        if key in self.leaderboard:\n",
    "            r = np.array(self.leaderboard[key], dtype=float)\n",
    "            r = np.maximum(r, 1e-9)\n",
    "            r = r / np.sum(r)\n",
    "            if len(r) != n_out:\n",
    "                r = np.ones(n_out)/n_out\n",
    "            return r.tolist()\n",
    "\n",
    "        # heuristic fallback\n",
    "        if distance <= 200:\n",
    "            base = np.linspace(1.05, 0.95, n_out)\n",
    "        else:\n",
    "            base = np.linspace(0.98, 1.02, n_out)\n",
    "        base = np.maximum(base, 1e-9)\n",
    "        return (base / np.sum(base)).tolist()\n",
    "\n",
    "# ------------------ load your files ------------------\n",
    "\n",
    "def load_all_datasets():\n",
    "    files = {\n",
    "        \"freestyle\":  first_existing(\"/mnt/data/freestyle_dataset.csv\",  \"./datasets/freestyle.csv\",  \"./datasets/freestyle_dataset.csv\"),\n",
    "        \"backstroke\": first_existing(\"/mnt/data/backstroke_dataset.csv\", \"./datasets/backstroke.csv\", \"./datasets/backstroke_dataset.csv\"),\n",
    "        \"breaststroke\": first_existing(\"/mnt/data/breaststroke_dataset.csv\",\"./datasets/breaststroke.csv\",\"./datasets/breaststroke_dataset.csv\"),\n",
    "        \"butterfly\":  first_existing(\"/mnt/data/butterfly_dataset.csv\", \"./datasets/butterfly.csv\",  \"./datasets/butterfly_dataset.csv\"),\n",
    "        \"im\":         first_existing(\"/mnt/data/im_dataset.csv\",         \"./datasets/im.csv\",         \"./datasets/im_dataset.csv\"),\n",
    "    }\n",
    "    frames = []\n",
    "    print(\"File load report:\")\n",
    "    for stroke_name, path in files.items():\n",
    "        if not path:\n",
    "            print(f\"  {stroke_name:12s}: not found\")\n",
    "            continue\n",
    "        df = load_csv_if_exists(path)\n",
    "        if df is None or df.empty:\n",
    "            print(f\"  {stroke_name:12s}: found at {path} but 0 rows after parsing\")\n",
    "            continue\n",
    "        before = len(df)\n",
    "        df_std = standardize_event_rows(df, default_stroke=stroke_name)\n",
    "        after = len(df_std)\n",
    "        print(f\"  {stroke_name:12s}: {path} | raw={before} -> usable={after}\")\n",
    "        frames.append(df_std)\n",
    "    if frames:\n",
    "        return pd.concat(frames, ignore_index=True)\n",
    "    return pd.DataFrame()\n",
    "\n",
    "def load_leaderboard_and_hints():\n",
    "    path = first_existing(\"/mnt/data/best_models_by_stroke.csv\", \"./datasets/best_models_by_stroke.csv\", \"./datasets/leaderboards.csv\")\n",
    "    if not path:\n",
    "        print(\"  leaderboard: not found\")\n",
    "        return {}, {}\n",
    "    lb, hints = load_leaderboard_and_models(path)\n",
    "    print(f\"  leaderboard: {path} | events={len(lb)} | model_hints={len(hints)}\")\n",
    "    return lb, hints\n",
    "\n",
    "# ------------------ CLI + core functions ------------------\n",
    "\n",
    "def pretty_print_splits(splits_sec: List[float]) -> str:\n",
    "    return \";\".join(seconds_to_time_str(x) for x in splits_sec)\n",
    "\n",
    "def ideal_splits_from_target(stroke: str, distance: int, target_time_str: str, personal_best_str: Optional[str] = None) -> List[float]:\n",
    "    target_sec = parse_time_to_seconds(target_time_str)\n",
    "    if target_sec is None or target_sec <= 0:\n",
    "        raise ValueError(\"Invalid target time.\")\n",
    "    extra = {}\n",
    "    if personal_best_str:\n",
    "        pb_sec = parse_time_to_seconds(personal_best_str)\n",
    "        if pb_sec and np.isfinite(pb_sec) and pb_sec > 0:\n",
    "            extra[\"pb_sec\"] = pb_sec\n",
    "    ratios = MODEL_STORE.predict_ratios(stroke, distance, total_time_sec=target_sec, extra_features=extra)\n",
    "    return (np.array(ratios)*target_sec).tolist()\n",
    "\n",
    "def analyze_post_race(stroke: str, distance: int, given_splits_str: str, personal_best_str: Optional[str] = None):\n",
    "    given = parse_splits_from_string(given_splits_str)\n",
    "    if not given:\n",
    "        raise ValueError(\"Could not parse given splits. Use format like '32.33;33.11;...'.\")\n",
    "    n_expected = n50s_for_distance(distance)\n",
    "    if n_expected <= 0: raise ValueError(\"Distance must be a positive multiple of 50.\")\n",
    "    if len(given) != n_expected:\n",
    "        given = resample_splits(given, n_expected)\n",
    "    total_given = sum(given)\n",
    "    target_sec = None\n",
    "    if personal_best_str:\n",
    "        pb = parse_time_to_seconds(personal_best_str)\n",
    "        if pb and pb > 0:\n",
    "            target_sec = pb\n",
    "    if not target_sec:\n",
    "        target_sec = total_given\n",
    "    ratios = MODEL_STORE.predict_ratios(stroke, distance, total_time_sec=target_sec, extra_features={\"pb_sec\": target_sec})\n",
    "    ideal = (np.array(ratios) * target_sec).tolist()\n",
    "\n",
    "    diff_per_split = (np.array(given) - np.array(ideal)).tolist()\n",
    "    cumulative_given = np.cumsum(given)\n",
    "    cumulative_ideal = np.cumsum(ideal)\n",
    "    total_delta = total_given - sum(ideal)\n",
    "\n",
    "    return {\n",
    "        \"stroke\": stroke, \"distance\": distance,\n",
    "        \"total_given_sec\": total_given, \"target_sec_used\": target_sec,\n",
    "        \"total_ideal_sec\": float(sum(ideal)), \"total_delta_sec\": float(total_delta),\n",
    "        \"given_splits_sec\": [float(x) for x in given], \"ideal_splits_sec\": [float(x) for x in ideal],\n",
    "        \"split_delta_sec\": [float(x) for x in diff_per_split],\n",
    "        \"cumulative_given_sec\": [float(x) for x in cumulative_given],\n",
    "        \"cumulative_ideal_sec\": [float(x) for x in cumulative_ideal],\n",
    "    }\n",
    "\n",
    "def figure_compare_splits(given: List[float], ideal: List[float], title: str = \"Splits Comparison\"):\n",
    "    n = max(len(given), len(ideal))\n",
    "    if n == 0:\n",
    "        print(\"[info] Nothing to plot.\"); return\n",
    "    if len(given) != n:\n",
    "        total = sum(given) if given else 0.0\n",
    "        given = [total/n]*n if n>0 else []\n",
    "    if len(ideal) != n:\n",
    "        total = sum(ideal) if ideal else 0.0\n",
    "        ideal = [total/n]*n if n>0 else []\n",
    "    xs = np.arange(1, n+1)\n",
    "    plt.figure(figsize=(8,4.5))\n",
    "    plt.plot(xs, given, marker='o', label=\"Given splits (sec)\")\n",
    "    plt.plot(xs, ideal, marker='o', label=\"Ideal splits (sec)\")\n",
    "    plt.xlabel(\"50m split #\"); plt.ylabel(\"Time (s)\")\n",
    "    plt.title(title); plt.legend(); plt.grid(True); plt.show()\n",
    "\n",
    "MENU = textwrap.dedent(\"\"\"\n",
    "    -------------------------\n",
    "    Competitive Pacing - CLI\n",
    "    -------------------------\n",
    "    Choose mode:\n",
    "      1) Pre-race (PB + Target -> ideal 50s splits)\n",
    "      2) Post-race (Given splits + PB -> analysis & chart)\n",
    "      3) Exit\n",
    "\"\"\")\n",
    "\n",
    "def ask(prompt: str, cast=str, allow_blank=False):\n",
    "    while True:\n",
    "        val = input(prompt).strip()\n",
    "        if not val and allow_blank:\n",
    "            return None\n",
    "        try:\n",
    "            return cast(val)\n",
    "        except Exception:\n",
    "            print(\"  Invalid input, try again.\")\n",
    "\n",
    "def parse_stroke_input(s: str) -> str:\n",
    "    st = normalize_stroke(s)\n",
    "    if not st:\n",
    "        raise ValueError(\"Unknown stroke. Use Freestyle/Backstroke/Breaststroke/Butterfly/IM.\")\n",
    "    return st\n",
    "\n",
    "def parse_distance_input(s: str) -> int:\n",
    "    d = int(float(s))\n",
    "    if d % 50 != 0 or d <= 0:\n",
    "        raise ValueError(\"Distance must be a positive multiple of 50.\")\n",
    "    return d\n",
    "\n",
    "def cli_loop():\n",
    "    while True:\n",
    "        print(MENU)\n",
    "        choice = ask(\"Enter choice (1/2/3): \", cast=str)\n",
    "        if choice == \"3\":\n",
    "            print(\"Bye!\"); break\n",
    "        elif choice == \"1\":\n",
    "            try:\n",
    "                stroke = parse_stroke_input(ask(\"Stroke (Freestyle/Backstroke/Breaststroke/Butterfly/IM): \", cast=str))\n",
    "                distance = parse_distance_input(ask(\"Distance (e.g., 50/100/200/400): \", cast=str))\n",
    "                pb = ask(\"Personal Best time (e.g., 1:45.23 or 65.23) [optional]: \", cast=str, allow_blank=True)\n",
    "                target = ask(\"Target time (e.g., 1:40.00 or 100.0): \", cast=str)\n",
    "                splits = ideal_splits_from_target(stroke, distance, target_time_str=target, personal_best_str=pb)\n",
    "                print(\"\\nIdeal 50m splits:\\n\", pretty_print_splits(splits))\n",
    "                figure_compare_splits([], splits, title=f\"Ideal Splits — {stroke.title()} {distance}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[error] {e}\")\n",
    "        elif choice == \"2\":\n",
    "            try:\n",
    "                stroke = parse_stroke_input(ask(\"Stroke (Freestyle/Backstroke/Breaststroke/Butterfly/IM): \", cast=str))\n",
    "                distance = parse_distance_input(ask(\"Distance (e.g., 50/100/200/400): \", cast=str))\n",
    "                given = ask(\"Given splits (semicolon-separated, e.g., 32.33;33.11;...): \", cast=str)\n",
    "                pb = ask(\"Personal Best time (e.g., 1:45.23 or 65.23) [optional]: \", cast=str, allow_blank=True)\n",
    "                report = analyze_post_race(stroke, distance, given_splits_str=given, personal_best_str=pb)\n",
    "                print(\"\\n--- Post-race Analysis ---\")\n",
    "                print(f\"Event: {stroke.title()} {distance}m\")\n",
    "                print(f\"Given total: {seconds_to_time_str(report['total_given_sec'])}\")\n",
    "                print(f\"Ideal total: {seconds_to_time_str(report['total_ideal_sec'])}\")\n",
    "                print(f\"Delta (Given - Ideal): {report['total_delta_sec']:+.2f} s\\n\")\n",
    "                print(\"Split-by-split (Given | Ideal | Δ):\")\n",
    "                for i, (g, idl, dlt) in enumerate(zip(report[\"given_splits_sec\"], report[\"ideal_splits_sec\"], report[\"split_delta_sec\"]), start=1):\n",
    "                    print(f\"  50#{i:>2}: {g:6.2f} | {idl:6.2f} | {dlt:+6.2f}\")\n",
    "                figure_compare_splits(report[\"given_splits_sec\"], report[\"ideal_splits_sec\"], \n",
    "                                      title=f\"Given vs Ideal — {stroke.title()} {distance}\")\n",
    "            except Exception as e:\n",
    "                print(f\"[error] {e}\")\n",
    "        else:\n",
    "            print(\"Invalid choice.\")\n",
    "\n",
    "def list_available_events():\n",
    "    keys = set(MODEL_STORE.models.keys()) | set(MODEL_STORE.leaderboard.keys())\n",
    "    if not keys:\n",
    "        print(\"No events available yet. Add/verify CSVs and re-run this cell.\")\n",
    "        return\n",
    "    print(\"Available (stroke, distance) events:\")\n",
    "    for k in sorted(keys):\n",
    "        print(\"  \", k)\n",
    "\n",
    "# ------------------ train now ------------------\n",
    "\n",
    "print(\"=== Competitive Pacing (resilient) ===\")\n",
    "ALL_DATA = load_all_datasets()\n",
    "LEADERBOARD, MODEL_HINTS = load_leaderboard_and_hints()\n",
    "\n",
    "MODEL_STORE = SplitModelStore()\n",
    "MODEL_STORE.set_leaderboard(LEADERBOARD)\n",
    "MODEL_STORE.set_model_hints(MODEL_HINTS)\n",
    "MODEL_STORE.train_from_dataframe(ALL_DATA)\n",
    "\n",
    "print(f\"Usable rows: {len(ALL_DATA)}\")\n",
    "if MODEL_HINTS: print(\"Model hints by stroke:\", MODEL_HINTS)\n",
    "print(\"Models trained for events:\", sorted(list(MODEL_STORE.models.keys())))\n",
    "print(\"Leaderboard events:\", sorted(list(LEADERBOARD.keys())))\n",
    "\n",
    "print(\"\\nTip: list_available_events() to view trained/baseline events.\")\n",
    "print(\"Run: cli_loop() to start the CLI.\\n\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
